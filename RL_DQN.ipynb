{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUOKU4RGfNCrdHneEEU7FQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bast-94/RL-DQN/blob/dqn-draft/RL_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projet de Reinforcement Learning : Deep Q-Learning sur le casse-brique d'Atari"
      ],
      "metadata": {
        "id": "IxUx75ISnaC2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IT6J8Qb1nTG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7aefc87-65fb-46ed-c8dc-6d9bbf6ef7f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install -q gymnasium[\"atari\",\"accept-rom-license\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Présentation globale du projet"
      ],
      "metadata": {
        "id": "R6RsUvhwom-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objectifs du projet"
      ],
      "metadata": {
        "id": "NJtQe-8OO2_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algorithme principal"
      ],
      "metadata": {
        "id": "mdognuOSOxDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pseudo code de l'algorithme"
      ],
      "metadata": {
        "id": "QTVHA36NPEcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$\\text{Algorithme de Q-Leearning profond avec répétition d'expérience}$\n",
        "1. **Initialisation:**\n",
        "   - Initialiser le réseau de neurones $Q$ avec des poids aléatoires.\n",
        "   - Initialiser la mémoire de relecture $D$ avec capacité maximale $N$.\n",
        "   - Initialiser aléatoirement les paramètres d'apprentissage.\n",
        "   - Initialiser la fonction $Q$ avec des $\\theta$ aléatoire.\n",
        "   - Initialiser $\\hat{Q}$ avec $\\theta^⁻ = \\theta$.\n",
        "\n",
        "2. **Pour chaque épisode:**\n",
        "   - Initialiser l'environnement et l'état initial $s_1=\\{x_1\\}$\n",
        "   - Appliquer le prétraitement $\\phi_1 = \\phi(s_1)$\n",
        "   \n",
        "   3. **Pour chaque étape $t$ de l'épisode:**\n",
        "      - Choisir l'action $a_t$ avec la politique $\\varepsilon$-greedy\n",
        "        - $\\mathbb{P}(a_t = argmax_a(Q(s_t,a;\\theta)) = 1 - \\varepsilon$\n",
        "        - $\\mathbb{P}(a_t = \\text{random\\_sample(}A)) = \\varepsilon$\n",
        "      - Exécuter l'action $a_t$, observer la récompense $r_t$ et l'état suivant $s_{t+1}$\n",
        "      - Stocker la transition $(s_t, a_t, r_{t}, s_{t+1})$ dans la mémoire de relecture $D$\n",
        "      - Affecter $s_{t+1}=s_t,a_t,x_{t+1}$\n",
        "      - Prétraitement de $s_{t+1}$ : $\\phi_{t+1}=\\phi(s_{t+1})$\n",
        "      - Échantillonner un lot aléatoire de transitions $(s_i, a_i, r_i, s_{i+1})$ de $D$\n",
        "      - Calculer la vérité terrain $y_i$ pour chaque transition $(s_i, a_i, r_i, s_{i+1})$ en utilisant le réseau $\\hat{Q}$ aux paramètre $\\theta^-$\n",
        "      - Cloner $Q$ dans $\\hat{Q}$ toutes les $C$ étapes\n",
        "      \n"
      ],
      "metadata": {
        "id": "QX8se4unQDDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Détails des variables\n",
        "- $Q$ : Fonction de qualité qui pour un couple état-action évalue à quel point une action dans un état donné est favorable.\n",
        "- $C$ : Nombre d'étapes à laquelle $\\hat{Q}$ se met à jour sur $Q$.\n",
        "- $\\hat{Q}$ : Target Network , il correspond à une version ancienne de $Q$ avec des paramètres $\\theta^-$ sur les $C$ dernières étapes.\n",
        "- $\\theta$ : Correspond aux paramètres du réseau de neurones.\n",
        "- ${A}$ : L'ensemble des actions possibles.\n",
        "- $a_t$ : L'action faite par l'agent à l'étape $t$.\n",
        "- $x_t$ : Correspond à l'image brut du jeu à l'étape $t$.\n",
        "- $s_t$ : Correspond à une séquence de couples action-image $\\{a_i \\times x_i\\}_{i\\lt t}$ .\n",
        "- $\\phi_t$ : Correspond au pré-traitement de l'état $s_t$ (Plus de détails dans la suite du notebook).\n",
        "- $\\varepsilon \\in [0,1]$ : Probabilité de choisir une action aléatoire.\n",
        "- $r_t$ : Récompense obtenue par la réalisation de l'action $a_t$ à l'instant $s_t$\n",
        "- $D$ : Mémoire de relecture.\n",
        "- $N$ : Nombre de simulations."
      ],
      "metadata": {
        "id": "9Se6OPbaO_cU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Démarche de recherche et implémentation"
      ],
      "metadata": {
        "id": "mWhORuoko1v4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "import copy"
      ],
      "metadata": {
        "id": "6MpkCLnpoSIw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prétraitement des données"
      ],
      "metadata": {
        "id": "Q78gPWyCXSAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(img_array:np.array) -> np.array :\n",
        "  transform = transforms.Compose([\n",
        "      transforms.ToPILImage(),\n",
        "      transforms.Resize((84, 84)),\n",
        "      transforms.Grayscale(num_output_channels=1),\n",
        "      transforms.ToTensor()])\n",
        "  return transform(img_array)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvsrhD0sQxqC",
        "outputId": "0d40254e-31a3-43a3-f8d2-3a5dac6f119a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('ALE/Breakout-v5', render_mode=\"rgb_array\")\n",
        "state_img = env.reset()[0]\n",
        "fig,axes = plt.subplots(1,2)\n",
        "fig.suptitle('Images comparison')\n",
        "axes[0].set_title('Original image')\n",
        "axes[0].imshow(state_img)\n",
        "axes[0].axis('off')\n",
        "new_img = preprocess_image(state_img)\n",
        "print(f'{new_img.size() = }')\n",
        "axes[1].axis('off')\n",
        "axes[1].set_title('Preprocessed image')\n",
        "axes[1].imshow(new_img.permute(1,2,0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "60aiphhbXCe_",
        "outputId": "8e264203-8ba1-422f-d261-0a8e408acf0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_img.size() = torch.Size([1, 84, 84])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f58a9d9c0a0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Elaboration d'un modèle de de DQN"
      ],
      "metadata": {
        "id": "0A1YB793pFc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self,n_action,height,width,linear_size=512,model_name:str=None):\n",
        "    super(DQN, self).__init__()\n",
        "    self.input_dimension = 1, height,width\n",
        "    self.conv_layers = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=self.input_dimension[0],out_channels=32,kernel_size=8),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    x0 = torch.zeros(1,*self.input_dimension)\n",
        "    x0 = self.conv_layers(x0)\n",
        "    flatten_dim = int(np.prod(x0.shape))\n",
        "\n",
        "    linear_size = 512\n",
        "    self.linear_layers = nn.Sequential(\n",
        "        nn.Linear(flatten_dim, linear_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(linear_size,n_action)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.conv_layers(x)\n",
        "    x = x.view(x.shape[0],-1)\n",
        "    return self.linear_layers(x)\n",
        "\n",
        "\n",
        "\n",
        "def create_model_from_env(env: gym.Env):\n",
        "\n",
        "  env_tensor = np.zeros(env.observation_space.shape,dtype=np.uint8)\n",
        "  x = preprocess_image(env_tensor)\n",
        "  _,height, width = x.shape\n",
        "  n_action=env.action_space.n\n",
        "  dqn = DQN(n_action=n_action,height=height, width=width)\n",
        "  return dqn"
      ],
      "metadata": {
        "id": "G823PH_jIctd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testons"
      ],
      "metadata": {
        "id": "bfOjRVyt_TRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('ALE/Breakout-v5', render_mode=\"rgb_array\")\n",
        "dqn = create_model_from_env(env)\n",
        "x = preprocess_image(env.reset()[0])\n",
        "x = x.unsqueeze(0)\n",
        "output = dqn(x)\n",
        "assert output.size(1) == env.action_space.n, print(f'{output.size(1)} != {env.action_space.n}')\n",
        "print(\"Les shapes de sortie du modèle sont cohérentes.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHL5liwk-X3B",
        "outputId": "3081fe8f-c8cd-4986-c5e1-66b1502bdedb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les shapes de sortie du modèle sont cohérentes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gestion de la mémoire des expériences"
      ],
      "metadata": {
        "id": "GkoNo8z40ulV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "class GameTransition():\n",
        "  tupled_transition = namedtuple('game_transition', ('initial_state', 'action', 'reward','next_state', 'done'))\n",
        "  def __init__(self, initial_state, action, reward, next_state, done):\n",
        "    self.initial_state = initial_state\n",
        "    self.action = action\n",
        "    self.reward = reward\n",
        "    self.next_state = next_state\n",
        "    self.done = done\n",
        "  def to_named_tuple(self):\n",
        "    return self.tupled_transition(self.initial_state, self.action, self.reward, self.next_state, self.done)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU9xgO1i1en3",
        "outputId": "46f039ca-776b-429d-e21f-670baee7bd1a"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ExpStack(): # D\n",
        "  def __init__(self, max_size:int):\n",
        "    self.transitions = []\n",
        "    self.max_size = max_size # N\n",
        "\n",
        "  def enqueue(self,transition:GameTransition):\n",
        "    if (len(self.transitions) >= self.max_size):\n",
        "      return\n",
        "    self.transitions.append(transition)\n",
        "\n",
        "  def get_experiences(self,nb_exp=1):\n",
        "    return random.sample(self.transitions, nb_exp)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.transitions)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.transitions[index]\n",
        "\n",
        "  def sample_minibatch(self,batch_size:int=32):\n",
        "    if (batch_size>=len(self)):\n",
        "      return self.transitions\n",
        "    ind = random.randint(0,len(self)-batch_size)\n",
        "    return self[ind:ind+batch_size]"
      ],
      "metadata": {
        "id": "tsLFma1J00XQ"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lancement du modèle sur l'algorithme **\"deep Q-learning with experience replay.\"** issu de l'article"
      ],
      "metadata": {
        "id": "sYoZhQMapgsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Action = int\n",
        "State = torch.Tensor"
      ],
      "metadata": {
        "id": "1kzOr7vmRNwk"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent():\n",
        "  def __init__(self, env: gym.Env, nb_episodes:int = int(2e3), max_step:int = int(1e5),max_experiences:int = int(1e5),update_frequency:int=5, epsilon:float = 0.05,batch_size:int=32,gamma:float=0.9,verbose:bool=True):\n",
        "    self.env = env\n",
        "    self.nb_episodes = nb_episodes\n",
        "    self.max_step = max_step\n",
        "    self.model = create_model_from_env(env)\n",
        "    self.target_model = copy.deepcopy(self.model)\n",
        "    self.experiences = ExpStack(max_size=max_experiences)\n",
        "    self.step_index = 0\n",
        "    self.episode_index = 0\n",
        "    self.verbose = verbose\n",
        "    self.update_frequency = update_frequency\n",
        "    self.epsilon = epsilon\n",
        "    self.current_state = None\n",
        "    self.legal_actions = list(range(env.action_space.n))\n",
        "    self.batch_size = batch_size\n",
        "    self.gamma = gamma\n",
        "\n",
        "\n",
        "  def train_agent(self):\n",
        "    while(self.episode_index < self.nb_episodes):\n",
        "      self.reset_game()\n",
        "      self.make_episode()\n",
        "      self.episode_index +=1\n",
        "      if(self.verbose and self.episode_index % 100==0):\n",
        "        print(f'{self.episode_index}/{self.nb_episodes} episodes are done')\n",
        "\n",
        "  def make_episode(self):\n",
        "    done = False\n",
        "    while(self.step_index <self.max_step and not done):\n",
        "      # Initialize State 1 # TODO\n",
        "      current_state = preprocess_image(self.env.reset()[0])\n",
        "      self.current_state = current_state\n",
        "      self.make_step(current_state)\n",
        "      self.step_index +=1\n",
        "\n",
        "  def make_step(self,current_state: State | None = None):\n",
        "    # Get action a_t\n",
        "    a_t = self.get_action(current_state=current_state)\n",
        "    # Make action and retrieve r_t and x_t\n",
        "    next_state,reward,done,info = self.make_action(a_t)\n",
        "    # Store transition\n",
        "    transition = GameTransition(current_state,action,reward,next_state,done)\n",
        "    self.experiences.enqueue(transition)\n",
        "    # Sample minibatch of transitions\n",
        "    mini_batch = self.get_minibatch()\n",
        "    self.learn(mini_batch)\n",
        "    if(self.step_index % self.update_frequency == 0):\n",
        "      self.update_parameters()\n",
        "\n",
        "    # make update according to step_index and update frequency # TODO\n",
        "\n",
        "  def get_minibatch(self):\n",
        "    return self.experiences.sample_minibatch(self.batch_size)\n",
        "\n",
        "  def learn(self,mini_batch:list[GameTransition]) -> None:\n",
        "    y = self.generate_y(mini_batch)\n",
        "\n",
        "\n",
        "  def generate_y(self,mini_batch:list[GameTransition]) -> torch.Tensor:\n",
        "    # Apply Bellman's equation\n",
        "    y = []\n",
        "    q_values = []\n",
        "    self.model.train()\n",
        "    for transition in mini_batch:\n",
        "      y_j = transition.reward\n",
        "      if (not transition.done):\n",
        "\n",
        "        next_state = transition.next_state.unsqueeze(0)\n",
        "        maximum_q_hat = self.target_model(next_state).max().item()\n",
        "        y_j += self.gamma * maximum_q_hat\n",
        "      #q_values.append()\n",
        "      y.append(y_j)\n",
        "\n",
        "    assert len(y) == len(mini_batch), \"Size problem between y and minibatch\"\n",
        "    y_tensor = torch.tensor(y)\n",
        "    y_tensor = y_tensor.unsqueeze(1)\n",
        "    assert y_tensor.shape == (len(mini_batch),1), \"Tensor size problem between y and minibatch\"\n",
        "    return y_tensor\n",
        "    # perform gradient descent # TODO\n",
        "\n",
        "  def make_action(self, action: Action) -> tuple[State,float, bool,dict]:\n",
        "    state,reward,truncated, terminated,info = self.env.step(action)\n",
        "    next_state = preprocess_image(state)\n",
        "    done = truncated or terminated\n",
        "    return next_state,reward,done,info\n",
        "\n",
        "\n",
        "  def get_action(self,current_state: State) -> Action:\n",
        "    if(random.random() < self.epsilon):\n",
        "      return random.choice(self.legal_actions)\n",
        "\n",
        "    current_state = current_state.unsqueeze(0)\n",
        "    model_output = self.model(current_state)\n",
        "    action = model_output.argmax().item()\n",
        "    return action\n",
        "\n",
        "  def update_parameters(self) -> None:\n",
        "    pass\n",
        "\n",
        "  def gradient_descent(self):\n",
        "    pass\n",
        "\n",
        "  def reset_game(self):\n",
        "    self.step_index = 0\n",
        "agent = DQNAgent(env,nb_episodes=1, max_step = 32)\n",
        "#agent.train_agent()"
      ],
      "metadata": {
        "id": "uHMulMZustrI"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.generate_y(agent.get_minibatch())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gqtR1DgKEui",
        "outputId": "20c27746-ddc1-4912-da1a-e4191edc966f"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([], size=(0, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.train_agent()\n"
      ],
      "metadata": {
        "id": "MYAqwefPX0gQ",
        "outputId": "f3c61bd5-6be9-418b-e263-028927bef977",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<__main__.GameTransition object at 0x7f58a9cc7b50>, <__main__.GameTransition object at 0x7f5891bf2da0>, <__main__.GameTransition object at 0x7f5891bf0cd0>, <__main__.GameTransition object at 0x7f589407fb50>, <__main__.GameTransition object at 0x7f58a9c5aa70>, <__main__.GameTransition object at 0x7f5891bf2200>, <__main__.GameTransition object at 0x7f5891bf3e80>, <__main__.GameTransition object at 0x7f5891bf3cd0>, <__main__.GameTransition object at 0x7f5891bf2e90>, <__main__.GameTransition object at 0x7f5891bf3370>, <__main__.GameTransition object at 0x7f5891bf0c10>, <__main__.GameTransition object at 0x7f5891bf2e00>, <__main__.GameTransition object at 0x7f5891bf3280>, <__main__.GameTransition object at 0x7f58a9cc74f0>, <__main__.GameTransition object at 0x7f58a9cc5b70>, <__main__.GameTransition object at 0x7f58a9cc6ad0>, <__main__.GameTransition object at 0x7f58a9cc4eb0>, <__main__.GameTransition object at 0x7f5891bf1450>, <__main__.GameTransition object at 0x7f58a9cc75b0>, <__main__.GameTransition object at 0x7f58a9cc4d90>, <__main__.GameTransition object at 0x7f5891bf0fa0>, <__main__.GameTransition object at 0x7f5891bf24d0>, <__main__.GameTransition object at 0x7f5891bf23b0>, <__main__.GameTransition object at 0x7f5891bf10f0>, <__main__.GameTransition object at 0x7f5891bf1bd0>, <__main__.GameTransition object at 0x7f5891bf1f00>, <__main__.GameTransition object at 0x7f5891bf37c0>, <__main__.GameTransition object at 0x7f5891bf1d20>, <__main__.GameTransition object at 0x7f5891bf3340>, <__main__.GameTransition object at 0x7f5891bf2d70>, <__main__.GameTransition object at 0x7f5891bf27d0>, <__main__.GameTransition object at 0x7f5891bf3190>]\n",
            "[False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = agent.get_minibatch()\n",
        "print(batch)\n",
        "clean_batch = [transition.to_named_tuple() for transition in batch]\n",
        "print(clean_batch)"
      ],
      "metadata": {
        "id": "r3A3bLvSbqTW",
        "outputId": "6e493e65-3e43-44f9-ace5-967694df89ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<__main__.GameTransition object at 0x7f58a9cc7b50>, <__main__.GameTransition object at 0x7f5891bf2da0>, <__main__.GameTransition object at 0x7f5891bf0cd0>, <__main__.GameTransition object at 0x7f589407fb50>, <__main__.GameTransition object at 0x7f58a9c5aa70>, <__main__.GameTransition object at 0x7f5891bf2200>, <__main__.GameTransition object at 0x7f5891bf3e80>, <__main__.GameTransition object at 0x7f5891bf3cd0>, <__main__.GameTransition object at 0x7f5891bf2e90>, <__main__.GameTransition object at 0x7f5891bf3370>, <__main__.GameTransition object at 0x7f5891bf0c10>, <__main__.GameTransition object at 0x7f5891bf2e00>, <__main__.GameTransition object at 0x7f5891bf3280>, <__main__.GameTransition object at 0x7f58a9cc74f0>, <__main__.GameTransition object at 0x7f58a9cc5b70>, <__main__.GameTransition object at 0x7f58a9cc6ad0>, <__main__.GameTransition object at 0x7f58a9cc4eb0>, <__main__.GameTransition object at 0x7f5891bf1450>, <__main__.GameTransition object at 0x7f58a9cc75b0>, <__main__.GameTransition object at 0x7f58a9cc4d90>, <__main__.GameTransition object at 0x7f5891bf0fa0>, <__main__.GameTransition object at 0x7f5891bf24d0>, <__main__.GameTransition object at 0x7f5891bf23b0>, <__main__.GameTransition object at 0x7f5891bf10f0>, <__main__.GameTransition object at 0x7f5891bf1bd0>, <__main__.GameTransition object at 0x7f5891bf1f00>, <__main__.GameTransition object at 0x7f5891bf37c0>, <__main__.GameTransition object at 0x7f5891bf1d20>, <__main__.GameTransition object at 0x7f5891bf3340>, <__main__.GameTransition object at 0x7f5891bf2d70>, <__main__.GameTransition object at 0x7f5891bf27d0>, <__main__.GameTransition object at 0x7f5891bf3190>]\n",
            "[game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False), game_transition(initial_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), action=0, reward=0.0, next_state=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), done=False)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    }
  ]
}