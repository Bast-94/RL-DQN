{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc69e154",
   "metadata": {
    "id": "view-in-github",
    "papermill": {
     "duration": 0.012419,
     "end_time": "2023-11-21T08:42:52.425400",
     "exception": false,
     "start_time": "2023-11-21T08:42:52.412981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Bast-94/RL-DQN/blob/dqn-draft/RL-DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7309fdd1",
   "metadata": {
    "id": "IxUx75ISnaC2",
    "papermill": {
     "duration": 0.012089,
     "end_time": "2023-11-21T08:42:52.450576",
     "exception": false,
     "start_time": "2023-11-21T08:42:52.438487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Projet de Reinforcement Learning : Deep Q-Learning sur le casse-brique d'Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70551624",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:42:52.477400Z",
     "iopub.status.busy": "2023-11-21T08:42:52.476429Z",
     "iopub.status.idle": "2023-11-21T08:43:41.135719Z",
     "shell.execute_reply": "2023-11-21T08:43:41.134225Z"
    },
    "id": "IT6J8Qb1nTG4",
    "outputId": "e64dc404-94ee-4e38-9684-98c18ef21579",
    "papermill": {
     "duration": 48.675994,
     "end_time": "2023-11-21T08:43:41.138647",
     "exception": false,
     "start_time": "2023-11-21T08:42:52.462653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[accept-rom-license] in /opt/conda/lib/python3.10/site-packages (0.26.3)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[accept-rom-license]) (1.24.3)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[accept-rom-license]) (2.2.1)\r\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[accept-rom-license]) (0.0.1)\r\n",
      "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license])\r\n",
      "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.7)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.66.1)\r\n",
      "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license])\r\n",
      "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2023.7.22)\r\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license\r\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=7408ed67009f0c9199ea5903146bab2dfe3d99e37c72d39764af6d2aa331d7a8\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\r\n",
      "Successfully built AutoROM.accept-rom-license\r\n",
      "Installing collected packages: AutoROM.accept-rom-license, autorom\r\n",
      "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\r\n",
      "Requirement already satisfied: gymnasium[atari] in /opt/conda/lib/python3.10/site-packages (0.26.3)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[atari]) (1.24.3)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[atari]) (2.2.1)\r\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[atari]) (0.0.1)\r\n",
      "Collecting ale-py~=0.8.0 (from gymnasium[atari])\r\n",
      "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gymnasium[atari]) (5.13.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gymnasium[atari]) (4.5.0)\r\n",
      "Installing collected packages: ale-py\r\n",
      "Successfully installed ale-py-0.8.1\r\n"
     ]
    }
   ],
   "source": [
    "! pip install gymnasium[\"accept-rom-license\"]\n",
    "! pip install gymnasium[\"atari\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b827cfc",
   "metadata": {
    "id": "R6RsUvhwom-z",
    "papermill": {
     "duration": 0.015261,
     "end_time": "2023-11-21T08:43:41.170551",
     "exception": false,
     "start_time": "2023-11-21T08:43:41.155290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Présentation globale du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec0bfd",
   "metadata": {
    "id": "NJtQe-8OO2_1",
    "papermill": {
     "duration": 0.015275,
     "end_time": "2023-11-21T08:43:41.201832",
     "exception": false,
     "start_time": "2023-11-21T08:43:41.186557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Objectifs du projet\n",
    "\n",
    "Le but de ce projet est de mettre en place un algorithme d'apprentissage par renforcement basé sur les réseaux de neurones capable de jouer au Casse Brique d'Atari en maximisant ses gains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1528f1ec",
   "metadata": {
    "id": "mdognuOSOxDy",
    "papermill": {
     "duration": 0.01532,
     "end_time": "2023-11-21T08:43:41.232806",
     "exception": false,
     "start_time": "2023-11-21T08:43:41.217486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Algorithme principal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9571eca9",
   "metadata": {
    "id": "Zs_C95Ewndez",
    "papermill": {
     "duration": 0.016042,
     "end_time": "2023-11-21T08:43:41.264290",
     "exception": false,
     "start_time": "2023-11-21T08:43:41.248248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "L'algorithme de l'agent va être quelque peu différent de ceux vus dans les tavaux précédents (Sarsa, Qlearning). Plusieurs éléments vont compléxifier la tâche:\n",
    "- Les états sont sous formes d'images de taille $(210,160,3)$ et non plus sous forme numérique.\n",
    "- La fonction $Q(s,a)$ va faire intervenir un réseaux de neurones $\\theta$ qui devra être entraîné.\n",
    "- Les anciennes expériences devront être stockées dans le but de donner une vérité terrain pour l'entrainement du réseau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe979cd",
   "metadata": {
    "id": "QTVHA36NPEcA",
    "papermill": {
     "duration": 0.015286,
     "end_time": "2023-11-21T08:43:41.295742",
     "exception": false,
     "start_time": "2023-11-21T08:43:41.280456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Pseudo code de l'algorithme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfa379",
   "metadata": {
    "id": "mgyJ9MDNo04q",
    "papermill": {
     "duration": 0.015754,
     "end_time": "2023-11-21T08:43:41.327063",
     "exception": false,
     "start_time": "2023-11-21T08:43:41.311309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Le pseudo-code de l'algorithme ci-dessous provient de la publication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b5517",
   "metadata": {
    "id": "QX8se4unQDDr",
    "papermill": {
     "duration": 0.015769,
     "end_time": "2023-11-21T08:43:41.358502",
     "exception": false,
     "start_time": "2023-11-21T08:43:41.342733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "$\\text{Algorithme de Q-Leearning profond avec répétition d'expérience}$\n",
    "1. **Initialisation:**\n",
    "   - Initialiser le réseau de neurones $Q$ avec des poids aléatoires.\n",
    "   - Initialiser la mémoire de relecture $D$ avec capacité maximale $N$.\n",
    "   - Initialiser aléatoirement les paramètres d'apprentissage.\n",
    "   - Initialiser la fonction $Q$ avec des $\\theta$ aléatoire.\n",
    "   - Initialiser $\\hat{Q}$ avec $\\theta^⁻ = \\theta$.\n",
    "\n",
    "2. **Pour chaque épisode:**\n",
    "   - Initialiser l'environnement et l'état initial $s_1=\\{x_1\\}$\n",
    "   - Appliquer le prétraitement $\\phi_1 = \\phi(s_1)$\n",
    "   \n",
    "   3. **Pour chaque étape $t$ de l'épisode:**\n",
    "      - Choisir l'action $a_t$ avec la politique $\\varepsilon$-greedy\n",
    "        - $\\mathbb{P}(a_t = argmax_a(Q(s_t,a;\\theta)) = 1 - \\varepsilon$\n",
    "        - $\\mathbb{P}(a_t = \\text{random\\_sample(}A)) = \\varepsilon$\n",
    "      - Exécuter l'action $a_t$, observer la récompense $r_t$ et l'état suivant $s_{t+1}$\n",
    "      - Stocker la transition $(s_t, a_t, r_{t}, s_{t+1})$ dans la mémoire de relecture $D$\n",
    "      - Affecter $s_{t+1}=s_t,a_t,x_{t+1}$\n",
    "      - Prétraitement de $s_{t+1}$ : $\\phi_{t+1}=\\phi(s_{t+1})$\n",
    "      - Échantillonner un lot aléatoire de transitions $(s_i, a_i, r_i, s_{i+1})$ de $D$\n",
    "      - Calculer la vérité terrain $y_i$ pour chaque transition $(s_i, a_i, r_i, s_{i+1})$ en utilisant le réseau $\\hat{Q}$ aux paramètre $\\theta^-$\n",
    "      - Cloner $Q$ dans $\\hat{Q}$ toutes les $C$ étapes\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4d95d",
   "metadata": {
    "id": "9Se6OPbaO_cU",
    "papermill": {
     "duration": 0.015859,
     "end_time": "2023-11-21T08:43:41.390050",
     "exception": false,
     "start_time": "2023-11-21T08:43:41.374191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Détails des variables\n",
    "- $Q$ : Fonction de qualité qui pour un couple état-action évalue à quel point une action dans un état donné est favorable.\n",
    "- $C$ : Nombre d'étapes à laquelle $\\hat{Q}$ se met à jour sur $Q$.\n",
    "- $\\hat{Q}$ : Target Network , il correspond à une version ancienne de $Q$ avec des paramètres $\\theta^-$ sur les $C$ dernières étapes.\n",
    "- $\\theta$ : Correspond aux paramètres du réseau de neurones.\n",
    "- ${A}$ : L'ensemble des actions possibles.\n",
    "- $a_t$ : L'action faite par l'agent à l'étape $t$.\n",
    "- $x_t$ : Correspond à l'image brut du jeu à l'étape $t$.\n",
    "- $s_t$ : Correspond à une séquence de couples action-image $\\{a_i \\times x_i\\}_{i\\lt t}$ .\n",
    "- $\\phi_t$ : Correspond au pré-traitement de l'état $s_t$ (Plus de détails dans la suite du notebook).\n",
    "- $\\varepsilon \\in [0,1]$ : Probabilité de choisir une action aléatoire.\n",
    "- $r_t$ : Récompense obtenue par la réalisation de l'action $a_t$ à l'instant $s_t$\n",
    "- $D$ : Mémoire de relecture.\n",
    "- $N$ : Nombre de simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb95d0",
   "metadata": {
    "id": "mWhORuoko1v4",
    "papermill": {
     "duration": 0.015357,
     "end_time": "2023-11-21T08:43:41.420938",
     "exception": false,
     "start_time": "2023-11-21T08:43:41.405581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Démarche de recherche et implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8c4aaa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:41.454566Z",
     "iopub.status.busy": "2023-11-21T08:43:41.454119Z",
     "iopub.status.idle": "2023-11-21T08:43:42.428313Z",
     "shell.execute_reply": "2023-11-21T08:43:42.427356Z"
    },
    "papermill": {
     "duration": 0.994233,
     "end_time": "2023-11-21T08:43:42.430919",
     "exception": false,
     "start_time": "2023-11-21T08:43:41.436686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "if gymnasium.envs.registration.registry.get('ALE/Breakout-v5') is None:\n",
    "    import gym\n",
    "else:\n",
    "    import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04918bb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:42.464091Z",
     "iopub.status.busy": "2023-11-21T08:43:42.463637Z",
     "iopub.status.idle": "2023-11-21T08:43:47.069717Z",
     "shell.execute_reply": "2023-11-21T08:43:47.068358Z"
    },
    "id": "6MpkCLnpoSIw",
    "outputId": "ec7dcbf7-c456-4383-85d5-6949e51f5109",
    "papermill": {
     "duration": 4.626259,
     "end_time": "2023-11-21T08:43:47.072659",
     "exception": false,
     "start_time": "2023-11-21T08:43:42.446400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importation pour la création des réseaux de neurones\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from time import time\n",
    "from collections import namedtuple\n",
    "# Librairie dédiée à la création des gif finaux\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f8b47",
   "metadata": {
    "id": "Q78gPWyCXSAQ",
    "papermill": {
     "duration": 0.01549,
     "end_time": "2023-11-21T08:43:47.103849",
     "exception": false,
     "start_time": "2023-11-21T08:43:47.088359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Prétraitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72449f",
   "metadata": {
    "id": "sXcCJXtob6NO",
    "papermill": {
     "duration": 0.016311,
     "end_time": "2023-11-21T08:43:47.135844",
     "exception": false,
     "start_time": "2023-11-21T08:43:47.119533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Le prétraitement des données se justifie par le fait que l'entrainement des des réseaux de neurones nécessite d'enregistrer les états précédents. Les états correspondent ici à une séquence d'image du jeux sous frome de tableau de dimension $\\text{(hauteur,largeur,nombre de canaux)}$. Afin de réduire le coût en mémoire il faut réduire la taille de chaque état en les prétraitant et en conservant l'information. L'interface du casse-brique d'Atari est très pixelisée et redondante en terme de couleurs, il est donc possible de pouvoir faire une réduction de dimension au niveau de la couleur et de la taille de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "001ec69b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:47.170120Z",
     "iopub.status.busy": "2023-11-21T08:43:47.168877Z",
     "iopub.status.idle": "2023-11-21T08:43:47.175080Z",
     "shell.execute_reply": "2023-11-21T08:43:47.174163Z"
    },
    "id": "vvsrhD0sQxqC",
    "papermill": {
     "duration": 0.025937,
     "end_time": "2023-11-21T08:43:47.177301",
     "exception": false,
     "start_time": "2023-11-21T08:43:47.151364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img_array:np.array) -> np.array :\n",
    "  transform = transforms.Compose([\n",
    "      transforms.ToPILImage(),\n",
    "      transforms.Resize((84, 84)),\n",
    "      transforms.Grayscale(num_output_channels=1),\n",
    "      transforms.ToTensor()])\n",
    "  return transform(img_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb2aa7",
   "metadata": {
    "id": "I6UE9iZUfVl6",
    "papermill": {
     "duration": 0.015863,
     "end_time": "2023-11-21T08:43:47.209092",
     "exception": false,
     "start_time": "2023-11-21T08:43:47.193229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ainsi l'image passera d'une shape $(210,160,3)$ à une shape $(84,84,1)$ réduisant considérablement la taille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c06034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:47.242709Z",
     "iopub.status.busy": "2023-11-21T08:43:47.241961Z",
     "iopub.status.idle": "2023-11-21T08:43:48.049974Z",
     "shell.execute_reply": "2023-11-21T08:43:48.048720Z"
    },
    "id": "60aiphhbXCe_",
    "outputId": "09053a52-58b0-4858-f8fc-db3452414bdc",
    "papermill": {
     "duration": 0.828481,
     "end_time": "2023-11-21T08:43:48.053196",
     "exception": false,
     "start_time": "2023-11-21T08:43:47.224715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, False, False, {'lives': 5, 'episode_frame_number': 4, 'frame_number': 4})\n",
      "new_img.size() = torch.Size([1, 84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x793a8970fe50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('ALE/Breakout-v5', render_mode=\"rgb_array\")\n",
    "state_img = env.reset()[0]\n",
    "print(env.step(1)[1:])\n",
    "fig,axes = plt.subplots(1,2)\n",
    "fig.suptitle('Images comparison')\n",
    "axes[0].set_title('Original image')\n",
    "axes[0].imshow(state_img)\n",
    "axes[0].axis('off')\n",
    "new_img = preprocess_image(state_img)\n",
    "print(f'{new_img.size() = }')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Preprocessed image')\n",
    "axes[1].imshow(new_img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c467a14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:48.153207Z",
     "iopub.status.busy": "2023-11-21T08:43:48.152803Z",
     "iopub.status.idle": "2023-11-21T08:43:48.159678Z",
     "shell.execute_reply": "2023-11-21T08:43:48.158336Z"
    },
    "id": "Uj5w4MoD3dxR",
    "papermill": {
     "duration": 0.028008,
     "end_time": "2023-11-21T08:43:48.162556",
     "exception": false,
     "start_time": "2023-11-21T08:43:48.134548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Input = namedtuple('input', ('height', 'width', 'n_action'))\n",
    "\n",
    "def get_input_shapes(env:gym.Env):\n",
    "  env_tensor = np.zeros(env.observation_space.shape,dtype=np.uint8)\n",
    "  x = preprocess_image(env_tensor)\n",
    "  _,height, width = x.shape\n",
    "  n_action=env.action_space.n\n",
    "  return Input(height,width,n_action)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1428c",
   "metadata": {
    "id": "sotZRDhx4Mc8",
    "papermill": {
     "duration": 0.016238,
     "end_time": "2023-11-21T08:43:48.195530",
     "exception": false,
     "start_time": "2023-11-21T08:43:48.179292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Il faut ensuite pouvoir faire en sorte que chaque état corresponde à un séquence d'images réduites du jeu. Nous allons donc créer la classe `StateGenerator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7588fa79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:48.231041Z",
     "iopub.status.busy": "2023-11-21T08:43:48.230123Z",
     "iopub.status.idle": "2023-11-21T08:43:48.243128Z",
     "shell.execute_reply": "2023-11-21T08:43:48.241869Z"
    },
    "id": "ecO981RXCLVj",
    "outputId": "5cba5934-921d-4981-f7e2-0ac853487db8",
    "papermill": {
     "duration": 0.033533,
     "end_time": "2023-11-21T08:43:48.245620",
     "exception": false,
     "start_time": "2023-11-21T08:43:48.212087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Action = int\n",
    "from collections import deque\n",
    "class StateGenerator():\n",
    "  def __init__(self,env:gym.Env,seq_len:int):\n",
    "    self.env = env\n",
    "    env_tensor = np.zeros(env.observation_space.shape,dtype=np.uint8)\n",
    "    x = preprocess_image(env_tensor)\n",
    "    _,height, width = x.shape\n",
    "    n_action=env.action_space.n\n",
    "    self.seq_len = seq_len\n",
    "    self.stack = torch.zeros(seq_len,height,width)\n",
    "    self.current_frame = np.ndarray(shape=(height,width,1))\n",
    "\n",
    "  def init_env(self):\n",
    "    self.current_frame  = self.env.reset()[0]\n",
    "    state = preprocess_image(self.current_frame)\n",
    "    self.stack[:] =state\n",
    "    return self.stack, self.current_frame\n",
    "\n",
    "  def make_action(self,action:Action):\n",
    "    next_state,reward,truncated, terminated,info =self.env.step(action)\n",
    "    self.current_frame = next_state\n",
    "    next_state = preprocess_image(next_state)\n",
    "    done = truncated or terminated\n",
    "    self._update_stack(next_state)\n",
    "\n",
    "    return self.stack,reward, done, info, self.current_frame\n",
    "\n",
    "  def _update_stack(self, new_state: torch.Tensor):\n",
    "        self.stack[:-1] = self.stack[1:].clone()\n",
    "        self.stack[-1] = new_state\n",
    "\n",
    "  def reset_stack(self):\n",
    "    self.stack = torch.zeros(self.seq_len, 1, *self.stack.shape[2:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e710b",
   "metadata": {
    "id": "0A1YB793pFc2",
    "papermill": {
     "duration": 0.016472,
     "end_time": "2023-11-21T08:43:48.279789",
     "exception": false,
     "start_time": "2023-11-21T08:43:48.263317",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Elaboration de modèles de de DQN\n",
    "\n",
    "Ici deux modèles vont être implémentés pour étudier quelle est la stratégie la plus adaptée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e02857",
   "metadata": {
    "id": "FxrkcKgrgGX5",
    "papermill": {
     "duration": 0.016549,
     "end_time": "2023-11-21T08:43:48.313020",
     "exception": false,
     "start_time": "2023-11-21T08:43:48.296471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### DQN simple\n",
    "\n",
    "Le premier dispose de 3 couches de convolutions et d'une couche linaire. Le tenseur de sorti correspond aux Q values de chaque action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f71a2c52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:48.349660Z",
     "iopub.status.busy": "2023-11-21T08:43:48.348869Z",
     "iopub.status.idle": "2023-11-21T08:43:48.362225Z",
     "shell.execute_reply": "2023-11-21T08:43:48.360620Z"
    },
    "id": "G823PH_jIctd",
    "outputId": "f6fc310e-80d7-471a-e819-064b6e0a5ee2",
    "papermill": {
     "duration": 0.035016,
     "end_time": "2023-11-21T08:43:48.365128",
     "exception": false,
     "start_time": "2023-11-21T08:43:48.330112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_action, height, width,seq_len=1 ,linear_size=1024, model_name=None,):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_dimension = seq_len, height, width\n",
    "        self.conv1 = nn.Conv2d(in_channels=seq_len, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        x0 = torch.zeros(1, seq_len, height, width)\n",
    "        x0 = self.convolute(x0)\n",
    "        x0 = self.flatten(x0)\n",
    "        flatten_dim = x0.shape[1]\n",
    "\n",
    "        self.linear1 = nn.Linear(flatten_dim, linear_size)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(linear_size, n_action)\n",
    "\n",
    "    def flatten(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "    def convolute(self,x):\n",
    "      x = self.conv1(x)\n",
    "      x = self.relu1(x)\n",
    "      x = self.conv2(x)\n",
    "      x = self.relu2(x)\n",
    "      x = self.conv3(x)\n",
    "      x = self.relu3(x)\n",
    "      return x\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.convolute(x)\n",
    "\n",
    "      x = self.flatten(x)\n",
    "\n",
    "      x = self.linear1(x)\n",
    "      x = self.relu4(x)\n",
    "      return self.linear2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79452c1",
   "metadata": {
    "id": "J8rKs2lx6yG1",
    "papermill": {
     "duration": 0.016617,
     "end_time": "2023-11-21T08:43:48.398629",
     "exception": false,
     "start_time": "2023-11-21T08:43:48.382012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Le second est un  dueling network, il décompose la valeur d'action en deux parties: la valeur de l'état (à quel point l'état actuel est cool) et l'avantage de l'action (à quel point choisir cette action par rapport aux autres est cool). Cela permet au réseau de mieux comprendre ce qui se passe dans le jeu et d'apprendre de manière plus efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4056d7f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:48.435268Z",
     "iopub.status.busy": "2023-11-21T08:43:48.434825Z",
     "iopub.status.idle": "2023-11-21T08:43:48.448154Z",
     "shell.execute_reply": "2023-11-21T08:43:48.447176Z"
    },
    "id": "ST4yINSA3j76",
    "papermill": {
     "duration": 0.033959,
     "end_time": "2023-11-21T08:43:48.450488",
     "exception": false,
     "start_time": "2023-11-21T08:43:48.416529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DuellingDQN(nn.Module):\n",
    "    def __init__(self, n_action, height, width, seq_len =1,linear_size=1024, model_name=None):\n",
    "        super(DuellingDQN, self).__init__()\n",
    "        self.input_dimension = seq_len, height, width\n",
    "        self.model_name = model_name\n",
    "        self.conv1 = nn.Conv2d(in_channels=self.input_dimension[0], out_channels=64, kernel_size=8, stride=4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        x0 = torch.zeros(1, seq_len, height, width)\n",
    "        x0 = self.convolute(x0)\n",
    "        x0 = self.flatten(x0)\n",
    "        flatten_dim = x0.shape[1]\n",
    "        self.value = nn.Linear(flatten_dim,1)\n",
    "        self.advantage = nn.Linear(flatten_dim,n_action)\n",
    "\n",
    "\n",
    "\n",
    "    def flatten(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "    def convolute(self,x):\n",
    "      x = self.conv1(x)\n",
    "      x = self.relu1(x)\n",
    "      x = self.conv2(x)\n",
    "      x = self.relu2(x)\n",
    "      x = self.conv3(x)\n",
    "      x = self.relu3(x)\n",
    "      return x\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.convolute(x)\n",
    "      x = self.flatten(x)\n",
    "      value = self.value(x)\n",
    "      advantage = self.advantage(x)\n",
    "      q_value = value + (advantage - advantage.mean(dim=1,keepdim=True))\n",
    "      return q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f733712",
   "metadata": {
    "id": "bfOjRVyt_TRz",
    "papermill": {
     "duration": 0.017077,
     "end_time": "2023-11-21T08:43:48.484358",
     "exception": false,
     "start_time": "2023-11-21T08:43:48.467281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Vérifions l'homogénéité des shapes d'entrée et de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f23e1b5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:48.521928Z",
     "iopub.status.busy": "2023-11-21T08:43:48.521203Z",
     "iopub.status.idle": "2023-11-21T08:43:48.989043Z",
     "shell.execute_reply": "2023-11-21T08:43:48.988147Z"
    },
    "id": "bHL5liwk-X3B",
    "outputId": "2c29616f-bf30-46ff-d98c-9987a59c6617",
    "papermill": {
     "duration": 0.490008,
     "end_time": "2023-11-21T08:43:48.991608",
     "exception": false,
     "start_time": "2023-11-21T08:43:48.501600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les shapes de sortie du modèle sont cohérentes.\n",
      "torch.Size([1, 4])\n",
      "Les shapes de sortie du modèle sont cohérentes.\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('ALE/Breakout-v5', render_mode=\"rgb_array\")\n",
    "input_data = get_input_shapes(env)\n",
    "\n",
    "dqn = DQN(input_data.n_action,input_data.height,input_data.width)\n",
    "duelling_dqn = DuellingDQN(input_data.n_action,input_data.height,input_data.width)\n",
    "\n",
    "x = preprocess_image(env.reset()[0])\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "for model in [dqn, duelling_dqn]:\n",
    "  output = model(x)\n",
    "  assert output.size(1) == env.action_space.n, print(f'{output.size(1)} != {env.action_space.n}')\n",
    "  print(\"Les shapes de sortie du modèle sont cohérentes.\")\n",
    "  print(f'{output.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29af83",
   "metadata": {
    "id": "GkoNo8z40ulV",
    "papermill": {
     "duration": 0.01746,
     "end_time": "2023-11-21T08:43:49.027160",
     "exception": false,
     "start_time": "2023-11-21T08:43:49.009700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Gestion de la mémoire des expériences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e8094",
   "metadata": {
    "id": "y_hG3WCN7nBT",
    "papermill": {
     "duration": 0.017169,
     "end_time": "2023-11-21T08:43:49.062211",
     "exception": false,
     "start_time": "2023-11-21T08:43:49.045042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Le but est de stocker les états précédents afin de permettre un meilleur apprentissage des Q values. Nous allons donc créer la classe `ExpStack`, qui génèrera des batchs aléatoires. La taille d'une telle structure est plafonnée afin d'éviter que la mémoire ne sature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd69d183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:49.104840Z",
     "iopub.status.busy": "2023-11-21T08:43:49.104010Z",
     "iopub.status.idle": "2023-11-21T08:43:49.109798Z",
     "shell.execute_reply": "2023-11-21T08:43:49.108863Z"
    },
    "id": "-EEcI22jgs3u",
    "papermill": {
     "duration": 0.030181,
     "end_time": "2023-11-21T08:43:49.112620",
     "exception": false,
     "start_time": "2023-11-21T08:43:49.082439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GameTransition = namedtuple('game_transition', ('initial_state', 'action', 'reward','next_state', 'done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "327f02b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:49.148665Z",
     "iopub.status.busy": "2023-11-21T08:43:49.148229Z",
     "iopub.status.idle": "2023-11-21T08:43:49.161972Z",
     "shell.execute_reply": "2023-11-21T08:43:49.160943Z"
    },
    "id": "tsLFma1J00XQ",
    "outputId": "6fa7d2da-d097-4280-8af8-f1afc78979e4",
    "papermill": {
     "duration": 0.034654,
     "end_time": "2023-11-21T08:43:49.164361",
     "exception": false,
     "start_time": "2023-11-21T08:43:49.129707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ExpStack(): # D\n",
    "  def __init__(self, max_size:int, seq_len:int = 4):\n",
    "    self.transitions = []\n",
    "    self.max_size = max_size # N\n",
    "    self.index = 0\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "  def enqueue(self,transition:GameTransition):\n",
    "    if (len(self.transitions) < self.max_size):\n",
    "      self.transitions.append(transition)\n",
    "    else:\n",
    "      self.transitions[self.index] = transition\n",
    "    self.index +=1\n",
    "    self.index = self.index % self.max_size\n",
    "\n",
    "  def get_experiences(self,nb_exp=1):\n",
    "    return random.sample(self.transitions, nb_exp)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.transitions)\n",
    "\n",
    "  def __getitem__(self,index):\n",
    "    return self.transitions[index]\n",
    "\n",
    "  def sample_minibatch(self,batch_size:int=32):\n",
    "    if (batch_size>len(self)):\n",
    "      return self.transitions\n",
    "\n",
    "    return random.sample(self.transitions, batch_size)\n",
    "\n",
    "  def tensor_batch(self,batch_size):\n",
    "    batch = random.sample(self.transitions, batch_size)\n",
    "    batch = GameTransition(*(zip(*batch)))\n",
    "    initial_state = torch.stack(batch.initial_state)\n",
    "\n",
    "    next_state = torch.stack(batch.next_state)\n",
    "\n",
    "    reward = torch.tensor(batch.reward)\n",
    "    reward = reward.unsqueeze(1)\n",
    "\n",
    "    done = torch.tensor(batch.done).float()\n",
    "    done = done.unsqueeze(1)\n",
    "\n",
    "    action = torch.tensor(batch.action)\n",
    "    action = action.unsqueeze(1)\n",
    "\n",
    "    return GameTransition(initial_state,action,reward,next_state,done)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298fa8be",
   "metadata": {
    "id": "sYoZhQMapgsH",
    "papermill": {
     "duration": 0.017076,
     "end_time": "2023-11-21T08:43:49.198553",
     "exception": false,
     "start_time": "2023-11-21T08:43:49.181477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Lancement du modèle sur l'algorithme **\"deep Q-learning with experience replay.\"** issu de l'article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fc858",
   "metadata": {
    "id": "8LU2or0X8Isq",
    "papermill": {
     "duration": 0.016439,
     "end_time": "2023-11-21T08:43:49.232011",
     "exception": false,
     "start_time": "2023-11-21T08:43:49.215572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Implémentation de l'Agent\n",
    "\n",
    "Nous allons créer la classe `DQNAgent` qui va pouvoir entraîner un modèle et mettre à jour $\\hat{Q}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdc38c7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:49.267805Z",
     "iopub.status.busy": "2023-11-21T08:43:49.267145Z",
     "iopub.status.idle": "2023-11-21T08:43:49.272312Z",
     "shell.execute_reply": "2023-11-21T08:43:49.271273Z"
    },
    "id": "1kzOr7vmRNwk",
    "papermill": {
     "duration": 0.025969,
     "end_time": "2023-11-21T08:43:49.274889",
     "exception": false,
     "start_time": "2023-11-21T08:43:49.248920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "State = torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ca84ec1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:49.310634Z",
     "iopub.status.busy": "2023-11-21T08:43:49.310236Z",
     "iopub.status.idle": "2023-11-21T08:43:49.333485Z",
     "shell.execute_reply": "2023-11-21T08:43:49.332442Z"
    },
    "id": "uHMulMZustrI",
    "papermill": {
     "duration": 0.044335,
     "end_time": "2023-11-21T08:43:49.336120",
     "exception": false,
     "start_time": "2023-11-21T08:43:49.291785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "  epsilon = 0.05\n",
    "  batch_size = 32\n",
    "  gamma = 0.9\n",
    "  max_experiences:int = 2500\n",
    "  def __init__(self, env: gym.Env,\n",
    "\n",
    "               seq_len:int=5,\n",
    "               verbose:bool=True,\n",
    "               model_file:str=None,\n",
    "               model:nn.Module=None):\n",
    "    self.state_gen = StateGenerator(env,seq_len=seq_len)\n",
    "    self.seq_len = seq_len\n",
    "    self.model = model\n",
    "    self.target_model = copy.deepcopy(self.model)\n",
    "    self.target_model.eval()\n",
    "\n",
    "    self.experiences = ExpStack(max_size=self.max_experiences,seq_len=1)\n",
    "    self.legal_actions = list(range(env.action_space.n))\n",
    "\n",
    "    self.loss_evolution = []\n",
    "    self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "    self.loss_func = nn.MSELoss()\n",
    "\n",
    "    self.epoch_count = 0\n",
    "    self.frames = []\n",
    "    if (model_file is None):\n",
    "      self.model_file = 'model.pt'\n",
    "    else:\n",
    "      self.model_file = model_file\n",
    "  def clear_frames(self):\n",
    "    del self.frames\n",
    "    self.frames = []\n",
    "  def clear_experiences(self):\n",
    "    del self.experiences\n",
    "    self.experiences = ExpStack(self.max_experiences)\n",
    "\n",
    "  def train_model(self) -> None:\n",
    "    self.model.train()\n",
    "    self.epoch_count +=1\n",
    "    if (len(self.experiences) >= self.batch_size):\n",
    "      batch = self.experiences.tensor_batch(self.batch_size)\n",
    "      y_target = (1 - batch.done ) * self.target_model(batch.next_state) * self.gamma + batch.reward\n",
    "      y_target = y_target.max(1)[0].unsqueeze(1)\n",
    "      y_pred = self.model(batch.initial_state).gather(1,batch.action)\n",
    "      loss = self.loss_func(y_target,y_pred)\n",
    "      del y_target, y_pred, batch\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.loss_evolution.append(loss.item())\n",
    "      self.optimizer.step()\n",
    "\n",
    "      return loss.item()\n",
    "\n",
    "  def make_action(self, action: Action) -> tuple[State,float, bool,dict,np.array]:\n",
    "    res = self.state_gen.make_action(action)\n",
    "    assert res[0].shape[0] == self.seq_len, f'{res[0].shape} is wrong'\n",
    "    return res\n",
    "\n",
    "\n",
    "  def get_action(self,current_state: State) -> Action:\n",
    "    if(random.random() < self.epsilon):\n",
    "      return random.choice(self.legal_actions)\n",
    "\n",
    "    current_state = current_state.unsqueeze(0)\n",
    "    model_output = self.model(current_state)\n",
    "    action = model_output.argmax().item()\n",
    "    return action\n",
    "\n",
    "  def update_parameters(self) -> None:\n",
    "    self.target_model.load_state_dict(self.model.state_dict())\n",
    "    self.target_model.eval()\n",
    "\n",
    "  def save_model(self):\n",
    "    torch.save(self.model,self.model_file+'.pt')\n",
    "\n",
    "\n",
    "  def generate_gif(self, output_file:str) -> None:\n",
    "\n",
    "    frames = [Image.fromarray(f, mode='RGB') for f in self.frames]\n",
    "    frames[0].save(output_file, format='GIF', append_images=frames[1:], save_all=True, duration=10, loop=0)\n",
    "    print(f'Saving {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb20221",
   "metadata": {
    "id": "HNsw26Xv8lCy",
    "papermill": {
     "duration": 0.016933,
     "end_time": "2023-11-21T08:43:49.369990",
     "exception": false,
     "start_time": "2023-11-21T08:43:49.353057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "La classe `Trainer` est ensuite définie dans le but de pouvoir entraîner un `DQNAgent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b33df00b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:49.407182Z",
     "iopub.status.busy": "2023-11-21T08:43:49.406721Z",
     "iopub.status.idle": "2023-11-21T08:43:49.428572Z",
     "shell.execute_reply": "2023-11-21T08:43:49.427362Z"
    },
    "id": "BVCi0zEfNJW5",
    "papermill": {
     "duration": 0.043801,
     "end_time": "2023-11-21T08:43:49.431425",
     "exception": false,
     "start_time": "2023-11-21T08:43:49.387624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  max_step = int(1e5)\n",
    "  epsilon = 0.05\n",
    "  batch_size = 32\n",
    "  gamma = 0.9\n",
    "  update_frequency = 5\n",
    "  def __init__(self, agent: DQNAgent, verbose:bool=True,time_limit=3600,nb_episodes:int=25000):\n",
    "      self.agent = agent\n",
    "      self.verbose = verbose\n",
    "      self.epochs_count = 0\n",
    "      self.stop = False\n",
    "      self.time_limit = time_limit\n",
    "      self.nb_episodes = nb_episodes\n",
    "      self.rewards = []\n",
    "\n",
    "  def train_agent(self,save_model :bool =True, gif_file:str=None):\n",
    "      episode_index = 0\n",
    "      total_reward = 0\n",
    "      self.reboot_timer()\n",
    "      rewards = []\n",
    "      while episode_index < self.nb_episodes and not self.stop:\n",
    "          reward = self.play()\n",
    "          self.rewards.append(reward)\n",
    "          episode_index += 1\n",
    "          total_reward+=reward\n",
    "          self.log(f'Episode {episode_index} is done')\n",
    "          self.log(f'Total reward {reward}')\n",
    "      if(save_model):\n",
    "        self.agent.save_model()\n",
    "      if(gif_file is not None):\n",
    "        self.agent.generate_gif(gif_file)\n",
    "      return total_reward/episode_index\n",
    "\n",
    "\n",
    "\n",
    "  def log(self, *args,**kwargs):\n",
    "    if(self.verbose):\n",
    "      print(*args,**kwargs)\n",
    "\n",
    "  def reboot_timer(self):\n",
    "    self.begin_time = time()\n",
    "    self.stop = False\n",
    "\n",
    "  def update_time(self):\n",
    "    current_time = int(time())\n",
    "    delta = current_time - self.begin_time\n",
    "    self.stop = delta > self.time_limit\n",
    "    if(self.stop):\n",
    "      self.log(f'TIME OUT: Model stops training after {delta:.2f} seconds, Save model to {self.agent.model_file}.pt')\n",
    "\n",
    "\n",
    "  def play(self, train:bool=True,keep_frame:bool=False):\n",
    "      done = False\n",
    "      step_index = 0\n",
    "      agent = self.agent\n",
    "      current_state,frame = agent.state_gen.init_env()\n",
    "      agent.frames.append(frame)\n",
    "\n",
    "      total_reward = 0\n",
    "\n",
    "      while step_index < self.max_step and not done and not self.stop:\n",
    "          self.update_time()\n",
    "          a_t = agent.get_action(current_state=current_state)\n",
    "          next_state, reward, done, info, next_state_frame = agent.make_action(a_t)\n",
    "          assert next_state.shape[0] == (agent.seq_len), \"Bad Shapes\"\n",
    "          assert current_state.shape[0] == (agent.seq_len), \"Bad Shapes\"\n",
    "          if(keep_frame):\n",
    "            agent.frames.append(next_state_frame)\n",
    "          total_reward += reward\n",
    "\n",
    "          if train:\n",
    "              transition = GameTransition(current_state, a_t, reward, next_state, done)\n",
    "              agent.experiences.enqueue(transition)\n",
    "              loss = agent.train_model()\n",
    "              if step_index % self.update_frequency == 0:\n",
    "                  agent.update_parameters()\n",
    "              if(agent.epoch_count % 100 == 0):\n",
    "                self.log(f'Epoch: {agent.epoch_count}, loss:{loss}')\n",
    "\n",
    "          if done:\n",
    "              self.log(f'Game over after {step_index} steps')\n",
    "          current_state = next_state\n",
    "          step_index += 1\n",
    "\n",
    "      return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e70e5",
   "metadata": {
    "id": "G40Gtt_v878L",
    "papermill": {
     "duration": 0.017289,
     "end_time": "2023-11-21T08:43:49.466261",
     "exception": false,
     "start_time": "2023-11-21T08:43:49.448972",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Nous allons donc entraîner les deux agents avec les réseaux `DQN` et `DuellingDQN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0e3e606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:43:49.502288Z",
     "iopub.status.busy": "2023-11-21T08:43:49.501863Z",
     "iopub.status.idle": "2023-11-21T10:43:53.321329Z",
     "shell.execute_reply": "2023-11-21T10:43:53.319969Z"
    },
    "id": "As5XInr-2l7B",
    "papermill": {
     "duration": 7203.840452,
     "end_time": "2023-11-21T10:43:53.323851",
     "exception": false,
     "start_time": "2023-11-21T08:43:49.483399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, loss:3.4664975601117476e-07\n",
      "Epoch: 200, loss:2.1057369181676222e-08\n",
      "Epoch: 300, loss:1.31805650838146e-08\n",
      "Game over after 340 steps\n",
      "Episode 1 is done\n",
      "Total reward 0.0\n",
      "Epoch: 400, loss:1.9409735330100375e-07\n",
      "Epoch: 500, loss:6.986111111473292e-05\n",
      "Game over after 240 steps\n",
      "Episode 2 is done\n",
      "Total reward 2.0\n",
      "Epoch: 600, loss:0.030772164463996887\n",
      "Epoch: 700, loss:0.00020355288870632648\n",
      "Epoch: 800, loss:0.00023392256116494536\n",
      "Game over after 244 steps\n",
      "Episode 3 is done\n",
      "Total reward 3.0\n",
      "Epoch: 900, loss:3.1305815355153754e-05\n",
      "Game over after 121 steps\n",
      "Episode 4 is done\n",
      "Total reward 0.0\n",
      "Epoch: 1000, loss:0.06249690055847168\n",
      "Epoch: 1100, loss:4.437132156454027e-05\n",
      "Game over after 222 steps\n",
      "Episode 5 is done\n",
      "Total reward 2.0\n",
      "Epoch: 1200, loss:0.030683649703860283\n",
      "Game over after 121 steps\n",
      "Episode 6 is done\n",
      "Total reward 0.0\n",
      "Epoch: 1300, loss:0.00012030199286527932\n",
      "Epoch: 1400, loss:1.5140604773478117e-05\n",
      "Game over after 199 steps\n",
      "Episode 7 is done\n",
      "Total reward 2.0\n",
      "Epoch: 1500, loss:0.00016369203513022512\n",
      "Epoch: 1600, loss:0.0003759701212402433\n",
      "Epoch: 1700, loss:2.7689875423675403e-05\n",
      "Game over after 263 steps\n",
      "Episode 8 is done\n",
      "Total reward 4.0\n",
      "Epoch: 1800, loss:0.030948730185627937\n",
      "Epoch: 1900, loss:0.00010017268505180255\n",
      "Epoch: 2000, loss:8.383687963942066e-05\n",
      "Game over after 316 steps\n",
      "Episode 9 is done\n",
      "Total reward 5.0\n",
      "Epoch: 2100, loss:4.808320227311924e-05\n",
      "Epoch: 2200, loss:0.00015008129412308335\n",
      "Game over after 149 steps\n",
      "Episode 10 is done\n",
      "Total reward 1.0\n",
      "Epoch: 2300, loss:0.0023544346913695335\n",
      "Game over after 132 steps\n",
      "Episode 11 is done\n",
      "Total reward 0.0\n",
      "Epoch: 2400, loss:9.37221193453297e-05\n",
      "Game over after 121 steps\n",
      "Episode 12 is done\n",
      "Total reward 0.0\n",
      "Epoch: 2500, loss:0.0003304054553154856\n",
      "Epoch: 2600, loss:0.06141223758459091\n",
      "Game over after 121 steps\n",
      "Episode 13 is done\n",
      "Total reward 0.0\n",
      "Epoch: 2700, loss:4.0566646930528805e-05\n",
      "Game over after 187 steps\n",
      "Episode 14 is done\n",
      "Total reward 1.0\n",
      "Epoch: 2800, loss:5.054963185102679e-05\n",
      "Epoch: 2900, loss:6.16846518823877e-05\n",
      "Game over after 149 steps\n",
      "Episode 15 is done\n",
      "Total reward 0.0\n",
      "Epoch: 3000, loss:0.03096981719136238\n",
      "Epoch: 3100, loss:6.196824688231573e-05\n",
      "Game over after 215 steps\n",
      "Episode 16 is done\n",
      "Total reward 2.0\n",
      "Epoch: 3200, loss:3.1943811336532235e-05\n",
      "Epoch: 3300, loss:0.0001577704824740067\n",
      "Game over after 156 steps\n",
      "Episode 17 is done\n",
      "Total reward 1.0\n",
      "Epoch: 3400, loss:0.030989468097686768\n",
      "Game over after 164 steps\n",
      "Episode 18 is done\n",
      "Total reward 1.0\n",
      "Epoch: 3500, loss:0.030887549743056297\n",
      "Epoch: 3600, loss:0.030658001080155373\n",
      "Epoch: 3700, loss:0.00010126896813744679\n",
      "Game over after 246 steps\n",
      "Episode 19 is done\n",
      "Total reward 2.0\n",
      "Epoch: 3800, loss:0.0004937064368277788\n",
      "Epoch: 3900, loss:6.53214447083883e-05\n",
      "Game over after 193 steps\n",
      "Episode 20 is done\n",
      "Total reward 2.0\n",
      "Epoch: 4000, loss:0.00011927737068617716\n",
      "Game over after 167 steps\n",
      "Episode 21 is done\n",
      "Total reward 1.0\n",
      "Epoch: 4100, loss:6.401637801900506e-05\n",
      "Epoch: 4200, loss:5.089046317152679e-05\n",
      "Game over after 200 steps\n",
      "Episode 22 is done\n",
      "Total reward 2.0\n",
      "Epoch: 4300, loss:0.03132517635822296\n",
      "Epoch: 4400, loss:0.03107461892068386\n",
      "Epoch: 4500, loss:6.035429032635875e-05\n",
      "Game over after 257 steps\n",
      "Episode 23 is done\n",
      "Total reward 2.0\n",
      "Epoch: 4600, loss:2.83479894278571e-05\n",
      "Epoch: 4700, loss:0.00023736077127978206\n",
      "Game over after 213 steps\n",
      "Episode 24 is done\n",
      "Total reward 2.0\n",
      "Epoch: 4800, loss:3.730805474333465e-05\n",
      "Epoch: 4900, loss:0.030889851972460747\n",
      "Game over after 207 steps\n",
      "Episode 25 is done\n",
      "Total reward 1.0\n",
      "Epoch: 5000, loss:0.00022673654893878847\n",
      "Epoch: 5100, loss:2.1957284843665548e-05\n",
      "Game over after 173 steps\n",
      "Episode 26 is done\n",
      "Total reward 0.0\n",
      "Epoch: 5200, loss:0.031045986339449883\n",
      "Epoch: 5300, loss:0.0002163553872378543\n",
      "Game over after 169 steps\n",
      "Episode 27 is done\n",
      "Total reward 1.0\n",
      "Epoch: 5400, loss:0.0002525056479498744\n",
      "Epoch: 5500, loss:0.030818641185760498\n",
      "Game over after 216 steps\n",
      "Episode 28 is done\n",
      "Total reward 1.0\n",
      "Epoch: 5600, loss:2.5311837816843763e-05\n",
      "Epoch: 5700, loss:5.9468286053743213e-05\n",
      "Game over after 216 steps\n",
      "Episode 29 is done\n",
      "Total reward 2.0\n",
      "Epoch: 5800, loss:7.73002757341601e-05\n",
      "Epoch: 5900, loss:7.997779903234914e-05\n",
      "Game over after 252 steps\n",
      "Episode 30 is done\n",
      "Total reward 3.0\n",
      "Epoch: 6000, loss:5.5453794630011544e-05\n",
      "Epoch: 6100, loss:6.341516564134508e-05\n",
      "Epoch: 6200, loss:6.224833487067372e-05\n",
      "Game over after 213 steps\n",
      "Episode 31 is done\n",
      "Total reward 2.0\n",
      "Epoch: 6300, loss:9.458301065023988e-05\n",
      "Game over after 165 steps\n",
      "Episode 32 is done\n",
      "Total reward 1.0\n",
      "Epoch: 6400, loss:9.186407260131091e-05\n",
      "Epoch: 6500, loss:8.38470587041229e-05\n",
      "Epoch: 6600, loss:0.03125825151801109\n",
      "Game over after 249 steps\n",
      "Episode 33 is done\n",
      "Total reward 7.0\n",
      "Epoch: 6700, loss:7.922259828774258e-05\n",
      "Epoch: 6800, loss:4.0686158172320575e-05\n",
      "Epoch: 6900, loss:0.0004592780314851552\n",
      "Game over after 273 steps\n",
      "Episode 34 is done\n",
      "Total reward 1.0\n",
      "Epoch: 7000, loss:0.06147017329931259\n",
      "Epoch: 7100, loss:8.109096233965829e-05\n",
      "Game over after 208 steps\n",
      "Episode 35 is done\n",
      "Total reward 1.0\n",
      "Epoch: 7200, loss:0.4988248348236084\n",
      "Epoch: 7300, loss:0.5296653509140015\n",
      "Game over after 248 steps\n",
      "Episode 36 is done\n",
      "Total reward 2.0\n",
      "Epoch: 7400, loss:4.111039015697315e-05\n",
      "Epoch: 7500, loss:0.030528316274285316\n",
      "Game over after 218 steps\n",
      "Episode 37 is done\n",
      "Total reward 2.0\n",
      "Epoch: 7600, loss:0.030619574710726738\n",
      "Epoch: 7700, loss:8.583173621445894e-05\n",
      "Game over after 188 steps\n",
      "Episode 38 is done\n",
      "Total reward 2.0\n",
      "Epoch: 7800, loss:0.0005648390506394207\n",
      "Epoch: 7900, loss:9.818130638450384e-05\n",
      "Game over after 182 steps\n",
      "Episode 39 is done\n",
      "Total reward 1.0\n",
      "Epoch: 8000, loss:0.03071536310017109\n",
      "Epoch: 8100, loss:0.00011182478920090944\n",
      "Game over after 233 steps\n",
      "Episode 40 is done\n",
      "Total reward 2.0\n",
      "Epoch: 8200, loss:9.048644278664142e-05\n",
      "Epoch: 8300, loss:0.0005613736575469375\n",
      "Epoch: 8400, loss:0.030825966969132423\n",
      "Epoch: 8500, loss:0.4967195689678192\n",
      "Game over after 340 steps\n",
      "Episode 41 is done\n",
      "Total reward 3.0\n",
      "Epoch: 8600, loss:0.00016284509911201894\n",
      "Epoch: 8700, loss:1.4151498362480197e-05\n",
      "Epoch: 8800, loss:0.030365709215402603\n",
      "Game over after 273 steps\n",
      "Episode 42 is done\n",
      "Total reward 2.0\n",
      "Epoch: 8900, loss:0.00011598203127505258\n",
      "Epoch: 9000, loss:0.031229663640260696\n",
      "Game over after 280 steps\n",
      "Episode 43 is done\n",
      "Total reward 2.0\n",
      "Epoch: 9100, loss:0.00012892126687802374\n",
      "Epoch: 9200, loss:0.03074849769473076\n",
      "Epoch: 9300, loss:0.03116750158369541\n",
      "Epoch: 9400, loss:0.00011402610107325017\n",
      "Game over after 368 steps\n",
      "Episode 44 is done\n",
      "Total reward 1.0\n",
      "Epoch: 9500, loss:5.876619616174139e-05\n",
      "Epoch: 9600, loss:5.4786825785413384e-05\n",
      "Game over after 242 steps\n",
      "Episode 45 is done\n",
      "Total reward 1.0\n",
      "Epoch: 9700, loss:2.5563276722095907e-05\n",
      "Epoch: 9800, loss:1.4400790860236157e-05\n",
      "Epoch: 9900, loss:7.355110574280843e-05\n",
      "Game over after 289 steps\n",
      "Episode 46 is done\n",
      "Total reward 3.0\n",
      "Epoch: 10000, loss:5.556202449952252e-05\n",
      "Epoch: 10100, loss:5.259327735984698e-05\n",
      "Game over after 146 steps\n",
      "Episode 47 is done\n",
      "Total reward 0.0\n",
      "Epoch: 10200, loss:7.146650750655681e-05\n",
      "Epoch: 10300, loss:5.622329263132997e-05\n",
      "Game over after 190 steps\n",
      "Episode 48 is done\n",
      "Total reward 1.0\n",
      "Epoch: 10400, loss:5.516771489055827e-05\n",
      "Epoch: 10500, loss:9.505462367087603e-05\n",
      "Game over after 270 steps\n",
      "Episode 49 is done\n",
      "Total reward 4.0\n",
      "Epoch: 10600, loss:6.933849363122135e-05\n",
      "Epoch: 10700, loss:0.030934447422623634\n",
      "Game over after 128 steps\n",
      "Episode 50 is done\n",
      "Total reward 0.0\n",
      "Epoch: 10800, loss:5.71613636566326e-05\n",
      "Game over after 122 steps\n",
      "Episode 51 is done\n",
      "Total reward 0.0\n",
      "Epoch: 10900, loss:5.606596096185967e-05\n",
      "Game over after 121 steps\n",
      "Episode 52 is done\n",
      "Total reward 0.0\n",
      "Epoch: 11000, loss:0.0005675631109625101\n",
      "Epoch: 11100, loss:0.09210710227489471\n",
      "Game over after 143 steps\n",
      "Episode 53 is done\n",
      "Total reward 0.0\n",
      "Epoch: 11200, loss:0.030986515805125237\n",
      "Game over after 121 steps\n",
      "Episode 54 is done\n",
      "Total reward 0.0\n",
      "Epoch: 11300, loss:4.0414961404167116e-05\n",
      "Epoch: 11400, loss:4.247243487043306e-05\n",
      "Game over after 167 steps\n",
      "Episode 55 is done\n",
      "Total reward 0.0\n",
      "Epoch: 11500, loss:3.086391370743513e-05\n",
      "Game over after 162 steps\n",
      "Episode 56 is done\n",
      "Total reward 1.0\n",
      "Epoch: 11600, loss:3.436985571170226e-05\n",
      "Game over after 123 steps\n",
      "Episode 57 is done\n",
      "Total reward 0.0\n",
      "Epoch: 11700, loss:3.0822178814560175e-05\n",
      "Epoch: 11800, loss:0.030901316553354263\n",
      "Game over after 193 steps\n",
      "Episode 58 is done\n",
      "Total reward 2.0\n",
      "Epoch: 11900, loss:3.287456638645381e-05\n",
      "Epoch: 12000, loss:3.126634328509681e-05\n",
      "Game over after 165 steps\n",
      "Episode 59 is done\n",
      "Total reward 0.0\n",
      "Epoch: 12100, loss:2.719027725106571e-05\n",
      "Epoch: 12200, loss:2.416222559986636e-05\n",
      "Epoch: 12300, loss:0.03092365525662899\n",
      "Game over after 252 steps\n",
      "Episode 60 is done\n",
      "Total reward 3.0\n",
      "Epoch: 12400, loss:0.00010886618838412687\n",
      "Game over after 121 steps\n",
      "Episode 61 is done\n",
      "Total reward 0.0\n",
      "Epoch: 12500, loss:2.24280829570489e-05\n",
      "Epoch: 12600, loss:3.594194640754722e-05\n",
      "Game over after 179 steps\n",
      "Episode 62 is done\n",
      "Total reward 0.0\n",
      "Epoch: 12700, loss:3.141055640298873e-05\n",
      "Game over after 149 steps\n",
      "Episode 63 is done\n",
      "Total reward 1.0\n",
      "Epoch: 12800, loss:0.00013590090384241194\n",
      "Epoch: 12900, loss:1.2842878277297132e-05\n",
      "Game over after 178 steps\n",
      "Episode 64 is done\n",
      "Total reward 0.0\n",
      "Epoch: 13000, loss:1.4768653272767551e-05\n",
      "Epoch: 13100, loss:0.030944356694817543\n",
      "Game over after 211 steps\n",
      "Episode 65 is done\n",
      "Total reward 2.0\n",
      "Epoch: 13200, loss:7.309916782105574e-06\n",
      "Epoch: 13300, loss:2.7220818083151244e-05\n",
      "Epoch: 13400, loss:1.686277028056793e-05\n",
      "Epoch: 13500, loss:0.03153113275766373\n",
      "Epoch: 13600, loss:1.1726758202712517e-05\n",
      "Epoch: 13700, loss:0.0003567187814041972\n",
      "Epoch: 13800, loss:3.790650225710124e-05\n",
      "Epoch: 13900, loss:4.459706906345673e-05\n",
      "Game over after 766 steps\n",
      "Episode 66 is done\n",
      "Total reward 11.0\n",
      "Epoch: 14000, loss:4.202312265988439e-05\n",
      "Epoch: 14100, loss:0.00011806061229435727\n",
      "Epoch: 14200, loss:0.030909815803170204\n",
      "Epoch: 14300, loss:9.106720972340554e-05\n",
      "Epoch: 14400, loss:0.0001378539891447872\n",
      "Epoch: 14500, loss:0.0007258143159560859\n",
      "Game over after 657 steps\n",
      "Episode 67 is done\n",
      "Total reward 2.0\n",
      "Epoch: 14600, loss:0.0001503816747572273\n",
      "Epoch: 14700, loss:0.00010178973752772436\n",
      "Epoch: 14800, loss:0.00012107083603041247\n",
      "Game over after 291 steps\n",
      "Episode 68 is done\n",
      "Total reward 1.0\n",
      "Epoch: 14900, loss:0.030545607209205627\n",
      "Epoch: 15000, loss:7.407504017464817e-05\n",
      "Epoch: 15100, loss:9.686615521786734e-05\n",
      "Epoch: 15200, loss:9.39213641686365e-05\n",
      "Epoch: 15300, loss:6.975049473112449e-05\n",
      "Game over after 519 steps\n",
      "Episode 69 is done\n",
      "Total reward 2.0\n",
      "Epoch: 15400, loss:0.03097287379205227\n",
      "Epoch: 15500, loss:5.037467781221494e-05\n",
      "Game over after 198 steps\n",
      "Episode 70 is done\n",
      "Total reward 0.0\n",
      "Epoch: 15600, loss:5.261583282845095e-05\n",
      "Epoch: 15700, loss:3.797935278271325e-05\n",
      "Game over after 193 steps\n",
      "Episode 71 is done\n",
      "Total reward 1.0\n",
      "Epoch: 15800, loss:7.837234443286434e-05\n",
      "Epoch: 15900, loss:5.179252912057564e-05\n",
      "Epoch: 16000, loss:3.4478849556762725e-05\n",
      "Game over after 321 steps\n",
      "Episode 72 is done\n",
      "Total reward 1.0\n",
      "Epoch: 16100, loss:8.638916915515438e-05\n",
      "Epoch: 16200, loss:0.000535507861059159\n",
      "Game over after 161 steps\n",
      "Episode 73 is done\n",
      "Total reward 1.0\n",
      "Epoch: 16300, loss:0.03090059384703636\n",
      "Epoch: 16400, loss:1.88702397281304e-05\n",
      "Epoch: 16500, loss:2.8562935767695308e-05\n",
      "Epoch: 16600, loss:2.7866535674547777e-05\n",
      "Game over after 372 steps\n",
      "Episode 74 is done\n",
      "Total reward 6.0\n",
      "Epoch: 16700, loss:3.0250952477217652e-05\n",
      "Epoch: 16800, loss:5.40115506737493e-05\n",
      "Epoch: 16900, loss:0.031064704060554504\n",
      "Game over after 289 steps\n",
      "Episode 75 is done\n",
      "Total reward 3.0\n",
      "Epoch: 17000, loss:4.843009446631186e-05\n",
      "Epoch: 17100, loss:8.712645649211481e-05\n",
      "Epoch: 17200, loss:0.00047061711666174233\n",
      "Game over after 359 steps\n",
      "Episode 76 is done\n",
      "Total reward 3.0\n",
      "Epoch: 17300, loss:8.446568972431123e-05\n",
      "Epoch: 17400, loss:8.521515701431781e-05\n",
      "Epoch: 17500, loss:4.72114043077454e-05\n",
      "Epoch: 17600, loss:8.389772847294807e-05\n",
      "Game over after 356 steps\n",
      "Episode 77 is done\n",
      "Total reward 2.0\n",
      "Epoch: 17700, loss:0.030744358897209167\n",
      "Epoch: 17800, loss:5.737136598327197e-05\n",
      "Game over after 244 steps\n",
      "Episode 78 is done\n",
      "Total reward 2.0\n",
      "Epoch: 17900, loss:6.863312592031434e-05\n",
      "Epoch: 18000, loss:0.03092251345515251\n",
      "Epoch: 18100, loss:0.06226638704538345\n",
      "Game over after 299 steps\n",
      "Episode 79 is done\n",
      "Total reward 4.0\n",
      "Epoch: 18200, loss:7.751934754196554e-05\n",
      "Epoch: 18300, loss:0.00014409532013814896\n",
      "Game over after 199 steps\n",
      "Episode 80 is done\n",
      "Total reward 0.0\n",
      "Epoch: 18400, loss:7.989064033608884e-05\n",
      "Epoch: 18500, loss:0.0001331735256826505\n",
      "Epoch: 18600, loss:0.0006211721338331699\n",
      "Epoch: 18700, loss:0.0006078063743188977\n",
      "Game over after 336 steps\n",
      "Episode 81 is done\n",
      "Total reward 3.0\n",
      "Epoch: 18800, loss:0.0001300891162827611\n",
      "Epoch: 18900, loss:9.965943900169805e-05\n",
      "Game over after 216 steps\n",
      "Episode 82 is done\n",
      "Total reward 0.0\n",
      "Epoch: 19000, loss:7.760474545648322e-05\n",
      "Game over after 154 steps\n",
      "Episode 83 is done\n",
      "Total reward 1.0\n",
      "Epoch: 19100, loss:9.567328379489481e-05\n",
      "Epoch: 19200, loss:6.767525337636471e-05\n",
      "Epoch: 19300, loss:0.03103664144873619\n",
      "Game over after 267 steps\n",
      "Episode 84 is done\n",
      "Total reward 3.0\n",
      "Epoch: 19400, loss:0.03062862530350685\n",
      "Epoch: 19500, loss:4.0194805478677154e-05\n",
      "Epoch: 19600, loss:6.359162944136187e-05\n",
      "Epoch: 19700, loss:4.9540598411113024e-05\n",
      "Game over after 402 steps\n",
      "Episode 85 is done\n",
      "Total reward 2.0\n",
      "Epoch: 19800, loss:4.612511838786304e-05\n",
      "Epoch: 19900, loss:4.268161137588322e-05\n",
      "Epoch: 20000, loss:5.323636287357658e-05\n",
      "Game over after 290 steps\n",
      "Episode 86 is done\n",
      "Total reward 3.0\n",
      "Epoch: 20100, loss:8.626263297628611e-05\n",
      "Epoch: 20200, loss:9.816991951083764e-05\n",
      "Game over after 236 steps\n",
      "Episode 87 is done\n",
      "Total reward 2.0\n",
      "Epoch: 20300, loss:0.031462740153074265\n",
      "Epoch: 20400, loss:9.60011311690323e-05\n",
      "Epoch: 20500, loss:9.67604573816061e-05\n",
      "Game over after 234 steps\n",
      "Episode 88 is done\n",
      "Total reward 2.0\n",
      "Epoch: 20600, loss:6.3873223552946e-05\n",
      "Game over after 152 steps\n",
      "Episode 89 is done\n",
      "Total reward 1.0\n",
      "Epoch: 20700, loss:6.328998279059306e-05\n",
      "Epoch: 20800, loss:7.150012243073434e-05\n",
      "Epoch: 20900, loss:0.03110186569392681\n",
      "Game over after 244 steps\n",
      "Episode 90 is done\n",
      "Total reward 2.0\n",
      "Epoch: 21000, loss:7.697728142375126e-05\n",
      "Game over after 171 steps\n",
      "Episode 91 is done\n",
      "Total reward 0.0\n",
      "Epoch: 21100, loss:0.00036014270153827965\n",
      "Epoch: 21200, loss:5.26449985045474e-05\n",
      "Game over after 196 steps\n",
      "Episode 92 is done\n",
      "Total reward 2.0\n",
      "Epoch: 21300, loss:4.8067209718283266e-05\n",
      "Epoch: 21400, loss:0.0002506790915504098\n",
      "Epoch: 21500, loss:3.1520357879344374e-05\n",
      "Game over after 244 steps\n",
      "Episode 93 is done\n",
      "Total reward 2.0\n",
      "Epoch: 21600, loss:0.03079313039779663\n",
      "Epoch: 21700, loss:5.425299605121836e-05\n",
      "Epoch: 21800, loss:0.03130126744508743\n",
      "Game over after 304 steps\n",
      "Episode 94 is done\n",
      "Total reward 3.0\n",
      "Epoch: 21900, loss:7.75395383243449e-05\n",
      "Epoch: 22000, loss:0.03069130703806877\n",
      "Game over after 196 steps\n",
      "Episode 95 is done\n",
      "Total reward 2.0\n",
      "Epoch: 22100, loss:6.100048267398961e-05\n",
      "Epoch: 22200, loss:0.00043039783486165106\n",
      "Game over after 249 steps\n",
      "Episode 96 is done\n",
      "Total reward 2.0\n",
      "Epoch: 22300, loss:8.421098027611151e-05\n",
      "Epoch: 22400, loss:8.273850107798353e-05\n",
      "Game over after 197 steps\n",
      "Episode 97 is done\n",
      "Total reward 2.0\n",
      "Epoch: 22500, loss:0.03099973127245903\n",
      "Epoch: 22600, loss:5.1378170610405505e-05\n",
      "Game over after 121 steps\n",
      "Episode 98 is done\n",
      "Total reward 0.0\n",
      "Epoch: 22700, loss:0.00035790528636425734\n",
      "Game over after 164 steps\n",
      "Episode 99 is done\n",
      "Total reward 1.0\n",
      "Epoch: 22800, loss:0.03106064535677433\n",
      "Epoch: 22900, loss:4.5420005335472524e-05\n",
      "Game over after 139 steps\n",
      "Episode 100 is done\n",
      "Total reward 0.0\n",
      "Epoch: 23000, loss:5.109573612571694e-05\n",
      "Epoch: 23100, loss:2.236449836345855e-05\n",
      "Game over after 231 steps\n",
      "Episode 101 is done\n",
      "Total reward 2.0\n",
      "Epoch: 23200, loss:0.00020403614325914532\n",
      "Epoch: 23300, loss:1.2216725735925138e-05\n",
      "Game over after 167 steps\n",
      "Episode 102 is done\n",
      "Total reward 1.0\n",
      "Epoch: 23400, loss:2.4871989808161743e-05\n",
      "Epoch: 23500, loss:2.1704359824070707e-05\n",
      "Game over after 280 steps\n",
      "Episode 103 is done\n",
      "Total reward 2.0\n",
      "Epoch: 23600, loss:6.666082481388003e-05\n",
      "Epoch: 23700, loss:5.301596684148535e-05\n",
      "Epoch: 23800, loss:0.0003595786984078586\n",
      "Game over after 288 steps\n",
      "Episode 104 is done\n",
      "Total reward 2.0\n",
      "Epoch: 23900, loss:0.00037595556932501495\n",
      "Epoch: 24000, loss:0.00043402815936133265\n",
      "Epoch: 24100, loss:0.03103598579764366\n",
      "Game over after 288 steps\n",
      "Episode 105 is done\n",
      "Total reward 2.0\n",
      "Epoch: 24200, loss:0.031015358865261078\n",
      "Epoch: 24300, loss:3.238319914089516e-05\n",
      "Epoch: 24400, loss:4.9653328460408375e-05\n",
      "Game over after 303 steps\n",
      "Episode 106 is done\n",
      "Total reward 0.0\n",
      "Epoch: 24500, loss:0.00020801948267035186\n",
      "Epoch: 24600, loss:4.6473938709823415e-05\n",
      "Epoch: 24700, loss:0.030787281692028046\n",
      "Game over after 262 steps\n",
      "Episode 107 is done\n",
      "Total reward 2.0\n",
      "Epoch: 24800, loss:5.44146096217446e-05\n",
      "Epoch: 24900, loss:0.030995342880487442\n",
      "Game over after 215 steps\n",
      "Episode 108 is done\n",
      "Total reward 2.0\n",
      "Epoch: 25000, loss:3.881835436914116e-05\n",
      "Epoch: 25100, loss:5.776275065727532e-05\n",
      "Epoch: 25200, loss:8.267291559604928e-05\n",
      "Epoch: 25300, loss:0.03047403134405613\n",
      "Game over after 361 steps\n",
      "Episode 109 is done\n",
      "Total reward 4.0\n",
      "Epoch: 25400, loss:1.3039073564868886e-05\n",
      "Epoch: 25500, loss:0.0005565216415561736\n",
      "Epoch: 25600, loss:0.03091449849307537\n",
      "Epoch: 25700, loss:8.978645200841129e-05\n",
      "Game over after 456 steps\n",
      "Episode 110 is done\n",
      "Total reward 7.0\n",
      "Epoch: 25800, loss:7.145059498725459e-05\n",
      "Epoch: 25900, loss:0.0005341833457350731\n",
      "Game over after 176 steps\n",
      "Episode 111 is done\n",
      "Total reward 1.0\n",
      "Epoch: 26000, loss:7.12799810571596e-05\n",
      "Epoch: 26100, loss:6.32817464065738e-05\n",
      "Game over after 170 steps\n",
      "Episode 112 is done\n",
      "Total reward 1.0\n",
      "Epoch: 26200, loss:0.03070095367729664\n",
      "Game over after 171 steps\n",
      "Episode 113 is done\n",
      "Total reward 1.0\n",
      "Epoch: 26300, loss:0.03155365213751793\n",
      "Epoch: 26400, loss:0.00011711620754795149\n",
      "Game over after 122 steps\n",
      "Episode 114 is done\n",
      "Total reward 0.0\n",
      "Epoch: 26500, loss:9.33157280087471e-05\n",
      "Game over after 122 steps\n",
      "Episode 115 is done\n",
      "Total reward 0.0\n",
      "Epoch: 26600, loss:0.0001559544907649979\n",
      "Game over after 121 steps\n",
      "Episode 116 is done\n",
      "Total reward 0.0\n",
      "Epoch: 26700, loss:0.00015347270527854562\n",
      "Epoch: 26800, loss:0.00013831474643666297\n",
      "Game over after 218 steps\n",
      "Episode 117 is done\n",
      "Total reward 1.0\n",
      "Epoch: 26900, loss:0.00012764455459546298\n",
      "Epoch: 27000, loss:0.0007221779669634998\n",
      "Epoch: 27100, loss:0.000657561351545155\n",
      "Game over after 258 steps\n",
      "Episode 118 is done\n",
      "Total reward 3.0\n",
      "Epoch: 27200, loss:0.00016123631212394685\n",
      "Epoch: 27300, loss:0.00010375944839324802\n",
      "Game over after 161 steps\n",
      "Episode 119 is done\n",
      "Total reward 0.0\n",
      "Epoch: 27400, loss:0.030711887404322624\n",
      "Epoch: 27500, loss:8.938635437516496e-05\n",
      "Game over after 241 steps\n",
      "Episode 120 is done\n",
      "Total reward 3.0\n",
      "Epoch: 27600, loss:7.431025733239949e-05\n",
      "Epoch: 27700, loss:0.00037148024421185255\n",
      "Game over after 201 steps\n",
      "Episode 121 is done\n",
      "Total reward 1.0\n",
      "Epoch: 27800, loss:0.03124081902205944\n",
      "Epoch: 27900, loss:6.78214491927065e-05\n",
      "Epoch: 28000, loss:5.1130355132045224e-05\n",
      "Game over after 265 steps\n",
      "Episode 122 is done\n",
      "Total reward 2.0\n",
      "Epoch: 28100, loss:0.00011932655615964904\n",
      "Epoch: 28200, loss:0.00011318708857288584\n",
      "Epoch: 28300, loss:7.554521289421245e-05\n",
      "Game over after 328 steps\n",
      "Episode 123 is done\n",
      "Total reward 2.0\n",
      "Epoch: 28400, loss:5.198804137762636e-05\n",
      "Epoch: 28500, loss:0.0003005296166520566\n",
      "Epoch: 28600, loss:0.030787287279963493\n",
      "Epoch: 28700, loss:5.00665555591695e-05\n",
      "Game over after 374 steps\n",
      "Episode 124 is done\n",
      "Total reward 2.0\n",
      "Epoch: 28800, loss:5.698780660168268e-05\n",
      "Epoch: 28900, loss:1.8775004718918353e-05\n",
      "Epoch: 29000, loss:3.3641597838141024e-05\n",
      "Game over after 320 steps\n",
      "Episode 125 is done\n",
      "Total reward 3.0\n",
      "Epoch: 29100, loss:0.00040476536378264427\n",
      "Epoch: 29200, loss:0.00042283840593881905\n",
      "Epoch: 29300, loss:4.8876681830734015e-05\n",
      "Epoch: 29400, loss:5.8624500525183976e-05\n",
      "Epoch: 29500, loss:6.915585254319012e-05\n",
      "Game over after 481 steps\n",
      "Episode 126 is done\n",
      "Total reward 1.0\n",
      "Epoch: 29600, loss:0.03066154569387436\n",
      "Epoch: 29700, loss:0.030627110973000526\n",
      "Game over after 218 steps\n",
      "Episode 127 is done\n",
      "Total reward 2.0\n",
      "Epoch: 29800, loss:5.705639341613278e-05\n",
      "Game over after 121 steps\n",
      "Episode 128 is done\n",
      "Total reward 0.0\n",
      "Epoch: 29900, loss:7.223844295367599e-05\n",
      "Epoch: 30000, loss:7.807831570971757e-05\n",
      "Epoch: 30100, loss:0.030984017997980118\n",
      "Game over after 277 steps\n",
      "Episode 129 is done\n",
      "Total reward 2.0\n",
      "Epoch: 30200, loss:5.4902484407648444e-05\n",
      "Epoch: 30300, loss:6.692856550216675e-05\n",
      "Epoch: 30400, loss:0.030750535428524017\n",
      "Game over after 279 steps\n",
      "Episode 130 is done\n",
      "Total reward 3.0\n",
      "Epoch: 30500, loss:4.2704210500232875e-05\n",
      "Epoch: 30600, loss:4.883284418610856e-05\n",
      "Game over after 225 steps\n",
      "Episode 131 is done\n",
      "Total reward 3.0\n",
      "Epoch: 30700, loss:0.03065655194222927\n",
      "Epoch: 30800, loss:4.176620495854877e-05\n",
      "Epoch: 30900, loss:5.24378010595683e-05\n",
      "Game over after 259 steps\n",
      "Episode 132 is done\n",
      "Total reward 2.0\n",
      "Epoch: 31000, loss:5.145026807440445e-05\n",
      "Epoch: 31100, loss:3.9761114749126136e-05\n",
      "Epoch: 31200, loss:3.906110214302316e-05\n",
      "Epoch: 31300, loss:0.031013067811727524\n",
      "Game over after 429 steps\n",
      "Episode 133 is done\n",
      "Total reward 3.0\n",
      "Epoch: 31400, loss:0.00022144365357235074\n",
      "Epoch: 31500, loss:4.284211900085211e-05\n",
      "Epoch: 31600, loss:7.247351459227502e-05\n",
      "Epoch: 31700, loss:6.32248556939885e-05\n",
      "Game over after 452 steps\n",
      "Episode 134 is done\n",
      "Total reward 3.0\n",
      "Epoch: 31800, loss:6.080485036363825e-05\n",
      "Epoch: 31900, loss:3.161508357152343e-05\n",
      "Epoch: 32000, loss:0.00031915525323711336\n",
      "Epoch: 32100, loss:7.60464754421264e-05\n",
      "Epoch: 32200, loss:0.00035493605537340045\n",
      "Game over after 490 steps\n",
      "Episode 135 is done\n",
      "Total reward 4.0\n",
      "Epoch: 32300, loss:4.9085392674896866e-05\n",
      "Epoch: 32400, loss:1.1468272532511037e-05\n",
      "Epoch: 32500, loss:8.545922173652798e-05\n",
      "Epoch: 32600, loss:0.06191450357437134\n",
      "Epoch: 32700, loss:0.00011218574218219146\n",
      "Game over after 472 steps\n",
      "Episode 136 is done\n",
      "Total reward 3.0\n",
      "Epoch: 32800, loss:3.8232916267588735e-05\n",
      "Epoch: 32900, loss:0.00012877321569249034\n",
      "Epoch: 33000, loss:2.8670407118625008e-05\n",
      "Game over after 345 steps\n",
      "Episode 137 is done\n",
      "Total reward 0.0\n",
      "Epoch: 33100, loss:7.574701885459945e-05\n",
      "Epoch: 33200, loss:0.030787961557507515\n",
      "Epoch: 33300, loss:0.030935877934098244\n",
      "Epoch: 33400, loss:2.0316370864748023e-05\n",
      "Epoch: 33500, loss:4.260844434611499e-05\n",
      "Game over after 486 steps\n",
      "Episode 138 is done\n",
      "Total reward 2.0\n",
      "Epoch: 33600, loss:2.758530536084436e-05\n",
      "Epoch: 33700, loss:3.8368714740499854e-05\n",
      "Epoch: 33800, loss:0.031108105555176735\n",
      "Game over after 258 steps\n",
      "Episode 139 is done\n",
      "Total reward 0.0\n",
      "Epoch: 33900, loss:4.406800144352019e-05\n",
      "Epoch: 34000, loss:0.0002691742847673595\n",
      "Epoch: 34100, loss:3.910197119694203e-05\n",
      "Game over after 323 steps\n",
      "Episode 140 is done\n",
      "Total reward 3.0\n",
      "Epoch: 34200, loss:0.0002491008781362325\n",
      "Epoch: 34300, loss:4.349397568148561e-05\n",
      "Epoch: 34400, loss:0.00023353102733381093\n",
      "Epoch: 34500, loss:0.031222781166434288\n",
      "Game over after 420 steps\n",
      "Episode 141 is done\n",
      "Total reward 2.0\n",
      "Epoch: 34600, loss:0.03132999315857887\n",
      "Epoch: 34700, loss:2.1046254914836027e-05\n",
      "Epoch: 34800, loss:3.50001355400309e-05\n",
      "Epoch: 34900, loss:0.030667293816804886\n",
      "Game over after 345 steps\n",
      "Episode 142 is done\n",
      "Total reward 4.0\n",
      "Epoch: 35000, loss:4.058851845911704e-05\n",
      "Epoch: 35100, loss:4.772304237121716e-05\n",
      "Game over after 218 steps\n",
      "Episode 143 is done\n",
      "Total reward 2.0\n",
      "Epoch: 35200, loss:6.34559546597302e-05\n",
      "Epoch: 35300, loss:6.887940980959684e-05\n",
      "Game over after 149 steps\n",
      "Episode 144 is done\n",
      "Total reward 1.0\n",
      "Epoch: 35400, loss:0.03073502518236637\n",
      "Epoch: 35500, loss:8.579163841204718e-05\n",
      "Game over after 250 steps\n",
      "Episode 145 is done\n",
      "Total reward 3.0\n",
      "Epoch: 35600, loss:5.4919644753681496e-05\n",
      "Epoch: 35700, loss:6.699916411889717e-05\n",
      "Game over after 196 steps\n",
      "Episode 146 is done\n",
      "Total reward 2.0\n",
      "Epoch: 35800, loss:0.061101965606212616\n",
      "Game over after 121 steps\n",
      "Episode 147 is done\n",
      "Total reward 0.0\n",
      "Epoch: 35900, loss:0.03061755560338497\n",
      "Game over after 121 steps\n",
      "Episode 148 is done\n",
      "Total reward 0.0\n",
      "Epoch: 36000, loss:7.863489736337215e-05\n",
      "Epoch: 36100, loss:7.769571675453335e-05\n",
      "Epoch: 36200, loss:0.030691811814904213\n",
      "Game over after 277 steps\n",
      "Episode 149 is done\n",
      "Total reward 3.0\n",
      "Epoch: 36300, loss:0.00010008655226556584\n",
      "Epoch: 36400, loss:8.477331721223891e-05\n",
      "Game over after 196 steps\n",
      "Episode 150 is done\n",
      "Total reward 2.0\n",
      "Epoch: 36500, loss:0.030948784202337265\n",
      "TIME OUT: Model stops training after 3600.22 seconds, Save model to simple_dqn.pt\n",
      "Episode 151 is done\n",
      "Total reward 0.0\n",
      "Game over after 168 steps\n",
      "Saving simple_dqn.gif\n",
      "Epoch: 100, loss:4.359173715329234e-07\n",
      "Game over after 182 steps\n",
      "Episode 1 is done\n",
      "Total reward 0.0\n",
      "Epoch: 200, loss:2.2810321809174638e-07\n",
      "Epoch: 300, loss:5.172578676138073e-05\n",
      "Epoch: 400, loss:0.00012447383778635412\n",
      "Game over after 228 steps\n",
      "Episode 2 is done\n",
      "Total reward 1.0\n",
      "Epoch: 500, loss:1.4996387108112685e-05\n",
      "Epoch: 600, loss:6.741989182046382e-06\n",
      "Game over after 254 steps\n",
      "Episode 3 is done\n",
      "Total reward 0.0\n",
      "Epoch: 700, loss:2.871338665499934e-06\n",
      "Epoch: 800, loss:5.895120011700783e-06\n",
      "Epoch: 900, loss:6.841126833023736e-06\n",
      "Epoch: 1000, loss:9.782741472008638e-06\n",
      "Game over after 395 steps\n",
      "Episode 4 is done\n",
      "Total reward 0.0\n",
      "Epoch: 1100, loss:2.06229992727458e-06\n",
      "Epoch: 1200, loss:1.2432312814780744e-06\n",
      "Epoch: 1300, loss:2.333177235414041e-06\n",
      "Epoch: 1400, loss:6.268909601203632e-06\n",
      "Game over after 407 steps\n",
      "Episode 5 is done\n",
      "Total reward 3.0\n",
      "Epoch: 1500, loss:1.3966999176773243e-05\n",
      "Epoch: 1600, loss:1.5403089491883293e-05\n",
      "Epoch: 1700, loss:2.1030607967986725e-05\n",
      "Epoch: 1800, loss:8.171698027581442e-06\n",
      "Game over after 331 steps\n",
      "Episode 6 is done\n",
      "Total reward 2.0\n",
      "Epoch: 1900, loss:0.00022504606749862432\n",
      "Epoch: 2000, loss:0.00017476340872235596\n",
      "Epoch: 2100, loss:2.549574855947867e-05\n",
      "Game over after 329 steps\n",
      "Episode 7 is done\n",
      "Total reward 0.0\n",
      "Epoch: 2200, loss:0.0001879295305116102\n",
      "Epoch: 2300, loss:6.673985626548529e-05\n",
      "Epoch: 2400, loss:1.3825351743435021e-05\n",
      "Epoch: 2500, loss:3.156649472657591e-05\n",
      "Epoch: 2600, loss:0.03112056478857994\n",
      "Game over after 507 steps\n",
      "Episode 8 is done\n",
      "Total reward 4.0\n",
      "Epoch: 2700, loss:0.030967416241765022\n",
      "Epoch: 2800, loss:0.0003312084299977869\n",
      "Epoch: 2900, loss:6.237860361579806e-05\n",
      "Epoch: 3000, loss:5.30713441548869e-05\n",
      "Game over after 448 steps\n",
      "Episode 9 is done\n",
      "Total reward 1.0\n",
      "Epoch: 3100, loss:4.6640332584502175e-05\n",
      "Epoch: 3200, loss:2.501839117030613e-05\n",
      "Epoch: 3300, loss:3.2893720344873145e-05\n",
      "Game over after 270 steps\n",
      "Episode 10 is done\n",
      "Total reward 2.0\n",
      "Epoch: 3400, loss:1.189478916785447e-05\n",
      "Epoch: 3500, loss:1.9863540728692897e-05\n",
      "Epoch: 3600, loss:1.570306994835846e-05\n",
      "Game over after 279 steps\n",
      "Episode 11 is done\n",
      "Total reward 1.0\n",
      "Epoch: 3700, loss:3.499193553579971e-05\n",
      "Epoch: 3800, loss:6.123796629253775e-05\n",
      "Epoch: 3900, loss:0.00028439555899240077\n",
      "Epoch: 4000, loss:4.6671753807459027e-05\n",
      "Game over after 427 steps\n",
      "Episode 12 is done\n",
      "Total reward 5.0\n",
      "Epoch: 4100, loss:4.980832090950571e-05\n",
      "Epoch: 4200, loss:6.027272320352495e-05\n",
      "Epoch: 4300, loss:0.030578404664993286\n",
      "Epoch: 4400, loss:5.607673665508628e-05\n",
      "Game over after 350 steps\n",
      "Episode 13 is done\n",
      "Total reward 3.0\n",
      "Epoch: 4500, loss:0.030711231753230095\n",
      "Epoch: 4600, loss:4.4411488488549367e-05\n",
      "Epoch: 4700, loss:0.031070614233613014\n",
      "Game over after 296 steps\n",
      "Episode 14 is done\n",
      "Total reward 0.0\n",
      "Epoch: 4800, loss:0.00012453780800569803\n",
      "Epoch: 4900, loss:3.832461152342148e-05\n",
      "Game over after 249 steps\n",
      "Episode 15 is done\n",
      "Total reward 3.0\n",
      "Epoch: 5000, loss:0.030995167791843414\n",
      "Epoch: 5100, loss:5.713690188713372e-05\n",
      "Epoch: 5200, loss:0.030682967975735664\n",
      "Game over after 328 steps\n",
      "Episode 16 is done\n",
      "Total reward 5.0\n",
      "Epoch: 5300, loss:8.60569707583636e-05\n",
      "Epoch: 5400, loss:0.00011359676136635244\n",
      "Game over after 121 steps\n",
      "Episode 17 is done\n",
      "Total reward 0.0\n",
      "Epoch: 5500, loss:0.0001370576210319996\n",
      "Epoch: 5600, loss:0.0005241401959210634\n",
      "Game over after 257 steps\n",
      "Episode 18 is done\n",
      "Total reward 1.0\n",
      "Epoch: 5700, loss:0.03143548220396042\n",
      "Epoch: 5800, loss:0.030928073450922966\n",
      "Epoch: 5900, loss:7.26403814041987e-05\n",
      "Game over after 309 steps\n",
      "Episode 19 is done\n",
      "Total reward 3.0\n",
      "Epoch: 6000, loss:0.00012381098349578679\n",
      "Epoch: 6100, loss:0.031218212097883224\n",
      "Epoch: 6200, loss:0.00013104890240356326\n",
      "Epoch: 6300, loss:0.00011943047866225243\n",
      "Game over after 385 steps\n",
      "Episode 20 is done\n",
      "Total reward 3.0\n",
      "Epoch: 6400, loss:9.026608313433826e-05\n",
      "Epoch: 6500, loss:3.034107430721633e-05\n",
      "Epoch: 6600, loss:7.533549796789885e-05\n",
      "Game over after 305 steps\n",
      "Episode 21 is done\n",
      "Total reward 1.0\n",
      "Epoch: 6700, loss:7.409961835946888e-05\n",
      "Epoch: 6800, loss:7.55413857405074e-05\n",
      "Epoch: 6900, loss:0.0002559682761784643\n",
      "Game over after 279 steps\n",
      "Episode 22 is done\n",
      "Total reward 2.0\n",
      "Epoch: 7000, loss:4.60904702777043e-05\n",
      "Epoch: 7100, loss:0.0001383328635711223\n",
      "Epoch: 7200, loss:0.001064939540810883\n",
      "Game over after 309 steps\n",
      "Episode 23 is done\n",
      "Total reward 3.0\n",
      "Epoch: 7300, loss:9.027316264109686e-05\n",
      "Epoch: 7400, loss:4.858523607254028e-05\n",
      "Epoch: 7500, loss:8.449514280073345e-05\n",
      "Game over after 330 steps\n",
      "Episode 24 is done\n",
      "Total reward 4.0\n",
      "Epoch: 7600, loss:8.341984357684851e-05\n",
      "Epoch: 7700, loss:4.344620174379088e-05\n",
      "Epoch: 7800, loss:0.03137122467160225\n",
      "Epoch: 7900, loss:8.794914174359292e-05\n",
      "Game over after 312 steps\n",
      "Episode 25 is done\n",
      "Total reward 4.0\n",
      "Epoch: 8000, loss:0.031066343188285828\n",
      "Epoch: 8100, loss:0.00010367539653088897\n",
      "Epoch: 8200, loss:3.730390017153695e-05\n",
      "Game over after 335 steps\n",
      "Episode 26 is done\n",
      "Total reward 2.0\n",
      "Epoch: 8300, loss:2.9507846193155274e-05\n",
      "Game over after 121 steps\n",
      "Episode 27 is done\n",
      "Total reward 0.0\n",
      "Epoch: 8400, loss:3.77848009520676e-05\n",
      "Epoch: 8500, loss:0.0005439702654257417\n",
      "Game over after 183 steps\n",
      "Episode 28 is done\n",
      "Total reward 0.0\n",
      "Epoch: 8600, loss:5.454271740745753e-05\n",
      "Epoch: 8700, loss:8.19179040263407e-05\n",
      "Epoch: 8800, loss:7.655734225409105e-05\n",
      "Epoch: 8900, loss:0.031225210055708885\n",
      "Epoch: 9000, loss:0.06160170957446098\n",
      "Game over after 482 steps\n",
      "Episode 29 is done\n",
      "Total reward 3.0\n",
      "Epoch: 9100, loss:7.061400538077578e-05\n",
      "Epoch: 9200, loss:6.635280442424119e-05\n",
      "Epoch: 9300, loss:0.030858129262924194\n",
      "Epoch: 9400, loss:5.2324739954201505e-05\n",
      "Game over after 396 steps\n",
      "Episode 30 is done\n",
      "Total reward 3.0\n",
      "Epoch: 9500, loss:6.05450950388331e-05\n",
      "Epoch: 9600, loss:3.467108035692945e-05\n",
      "Epoch: 9700, loss:3.835953975794837e-05\n",
      "Epoch: 9800, loss:0.03096233867108822\n",
      "Game over after 458 steps\n",
      "Episode 31 is done\n",
      "Total reward 3.0\n",
      "Epoch: 9900, loss:3.933063999284059e-05\n",
      "Epoch: 10000, loss:0.030963832512497902\n",
      "Game over after 143 steps\n",
      "Episode 32 is done\n",
      "Total reward 0.0\n",
      "Epoch: 10100, loss:0.00024349679006263614\n",
      "Epoch: 10200, loss:1.8506274500396103e-05\n",
      "Epoch: 10300, loss:2.5203689801855944e-05\n",
      "Epoch: 10400, loss:2.7061027140007354e-05\n",
      "Epoch: 10500, loss:2.222213697677944e-05\n",
      "Game over after 486 steps\n",
      "Episode 33 is done\n",
      "Total reward 4.0\n",
      "Epoch: 10600, loss:3.312939952593297e-05\n",
      "Epoch: 10700, loss:7.55626751924865e-05\n",
      "Epoch: 10800, loss:3.363515133969486e-05\n",
      "Epoch: 10900, loss:0.00040609994903206825\n",
      "Game over after 431 steps\n",
      "Episode 34 is done\n",
      "Total reward 7.0\n",
      "Epoch: 11000, loss:3.500963794067502e-05\n",
      "Epoch: 11100, loss:0.03067825734615326\n",
      "Epoch: 11200, loss:0.00021575664868578315\n",
      "Game over after 267 steps\n",
      "Episode 35 is done\n",
      "Total reward 1.0\n",
      "Epoch: 11300, loss:0.00018952685059048235\n",
      "Epoch: 11400, loss:6.911531090736389e-05\n",
      "Epoch: 11500, loss:7.961095252539963e-05\n",
      "Epoch: 11600, loss:0.0008196980343200266\n",
      "Epoch: 11700, loss:0.030931169167160988\n",
      "Epoch: 11800, loss:7.760477456031367e-05\n",
      "Game over after 629 steps\n",
      "Episode 36 is done\n",
      "Total reward 1.0\n",
      "Epoch: 11900, loss:7.335367263294756e-05\n",
      "Epoch: 12000, loss:0.0001496443583164364\n",
      "Epoch: 12100, loss:7.433639257214963e-05\n",
      "Epoch: 12200, loss:7.40674149710685e-05\n",
      "Game over after 408 steps\n",
      "Episode 37 is done\n",
      "Total reward 3.0\n",
      "Epoch: 12300, loss:8.640014129923657e-05\n",
      "Epoch: 12400, loss:0.00010078731429530308\n",
      "Epoch: 12500, loss:0.030642004683613777\n",
      "Game over after 320 steps\n",
      "Episode 38 is done\n",
      "Total reward 2.0\n",
      "Epoch: 12600, loss:0.00022790383081883192\n",
      "Epoch: 12700, loss:9.980594768421724e-05\n",
      "Epoch: 12800, loss:0.00016079338092822582\n",
      "Epoch: 12900, loss:0.03095027431845665\n",
      "Game over after 382 steps\n",
      "Episode 39 is done\n",
      "Total reward 3.0\n",
      "Epoch: 13000, loss:8.825452823657542e-05\n",
      "Epoch: 13100, loss:0.00041913811583071947\n",
      "Game over after 192 steps\n",
      "Episode 40 is done\n",
      "Total reward 0.0\n",
      "Epoch: 13200, loss:0.030774008482694626\n",
      "Epoch: 13300, loss:0.030642202123999596\n",
      "Epoch: 13400, loss:4.923175947624259e-05\n",
      "Epoch: 13500, loss:1.8109411030309275e-05\n",
      "Epoch: 13600, loss:0.030834540724754333\n",
      "Epoch: 13700, loss:4.7142719267867506e-05\n",
      "Epoch: 13800, loss:3.266851490479894e-05\n",
      "Game over after 648 steps\n",
      "Episode 41 is done\n",
      "Total reward 4.0\n",
      "Epoch: 13900, loss:3.568157262634486e-05\n",
      "Epoch: 14000, loss:7.254142838064581e-05\n",
      "Epoch: 14100, loss:0.03043035976588726\n",
      "Epoch: 14200, loss:3.764860957744531e-05\n",
      "Game over after 395 steps\n",
      "Episode 42 is done\n",
      "Total reward 3.0\n",
      "Epoch: 14300, loss:7.876192103140056e-05\n",
      "Epoch: 14400, loss:4.143163823755458e-05\n",
      "Epoch: 14500, loss:5.7460540119791403e-05\n",
      "Game over after 361 steps\n",
      "Episode 43 is done\n",
      "Total reward 4.0\n",
      "Epoch: 14600, loss:7.901256321929395e-05\n",
      "Epoch: 14700, loss:7.3459159466438e-05\n",
      "Epoch: 14800, loss:0.00011682115291478112\n",
      "Epoch: 14900, loss:0.030766772106289864\n",
      "Epoch: 15000, loss:5.505084845935926e-05\n",
      "Epoch: 15100, loss:0.0003428361960686743\n",
      "Game over after 559 steps\n",
      "Episode 44 is done\n",
      "Total reward 1.0\n",
      "Epoch: 15200, loss:3.863100937451236e-05\n",
      "Epoch: 15300, loss:6.219744682312012e-05\n",
      "Epoch: 15400, loss:2.254521314171143e-05\n",
      "Epoch: 15500, loss:2.635920827742666e-05\n",
      "Epoch: 15600, loss:3.081879185629077e-05\n",
      "Game over after 491 steps\n",
      "Episode 45 is done\n",
      "Total reward 2.0\n",
      "Epoch: 15700, loss:6.834592204540968e-05\n",
      "Epoch: 15800, loss:4.8488702304894105e-05\n",
      "Epoch: 15900, loss:2.2554921088158153e-05\n",
      "Epoch: 16000, loss:5.2525607316056266e-05\n",
      "Epoch: 16100, loss:4.411370900925249e-05\n",
      "Epoch: 16200, loss:9.913557732943445e-05\n",
      "Epoch: 16300, loss:0.00022872185218147933\n",
      "Epoch: 16400, loss:4.0937251469586045e-05\n",
      "Game over after 875 steps\n",
      "Episode 46 is done\n",
      "Total reward 4.0\n",
      "Epoch: 16500, loss:2.8207079594722018e-05\n",
      "Epoch: 16600, loss:2.4051169020822272e-05\n",
      "Epoch: 16700, loss:2.1414605726022273e-05\n",
      "Game over after 286 steps\n",
      "Episode 47 is done\n",
      "Total reward 1.0\n",
      "Epoch: 16800, loss:0.0001527683634776622\n",
      "Epoch: 16900, loss:1.711657751002349e-05\n",
      "Epoch: 17000, loss:1.9428563973633572e-05\n",
      "Epoch: 17100, loss:2.2195912606548518e-05\n",
      "Epoch: 17200, loss:9.18436489882879e-06\n",
      "Game over after 417 steps\n",
      "Episode 48 is done\n",
      "Total reward 1.0\n",
      "Epoch: 17300, loss:7.2877669481385965e-06\n",
      "Epoch: 17400, loss:2.546956602600403e-05\n",
      "Epoch: 17500, loss:0.030734749510884285\n",
      "Game over after 391 steps\n",
      "Episode 49 is done\n",
      "Total reward 1.0\n",
      "Epoch: 17600, loss:1.532789428893011e-05\n",
      "Epoch: 17700, loss:0.03079717606306076\n",
      "Epoch: 17800, loss:2.6429286663187668e-05\n",
      "Epoch: 17900, loss:3.198949707439169e-05\n",
      "Game over after 331 steps\n",
      "Episode 50 is done\n",
      "Total reward 0.0\n",
      "Epoch: 18000, loss:4.04172606067732e-05\n",
      "Epoch: 18100, loss:0.00015420479758176953\n",
      "Epoch: 18200, loss:8.079833605734166e-06\n",
      "Epoch: 18300, loss:1.7998907424043864e-05\n",
      "Epoch: 18400, loss:2.9255090339574963e-05\n",
      "Game over after 536 steps\n",
      "Episode 51 is done\n",
      "Total reward 3.0\n",
      "Epoch: 18500, loss:2.602738641144242e-05\n",
      "Epoch: 18600, loss:4.6925575588829815e-05\n",
      "Epoch: 18700, loss:1.0167975233343896e-05\n",
      "Epoch: 18800, loss:4.024725058116019e-05\n",
      "Epoch: 18900, loss:0.031237630173563957\n",
      "Epoch: 19000, loss:2.869973104679957e-05\n",
      "Game over after 617 steps\n",
      "Episode 52 is done\n",
      "Total reward 5.0\n",
      "Epoch: 19100, loss:3.755997749976814e-05\n",
      "Epoch: 19200, loss:4.793973857886158e-05\n",
      "Epoch: 19300, loss:5.5885051551740617e-05\n",
      "Epoch: 19400, loss:3.098780507571064e-05\n",
      "Game over after 408 steps\n",
      "Episode 53 is done\n",
      "Total reward 2.0\n",
      "Epoch: 19500, loss:5.810140646644868e-05\n",
      "Epoch: 19600, loss:4.136070856475271e-05\n",
      "Epoch: 19700, loss:4.2029387259390205e-05\n",
      "Epoch: 19800, loss:3.1532475986750796e-05\n",
      "Epoch: 19900, loss:2.0649724319810048e-05\n",
      "Epoch: 20000, loss:0.00016907401732169092\n",
      "Game over after 526 steps\n",
      "Episode 54 is done\n",
      "Total reward 3.0\n",
      "Epoch: 20100, loss:0.0307310800999403\n",
      "Epoch: 20200, loss:0.03107522428035736\n",
      "Epoch: 20300, loss:4.586127761285752e-05\n",
      "Epoch: 20400, loss:0.030722076073288918\n",
      "Epoch: 20500, loss:5.7412577007198706e-05\n",
      "Game over after 543 steps\n",
      "Episode 55 is done\n",
      "Total reward 3.0\n",
      "Epoch: 20600, loss:3.076741631957702e-05\n",
      "Epoch: 20700, loss:6.555586878675967e-05\n",
      "Epoch: 20800, loss:0.03104744665324688\n",
      "Game over after 263 steps\n",
      "Episode 56 is done\n",
      "Total reward 1.0\n",
      "Epoch: 20900, loss:0.030644860118627548\n",
      "Epoch: 21000, loss:7.041757635306567e-05\n",
      "Epoch: 21100, loss:6.690651935059577e-05\n",
      "Epoch: 21200, loss:2.5356734113302082e-05\n",
      "Epoch: 21300, loss:5.1461065595503896e-05\n",
      "Epoch: 21400, loss:7.0839625550434e-05\n",
      "Game over after 604 steps\n",
      "Episode 57 is done\n",
      "Total reward 2.0\n",
      "Epoch: 21500, loss:0.0002856395731214434\n",
      "Epoch: 21600, loss:0.03126050531864166\n",
      "Epoch: 21700, loss:3.798503166763112e-05\n",
      "Epoch: 21800, loss:2.1509689759113826e-05\n",
      "Game over after 391 steps\n",
      "Episode 58 is done\n",
      "Total reward 1.0\n",
      "Epoch: 21900, loss:2.6275783966411836e-05\n",
      "Epoch: 22000, loss:1.747652277117595e-05\n",
      "Epoch: 22100, loss:0.030911259353160858\n",
      "Game over after 365 steps\n",
      "Episode 59 is done\n",
      "Total reward 1.0\n",
      "Epoch: 22200, loss:1.4315272892417852e-05\n",
      "Epoch: 22300, loss:0.030765671283006668\n",
      "Epoch: 22400, loss:1.9571412849472836e-05\n",
      "Epoch: 22500, loss:2.6138352041016333e-05\n",
      "Game over after 361 steps\n",
      "Episode 60 is done\n",
      "Total reward 1.0\n",
      "Epoch: 22600, loss:2.9395983801805414e-05\n",
      "Epoch: 22700, loss:3.1883344490779564e-05\n",
      "Epoch: 22800, loss:1.3223266250861343e-05\n",
      "Game over after 296 steps\n",
      "Episode 61 is done\n",
      "Total reward 2.0\n",
      "Epoch: 22900, loss:1.0022460628533736e-05\n",
      "Epoch: 23000, loss:1.666904063313268e-05\n",
      "Epoch: 23100, loss:0.030974164605140686\n",
      "Game over after 273 steps\n",
      "Episode 62 is done\n",
      "Total reward 2.0\n",
      "Epoch: 23200, loss:4.281994188204408e-05\n",
      "Epoch: 23300, loss:0.00030657456954941154\n",
      "Epoch: 23400, loss:7.051196007523686e-05\n",
      "Game over after 302 steps\n",
      "Episode 63 is done\n",
      "Total reward 3.0\n",
      "Epoch: 23500, loss:0.030762573704123497\n",
      "Epoch: 23600, loss:2.446437065373175e-05\n",
      "Game over after 179 steps\n",
      "Episode 64 is done\n",
      "Total reward 1.0\n",
      "Epoch: 23700, loss:5.296784365782514e-05\n",
      "Epoch: 23800, loss:2.4885435777832754e-05\n",
      "Epoch: 23900, loss:1.8614271539263427e-05\n",
      "Game over after 379 steps\n",
      "Episode 65 is done\n",
      "Total reward 4.0\n",
      "Epoch: 24000, loss:0.06202763319015503\n",
      "Epoch: 24100, loss:0.03094712272286415\n",
      "Game over after 188 steps\n",
      "Episode 66 is done\n",
      "Total reward 0.0\n",
      "Epoch: 24200, loss:3.4515749575803056e-05\n",
      "Epoch: 24300, loss:0.031094985082745552\n",
      "Game over after 144 steps\n",
      "Episode 67 is done\n",
      "Total reward 0.0\n",
      "Epoch: 24400, loss:0.030932888388633728\n",
      "Epoch: 24500, loss:0.031198102980852127\n",
      "Epoch: 24600, loss:5.261079786578193e-05\n",
      "Epoch: 24700, loss:6.0330177802825347e-05\n",
      "Game over after 411 steps\n",
      "Episode 68 is done\n",
      "Total reward 3.0\n",
      "Epoch: 24800, loss:0.030712032690644264\n",
      "Epoch: 24900, loss:0.0003371484635863453\n",
      "Epoch: 25000, loss:4.761748277815059e-05\n",
      "Game over after 318 steps\n",
      "Episode 69 is done\n",
      "Total reward 2.0\n",
      "Epoch: 25100, loss:3.197160913259722e-05\n",
      "Epoch: 25200, loss:4.5717431930825114e-05\n",
      "Epoch: 25300, loss:0.0003548033710103482\n",
      "Game over after 310 steps\n",
      "Episode 70 is done\n",
      "Total reward 3.0\n",
      "Epoch: 25400, loss:6.206782563822344e-05\n",
      "Epoch: 25500, loss:3.159423067700118e-05\n",
      "Game over after 237 steps\n",
      "Episode 71 is done\n",
      "Total reward 2.0\n",
      "Epoch: 25600, loss:4.2138952267123386e-05\n",
      "Epoch: 25700, loss:0.00011197286221431568\n",
      "Epoch: 25800, loss:6.508447404485196e-05\n",
      "Game over after 207 steps\n",
      "Episode 72 is done\n",
      "Total reward 0.0\n",
      "Epoch: 25900, loss:6.02664404141251e-05\n",
      "Epoch: 26000, loss:5.03661940456368e-05\n",
      "Epoch: 26100, loss:0.06184348091483116\n",
      "Game over after 365 steps\n",
      "Episode 73 is done\n",
      "Total reward 2.0\n",
      "Epoch: 26200, loss:4.348946094978601e-05\n",
      "Epoch: 26300, loss:5.382029848988168e-05\n",
      "Epoch: 26400, loss:5.597817653324455e-05\n",
      "Game over after 303 steps\n",
      "Episode 74 is done\n",
      "Total reward 3.0\n",
      "Epoch: 26500, loss:0.030728710815310478\n",
      "Epoch: 26600, loss:6.265132105909288e-05\n",
      "Epoch: 26700, loss:8.976334356702864e-05\n",
      "Game over after 266 steps\n",
      "Episode 75 is done\n",
      "Total reward 1.0\n",
      "Epoch: 26800, loss:5.316398528520949e-05\n",
      "Epoch: 26900, loss:4.994658593204804e-05\n",
      "Epoch: 27000, loss:8.365965186385438e-05\n",
      "Game over after 290 steps\n",
      "Episode 76 is done\n",
      "Total reward 2.0\n",
      "Epoch: 27100, loss:9.247345587937161e-05\n",
      "Epoch: 27200, loss:4.6431156079052016e-05\n",
      "Epoch: 27300, loss:3.234534233342856e-05\n",
      "Game over after 336 steps\n",
      "Episode 77 is done\n",
      "Total reward 2.0\n",
      "Epoch: 27400, loss:0.030920125544071198\n",
      "Epoch: 27500, loss:3.243327955715358e-05\n",
      "Game over after 185 steps\n",
      "Episode 78 is done\n",
      "Total reward 1.0\n",
      "Epoch: 27600, loss:4.640500628738664e-05\n",
      "Epoch: 27700, loss:2.2940850612940267e-05\n",
      "Game over after 200 steps\n",
      "Episode 79 is done\n",
      "Total reward 1.0\n",
      "Epoch: 27800, loss:5.603337558568455e-05\n",
      "Epoch: 27900, loss:4.3527215893846005e-05\n",
      "Game over after 200 steps\n",
      "Episode 80 is done\n",
      "Total reward 1.0\n",
      "Epoch: 28000, loss:5.400265581556596e-05\n",
      "Epoch: 28100, loss:0.00024080550065264106\n",
      "Epoch: 28200, loss:7.769572403049096e-05\n",
      "Epoch: 28300, loss:0.030620479956269264\n",
      "Epoch: 28400, loss:0.030128654092550278\n",
      "Game over after 530 steps\n",
      "Episode 81 is done\n",
      "Total reward 7.0\n",
      "Epoch: 28500, loss:0.00016141078958753496\n",
      "Epoch: 28600, loss:0.00010645303700584918\n",
      "Epoch: 28700, loss:0.00015562103362753987\n",
      "Game over after 295 steps\n",
      "Episode 82 is done\n",
      "Total reward 1.0\n",
      "Epoch: 28800, loss:5.7375051255803555e-05\n",
      "Epoch: 28900, loss:4.7251196519937366e-05\n",
      "Epoch: 29000, loss:4.4877437176182866e-05\n",
      "Epoch: 29100, loss:9.75567163550295e-05\n",
      "Game over after 396 steps\n",
      "Episode 83 is done\n",
      "Total reward 2.0\n",
      "Epoch: 29200, loss:0.49822911620140076\n",
      "Epoch: 29300, loss:0.00010960282816085964\n",
      "Epoch: 29400, loss:8.653888653498143e-05\n",
      "Epoch: 29500, loss:7.888635445851833e-05\n",
      "Game over after 400 steps\n",
      "Episode 84 is done\n",
      "Total reward 7.0\n",
      "Epoch: 29600, loss:0.0006730803870595992\n",
      "Epoch: 29700, loss:0.03090323694050312\n",
      "Game over after 192 steps\n",
      "Episode 85 is done\n",
      "Total reward 1.0\n",
      "Epoch: 29800, loss:0.00020395699539221823\n",
      "Epoch: 29900, loss:0.00014146679313853383\n",
      "Epoch: 30000, loss:0.00012607008102349937\n",
      "Epoch: 30100, loss:7.723704038653523e-05\n",
      "Epoch: 30200, loss:4.7370547690661624e-05\n",
      "Game over after 435 steps\n",
      "Episode 86 is done\n",
      "Total reward 3.0\n",
      "Epoch: 30300, loss:0.03048502840101719\n",
      "Epoch: 30400, loss:0.030750494450330734\n",
      "Epoch: 30500, loss:0.0001389053650200367\n",
      "Game over after 341 steps\n",
      "Episode 87 is done\n",
      "Total reward 0.0\n",
      "Epoch: 30600, loss:0.00010788260260596871\n",
      "Epoch: 30700, loss:4.739359792438336e-05\n",
      "Epoch: 30800, loss:8.801165677141398e-05\n",
      "Game over after 279 steps\n",
      "Episode 88 is done\n",
      "Total reward 3.0\n",
      "Epoch: 30900, loss:2.64130412688246e-05\n",
      "Epoch: 31000, loss:5.476088699651882e-05\n",
      "Epoch: 31100, loss:0.00012333474296610802\n",
      "Epoch: 31200, loss:8.169474313035607e-05\n",
      "Epoch: 31300, loss:5.597350173047744e-05\n",
      "Game over after 517 steps\n",
      "Episode 89 is done\n",
      "Total reward 7.0\n",
      "Epoch: 31400, loss:9.956468420568854e-05\n",
      "Epoch: 31500, loss:0.00014823040692135692\n",
      "Epoch: 31600, loss:0.00011633746180450544\n",
      "Game over after 276 steps\n",
      "Episode 90 is done\n",
      "Total reward 2.0\n",
      "Epoch: 31700, loss:8.180822624126449e-05\n",
      "Epoch: 31800, loss:6.677080818917602e-05\n",
      "Epoch: 31900, loss:0.00040143210208043456\n",
      "Game over after 369 steps\n",
      "Episode 91 is done\n",
      "Total reward 3.0\n",
      "Epoch: 32000, loss:0.0001120209417422302\n",
      "Epoch: 32100, loss:0.03136690333485603\n",
      "Epoch: 32200, loss:8.818868082016706e-05\n",
      "Epoch: 32300, loss:0.030620843172073364\n",
      "Epoch: 32400, loss:0.00014874665066599846\n",
      "Game over after 423 steps\n",
      "Episode 92 is done\n",
      "Total reward 6.0\n",
      "Epoch: 32500, loss:5.157885607331991e-05\n",
      "Epoch: 32600, loss:3.647629637271166e-05\n",
      "Epoch: 32700, loss:9.817936370382085e-05\n",
      "Epoch: 32800, loss:0.00010378445585956797\n",
      "Game over after 442 steps\n",
      "Episode 93 is done\n",
      "Total reward 4.0\n",
      "Epoch: 32900, loss:0.030637385323643684\n",
      "Epoch: 33000, loss:7.121344970073551e-05\n",
      "Epoch: 33100, loss:0.0001592631742823869\n",
      "Epoch: 33200, loss:0.030407343059778214\n",
      "Game over after 386 steps\n",
      "Episode 94 is done\n",
      "Total reward 3.0\n",
      "Epoch: 33300, loss:6.243169627850875e-05\n",
      "Epoch: 33400, loss:0.030742011964321136\n",
      "Epoch: 33500, loss:9.666490223025903e-05\n",
      "Game over after 321 steps\n",
      "Episode 95 is done\n",
      "Total reward 2.0\n",
      "Epoch: 33600, loss:9.312317706644535e-05\n",
      "Epoch: 33700, loss:0.0001151905016740784\n",
      "Epoch: 33800, loss:0.06250043213367462\n",
      "Epoch: 33900, loss:0.00011487497977213934\n",
      "Game over after 423 steps\n",
      "Episode 96 is done\n",
      "Total reward 0.0\n",
      "Epoch: 34000, loss:0.0006845842581242323\n",
      "Epoch: 34100, loss:0.030672632157802582\n",
      "Epoch: 34200, loss:9.448239143239334e-05\n",
      "Epoch: 34300, loss:6.006321927998215e-05\n",
      "Game over after 333 steps\n",
      "Episode 97 is done\n",
      "Total reward 3.0\n",
      "Epoch: 34400, loss:9.569105168338865e-05\n",
      "Epoch: 34500, loss:0.00011039335367968306\n",
      "Epoch: 34600, loss:9.210433199768886e-05\n",
      "Epoch: 34700, loss:6.400291749741882e-05\n",
      "Epoch: 34800, loss:7.603674021083862e-05\n",
      "Epoch: 34900, loss:0.03092522919178009\n",
      "Epoch: 35000, loss:0.00010507899423828349\n",
      "Epoch: 35100, loss:0.030756980180740356\n",
      "Game over after 787 steps\n",
      "Episode 98 is done\n",
      "Total reward 2.0\n",
      "Epoch: 35200, loss:0.031186753883957863\n",
      "Epoch: 35300, loss:4.072843512403779e-05\n",
      "Epoch: 35400, loss:0.03077351488173008\n",
      "Epoch: 35500, loss:4.824474308406934e-05\n",
      "Epoch: 35600, loss:7.151109457481652e-05\n",
      "Epoch: 35700, loss:8.093094947980717e-05\n",
      "Epoch: 35800, loss:0.00011872262984979898\n",
      "Game over after 719 steps\n",
      "Episode 99 is done\n",
      "Total reward 6.0\n",
      "Epoch: 35900, loss:0.0003017689159605652\n",
      "Game over after 132 steps\n",
      "Episode 100 is done\n",
      "Total reward 0.0\n",
      "Epoch: 36000, loss:3.386003299965523e-05\n",
      "Epoch: 36100, loss:3.4232722100568935e-05\n",
      "Epoch: 36200, loss:5.225339555181563e-05\n",
      "Epoch: 36300, loss:0.030835185199975967\n",
      "Game over after 410 steps\n",
      "Episode 101 is done\n",
      "Total reward 2.0\n",
      "Epoch: 36400, loss:4.618293314706534e-05\n",
      "Epoch: 36500, loss:0.00010030339035438374\n",
      "Epoch: 36600, loss:2.0204361135256477e-06\n",
      "Epoch: 36700, loss:2.4861294150468893e-05\n",
      "Epoch: 36800, loss:0.0308111310005188\n",
      "Epoch: 36900, loss:5.158985004527494e-05\n",
      "Game over after 520 steps\n",
      "Episode 102 is done\n",
      "Total reward 6.0\n",
      "Epoch: 37000, loss:0.00011028880544472486\n",
      "Epoch: 37100, loss:6.77522984915413e-05\n",
      "Epoch: 37200, loss:2.037317244685255e-05\n",
      "Epoch: 37300, loss:5.281691119307652e-05\n",
      "Game over after 482 steps\n",
      "Episode 103 is done\n",
      "Total reward 3.0\n",
      "Epoch: 37400, loss:4.131351670366712e-05\n",
      "Epoch: 37500, loss:8.480751603201497e-06\n",
      "Epoch: 37600, loss:0.0001310615480178967\n",
      "Game over after 213 steps\n",
      "Episode 104 is done\n",
      "Total reward 2.0\n",
      "Epoch: 37700, loss:0.09174388647079468\n",
      "Epoch: 37800, loss:8.435716154053807e-05\n",
      "Game over after 219 steps\n",
      "Episode 105 is done\n",
      "Total reward 2.0\n",
      "Epoch: 37900, loss:7.199815445346758e-05\n",
      "Epoch: 38000, loss:6.368196045514196e-05\n",
      "Epoch: 38100, loss:5.661788600264117e-05\n",
      "Game over after 308 steps\n",
      "Episode 106 is done\n",
      "Total reward 1.0\n",
      "Epoch: 38200, loss:0.00028221827233210206\n",
      "Epoch: 38300, loss:0.0002970714122056961\n",
      "Epoch: 38400, loss:5.5668533605057746e-05\n",
      "Epoch: 38500, loss:8.866691496223211e-05\n",
      "Game over after 436 steps\n",
      "Episode 107 is done\n",
      "Total reward 2.0\n",
      "Epoch: 38600, loss:3.557455784175545e-05\n",
      "Epoch: 38700, loss:4.985480336472392e-05\n",
      "Epoch: 38800, loss:6.639792263740674e-05\n",
      "Game over after 267 steps\n",
      "Episode 108 is done\n",
      "Total reward 1.0\n",
      "Epoch: 38900, loss:8.375367178814486e-05\n",
      "Epoch: 39000, loss:0.06081221625208855\n",
      "Game over after 190 steps\n",
      "Episode 109 is done\n",
      "Total reward 0.0\n",
      "Epoch: 39100, loss:0.061607882380485535\n",
      "Epoch: 39200, loss:6.281302194111049e-05\n",
      "Epoch: 39300, loss:3.0352262911037542e-05\n",
      "Game over after 298 steps\n",
      "Episode 110 is done\n",
      "Total reward 3.0\n",
      "Epoch: 39400, loss:5.349097409634851e-05\n",
      "Epoch: 39500, loss:0.03082701750099659\n",
      "Game over after 255 steps\n",
      "Episode 111 is done\n",
      "Total reward 1.0\n",
      "Epoch: 39600, loss:4.780394374392927e-05\n",
      "Epoch: 39700, loss:0.06216825544834137\n",
      "Epoch: 39800, loss:0.03147958219051361\n",
      "Epoch: 39900, loss:3.363684663781896e-05\n",
      "Game over after 401 steps\n",
      "Episode 112 is done\n",
      "Total reward 3.0\n",
      "Epoch: 40000, loss:3.0012146453373134e-05\n",
      "Epoch: 40100, loss:0.030828041955828667\n",
      "Epoch: 40200, loss:0.03110985830426216\n",
      "Game over after 240 steps\n",
      "Episode 113 is done\n",
      "Total reward 1.0\n",
      "Epoch: 40300, loss:0.00020463665714487433\n",
      "Epoch: 40400, loss:0.030978703871369362\n",
      "Epoch: 40500, loss:0.03081577643752098\n",
      "Epoch: 40600, loss:5.269686516840011e-05\n",
      "Epoch: 40700, loss:2.6960888135363348e-05\n",
      "Game over after 547 steps\n",
      "Episode 114 is done\n",
      "Total reward 2.0\n",
      "Epoch: 40800, loss:6.227401900105178e-05\n",
      "Epoch: 40900, loss:4.359468584880233e-05\n",
      "Epoch: 41000, loss:1.7027314243023284e-05\n",
      "TIME OUT: Model stops training after 3600.20 seconds, Save model to duelling_dqn.pt\n",
      "Episode 115 is done\n",
      "Total reward 2.0\n",
      "Game over after 121 steps\n",
      "Saving duelling_dqn.gif\n"
     ]
    }
   ],
   "source": [
    "seq_len = 5\n",
    "TIME_LIMIT = 3600\n",
    "env_name = 'ALE/Breakout-v5'\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "\n",
    "model_types = ['simple_dqn', 'duelling_dqn']\n",
    "simple_dqn = DQN(input_data.n_action, input_data.height, input_data.width,seq_len=seq_len)\n",
    "duelling_dqn = DuellingDQN(input_data.n_action, input_data.height, input_data.width,seq_len=seq_len)\n",
    "models = [simple_dqn,duelling_dqn]\n",
    "agents = []\n",
    "for model,model_type in zip(models,model_types):\n",
    "  agent = DQNAgent(env,model_file=model_type,model=model)\n",
    "  trainer = Trainer(agent,time_limit=TIME_LIMIT)\n",
    "  mean_reward = trainer.train_agent()\n",
    "  agent.clear_experiences()\n",
    "  agents.append(agent)\n",
    "  trainer.reboot_timer()\n",
    "  trainer.play(train=False,keep_frame=True)\n",
    "  agent.generate_gif(model_type+'.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089410e1",
   "metadata": {
    "id": "RnO4stmsNzA_",
    "papermill": {
     "duration": 0.113069,
     "end_time": "2023-11-21T10:43:53.549260",
     "exception": false,
     "start_time": "2023-11-21T10:43:53.436191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[](./simple_dqn.gif)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7266.07936,
   "end_time": "2023-11-21T10:43:54.804608",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-21T08:42:48.725248",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
