{"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMjeYrSqZREBB75eUehJbH5","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/Bast-94/RL-DQN/blob/dqn-draft/RL_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"# Projet de Reinforcement Learning : Deep Q-Learning sur le casse-brique d'Atari","metadata":{"id":"IxUx75ISnaC2"}},{"cell_type":"code","source":"! pip install gymnasium[\"accept-rom-license\"]\n! pip install gymnasium[\"atari\"]","metadata":{"id":"IT6J8Qb1nTG4","outputId":"7e2dfbac-e2b3-4bee-e6ed-571d6487c662","execution":{"iopub.status.busy":"2023-11-20T10:23:01.004589Z","iopub.execute_input":"2023-11-20T10:23:01.004993Z","iopub.status.idle":"2023-11-20T10:23:29.308609Z","shell.execute_reply.started":"2023-11-20T10:23:01.004961Z","shell.execute_reply":"2023-11-20T10:23:29.306590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Présentation globale du projet","metadata":{"id":"R6RsUvhwom-z"}},{"cell_type":"markdown","source":"### Objectifs du projet","metadata":{"id":"NJtQe-8OO2_1"}},{"cell_type":"markdown","source":"### Algorithme principal","metadata":{"id":"mdognuOSOxDy"}},{"cell_type":"markdown","source":"#### Pseudo code de l'algorithme","metadata":{"id":"QTVHA36NPEcA"}},{"cell_type":"markdown","source":"\n$\\text{Algorithme de Q-Leearning profond avec répétition d'expérience}$\n1. **Initialisation:**\n   - Initialiser le réseau de neurones $Q$ avec des poids aléatoires.\n   - Initialiser la mémoire de relecture $D$ avec capacité maximale $N$.\n   - Initialiser aléatoirement les paramètres d'apprentissage.\n   - Initialiser la fonction $Q$ avec des $\\theta$ aléatoire.\n   - Initialiser $\\hat{Q}$ avec $\\theta^⁻ = \\theta$.\n\n2. **Pour chaque épisode:**\n   - Initialiser l'environnement et l'état initial $s_1=\\{x_1\\}$\n   - Appliquer le prétraitement $\\phi_1 = \\phi(s_1)$\n   \n   3. **Pour chaque étape $t$ de l'épisode:**\n      - Choisir l'action $a_t$ avec la politique $\\varepsilon$-greedy\n        - $\\mathbb{P}(a_t = argmax_a(Q(s_t,a;\\theta)) = 1 - \\varepsilon$\n        - $\\mathbb{P}(a_t = \\text{random\\_sample(}A)) = \\varepsilon$\n      - Exécuter l'action $a_t$, observer la récompense $r_t$ et l'état suivant $s_{t+1}$\n      - Stocker la transition $(s_t, a_t, r_{t}, s_{t+1})$ dans la mémoire de relecture $D$\n      - Affecter $s_{t+1}=s_t,a_t,x_{t+1}$\n      - Prétraitement de $s_{t+1}$ : $\\phi_{t+1}=\\phi(s_{t+1})$\n      - Échantillonner un lot aléatoire de transitions $(s_i, a_i, r_i, s_{i+1})$ de $D$\n      - Calculer la vérité terrain $y_i$ pour chaque transition $(s_i, a_i, r_i, s_{i+1})$ en utilisant le réseau $\\hat{Q}$ aux paramètre $\\theta^-$\n      - Cloner $Q$ dans $\\hat{Q}$ toutes les $C$ étapes\n      \n","metadata":{"id":"QX8se4unQDDr"}},{"cell_type":"markdown","source":"#### Détails des variables\n- $Q$ : Fonction de qualité qui pour un couple état-action évalue à quel point une action dans un état donné est favorable.\n- $C$ : Nombre d'étapes à laquelle $\\hat{Q}$ se met à jour sur $Q$.\n- $\\hat{Q}$ : Target Network , il correspond à une version ancienne de $Q$ avec des paramètres $\\theta^-$ sur les $C$ dernières étapes.\n- $\\theta$ : Correspond aux paramètres du réseau de neurones.\n- ${A}$ : L'ensemble des actions possibles.\n- $a_t$ : L'action faite par l'agent à l'étape $t$.\n- $x_t$ : Correspond à l'image brut du jeu à l'étape $t$.\n- $s_t$ : Correspond à une séquence de couples action-image $\\{a_i \\times x_i\\}_{i\\lt t}$ .\n- $\\phi_t$ : Correspond au pré-traitement de l'état $s_t$ (Plus de détails dans la suite du notebook).\n- $\\varepsilon \\in [0,1]$ : Probabilité de choisir une action aléatoire.\n- $r_t$ : Récompense obtenue par la réalisation de l'action $a_t$ à l'instant $s_t$\n- $D$ : Mémoire de relecture.\n- $N$ : Nombre de simulations.","metadata":{"id":"9Se6OPbaO_cU"}},{"cell_type":"markdown","source":"## Démarche de recherche et implémentation","metadata":{"id":"mWhORuoko1v4"}},{"cell_type":"code","source":"from torch import nn\nimport torch\n#import gymnasium as gym\nimport gym\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport torchvision.transforms as transforms\nimport copy\nfrom time import time\nfrom collections import namedtuple","metadata":{"id":"6MpkCLnpoSIw","execution":{"iopub.status.busy":"2023-11-20T10:23:29.311654Z","iopub.execute_input":"2023-11-20T10:23:29.312110Z","iopub.status.idle":"2023-11-20T10:23:29.321689Z","shell.execute_reply.started":"2023-11-20T10:23:29.312071Z","shell.execute_reply":"2023-11-20T10:23:29.320349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Prétraitement des données","metadata":{"id":"Q78gPWyCXSAQ"}},{"cell_type":"code","source":"def preprocess_image(img_array:np.array) -> np.array :\n  transform = transforms.Compose([\n      transforms.ToPILImage(),\n      transforms.Resize((84, 84)),\n      transforms.Grayscale(num_output_channels=1),\n      transforms.ToTensor()])\n  return transform(img_array)\n","metadata":{"id":"vvsrhD0sQxqC","execution":{"iopub.status.busy":"2023-11-20T10:23:29.325233Z","iopub.execute_input":"2023-11-20T10:23:29.325635Z","iopub.status.idle":"2023-11-20T10:23:29.333531Z","shell.execute_reply.started":"2023-11-20T10:23:29.325593Z","shell.execute_reply":"2023-11-20T10:23:29.332323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = gym.make('ALE/Breakout-v5', render_mode=\"rgb_array\")\nstate_img = env.reset()[0]\nprint(env.step(1)[1:])\nfig,axes = plt.subplots(1,2)\nfig.suptitle('Images comparison')\naxes[0].set_title('Original image')\naxes[0].imshow(state_img)\naxes[0].axis('off')\nnew_img = preprocess_image(state_img)\nprint(f'{new_img.size() = }')\naxes[1].axis('off')\naxes[1].set_title('Preprocessed image')\naxes[1].imshow(new_img.permute(1,2,0))","metadata":{"id":"60aiphhbXCe_","outputId":"e23fc571-daf0-4473-e74b-32afeaa194b7","execution":{"iopub.status.busy":"2023-11-20T10:23:29.335245Z","iopub.execute_input":"2023-11-20T10:23:29.335574Z","iopub.status.idle":"2023-11-20T10:23:29.918409Z","shell.execute_reply.started":"2023-11-20T10:23:29.335546Z","shell.execute_reply":"2023-11-20T10:23:29.917256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Elaboration d'un modèle de de DQN","metadata":{"id":"0A1YB793pFc2"}},{"cell_type":"code","source":"\nclass DQN(nn.Module):\n    def __init__(self, n_action, height, width, linear_size=1024, model_name=None):\n        super(DQN, self).__init__()\n        self.input_dimension = 1, height, width\n\n        self.conv1 = nn.Conv2d(in_channels=self.input_dimension[0], out_channels=32, kernel_size=8, stride=4)\n        self.relu1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n        self.relu2 = nn.ReLU()\n        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.relu3 = nn.ReLU()\n\n        x0 = torch.zeros(1, 1, height, width)\n        x0 = self.convolute(x0)\n        x0 = self.flatten(x0)\n        flatten_dim = x0.shape[1]\n\n        self.linear1 = nn.Linear(flatten_dim, linear_size)\n        self.relu4 = nn.ReLU()\n        self.linear2 = nn.Linear(linear_size, n_action)\n\n    def flatten(self, x):\n        return x.view(x.shape[0], -1)\n\n    def convolute(self,x):\n      x = self.conv1(x)\n      x = self.relu1(x)\n      x = self.conv2(x)\n      x = self.relu2(x)\n      x = self.conv3(x)\n      x = self.relu3(x)\n      return x\n\n    def forward(self, x):\n      x = self.convolute(x)\n\n      x = self.flatten(x)\n\n      x = self.linear1(x)\n      x = self.relu4(x)\n      return self.linear2(x)","metadata":{"id":"G823PH_jIctd","execution":{"iopub.status.busy":"2023-11-20T10:23:29.922244Z","iopub.execute_input":"2023-11-20T10:23:29.922613Z","iopub.status.idle":"2023-11-20T10:23:29.937143Z","shell.execute_reply.started":"2023-11-20T10:23:29.922575Z","shell.execute_reply":"2023-11-20T10:23:29.936117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nclass DuellingDQN(nn.Module):\n    def __init__(self, n_action, height, width, linear_size=1024, model_name=None):\n        super(DuellingDQN, self).__init__()\n        self.input_dimension = 1, height, width\n        self.model_name = model_name\n        self.conv1 = nn.Conv2d(in_channels=self.input_dimension[0], out_channels=32, kernel_size=8, stride=4)\n        self.relu1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n        self.relu2 = nn.ReLU()\n        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.relu3 = nn.ReLU()\n\n        x0 = torch.zeros(1, 1, height, width)\n        x0 = self.convolute(x0)\n        x0 = self.flatten(x0)\n        flatten_dim = x0.shape[1]\n        self.value = nn.Linear(flatten_dim,1)\n        self.advantage = nn.Linear(flatten_dim,n_action)\n\n\n\n    def flatten(self, x):\n        return x.view(x.shape[0], -1)\n\n    def convolute(self,x):\n      x = self.conv1(x)\n      x = self.relu1(x)\n      x = self.conv2(x)\n      x = self.relu2(x)\n      x = self.conv3(x)\n      x = self.relu3(x)\n      return x\n\n    def forward(self, x):\n      x = self.convolute(x)\n      x = self.flatten(x)\n      value = self.value(x)\n      advantage = self.advantage(x)\n      q_value = value + (advantage - advantage.mean(dim=1,keepdim=True))\n      return q_value","metadata":{"id":"ST4yINSA3j76","execution":{"iopub.status.busy":"2023-11-20T10:23:29.938443Z","iopub.execute_input":"2023-11-20T10:23:29.939314Z","iopub.status.idle":"2023-11-20T10:23:29.953985Z","shell.execute_reply.started":"2023-11-20T10:23:29.939278Z","shell.execute_reply":"2023-11-20T10:23:29.952408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Input = namedtuple('input', ('height', 'width', 'n_action'))\n\ndef get_input_shapes(env:gym.Env):\n  env_tensor = np.zeros(env.observation_space.shape,dtype=np.uint8)\n  x = preprocess_image(env_tensor)\n  _,height, width = x.shape\n  n_action=env.action_space.n\n  return Input(height,width,n_action)\n","metadata":{"id":"Uj5w4MoD3dxR","execution":{"iopub.status.busy":"2023-11-20T10:23:29.955365Z","iopub.execute_input":"2023-11-20T10:23:29.955754Z","iopub.status.idle":"2023-11-20T10:23:29.970257Z","shell.execute_reply.started":"2023-11-20T10:23:29.955713Z","shell.execute_reply":"2023-11-20T10:23:29.969213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"testons","metadata":{"id":"bfOjRVyt_TRz"}},{"cell_type":"code","source":"env = gym.make('ALE/Breakout-v5', render_mode=\"rgb_array\")\ninput_data = get_input_shapes(env)\ndqn = DQN(input_data.n_action,input_data.height,input_data.width)\nduelling_dqn = DuellingDQN(input_data.n_action,input_data.height,input_data.width)\nx = preprocess_image(env.reset()[0])\nx = x.unsqueeze(0)\nfor model in [dqn, duelling_dqn]:\n  output = model(x)\n  assert output.size(1) == env.action_space.n, print(f'{output.size(1)} != {env.action_space.n}')\n  print(\"Les shapes de sortie du modèle sont cohérentes.\")\n  print(f'{output.size()}')","metadata":{"id":"bHL5liwk-X3B","outputId":"899378df-a821-4a93-ac72-6fb98569e2ac","execution":{"iopub.status.busy":"2023-11-20T10:23:29.971488Z","iopub.execute_input":"2023-11-20T10:23:29.971981Z","iopub.status.idle":"2023-11-20T10:23:30.303758Z","shell.execute_reply.started":"2023-11-20T10:23:29.971936Z","shell.execute_reply":"2023-11-20T10:23:30.302410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Gestion de la mémoire des expériences","metadata":{"id":"GkoNo8z40ulV"}},{"cell_type":"code","source":"GameTransition = namedtuple('game_transition', ('initial_state', 'action', 'reward','next_state', 'done'))","metadata":{"id":"-EEcI22jgs3u","execution":{"iopub.status.busy":"2023-11-20T10:23:30.305395Z","iopub.execute_input":"2023-11-20T10:23:30.305790Z","iopub.status.idle":"2023-11-20T10:23:30.311332Z","shell.execute_reply.started":"2023-11-20T10:23:30.305756Z","shell.execute_reply":"2023-11-20T10:23:30.310446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ExpStack(): # D\n  def __init__(self, max_size:int):\n    self.transitions = []\n    self.max_size = max_size # N\n    self.index = 0\n\n  def enqueue(self,transition:GameTransition):\n    if (len(self.transitions) < self.max_size):\n      self.transitions.append(transition)\n    else:\n      self.transitions[self.index] = transition\n    self.index +=1\n    self.index = self.index % self.max_size\n\n  def get_experiences(self,nb_exp=1):\n    return random.sample(self.transitions, nb_exp)\n\n  def __len__(self):\n    return len(self.transitions)\n\n  def __getitem__(self,index):\n    return self.transitions[index]\n\n  def sample_minibatch(self,batch_size:int=32):\n    if (batch_size>=len(self)):\n      return self.transitions\n\n    return random.sample(self.transitions, batch_size)\n\n  def tensor_batch(self,batch_size):\n    batch = self.sample_minibatch(batch_size)\n    batch = GameTransition(*(zip(*batch)))\n\n    initial_state = torch.cat(batch.initial_state, dim=0)\n    initial_state = initial_state.unsqueeze(1)\n\n    next_state = torch.cat(batch.next_state, dim=0)\n    next_state = next_state.unsqueeze(1)\n\n    reward = torch.tensor(batch.reward)\n    reward = reward.unsqueeze(1)\n\n    done = torch.tensor(batch.done).float()\n    done = done.unsqueeze(1)\n\n    action = torch.tensor(batch.action)\n    action = action.unsqueeze(1)\n\n    return GameTransition(initial_state,action,reward,next_state,done)","metadata":{"id":"tsLFma1J00XQ","execution":{"iopub.status.busy":"2023-11-20T10:23:30.312968Z","iopub.execute_input":"2023-11-20T10:23:30.313540Z","iopub.status.idle":"2023-11-20T10:23:30.329774Z","shell.execute_reply.started":"2023-11-20T10:23:30.313507Z","shell.execute_reply":"2023-11-20T10:23:30.328966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lancement du modèle sur l'algorithme **\"deep Q-learning with experience replay.\"** issu de l'article","metadata":{"id":"sYoZhQMapgsH"}},{"cell_type":"code","source":"Action = int\nState = torch.Tensor","metadata":{"id":"1kzOr7vmRNwk","execution":{"iopub.status.busy":"2023-11-20T10:23:30.331136Z","iopub.execute_input":"2023-11-20T10:23:30.331880Z","iopub.status.idle":"2023-11-20T10:23:30.344386Z","shell.execute_reply.started":"2023-11-20T10:23:30.331845Z","shell.execute_reply":"2023-11-20T10:23:30.343288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DQNAgent():\n  max_step = int(1e5)\n  epsilon = 0.05\n  batch_size = 32\n  gamma = 0.9\n  time_limit = 3600\n  def __init__(self, env: gym.Env, max_experiences:int = int(1e6),update_frequency:int=5, verbose:bool=True, model_file:str=None, model:nn.Module=None):\n    self.env = env\n\n    self.model = model\n    self.target_model = copy.deepcopy(self.model)\n    self.target_model.eval()\n\n    self.experiences = ExpStack(max_size=max_experiences)\n\n    self.verbose = verbose\n    self.update_frequency = update_frequency\n    self.legal_actions = list(range(env.action_space.n))\n\n    self.loss_evolution = []\n    self.optimizer = torch.optim.Adam(self.model.parameters())\n    self.loss_func = nn.MSELoss()\n    self.begin_time = time()\n    self.stop = False\n    self.epoch_count = 0\n    if (model_file is None):\n      self.model_file = 'model.pt'\n    else:\n      self.model_file = model_file\n\n  def train_agent(self,nb_episodes:int):\n    episode_index = 0\n    while(episode_index < nb_episodes and not self.stop):\n      self.make_episode()\n      episode_index +=1\n      if(self.verbose ):\n        print(f'{episode_index}/{nb_episodes} episodes are done')\n\n  def make_episode(self):\n    done = False\n    step_index = 0\n    while(step_index <self.max_step and not done and not self.stop):\n      # Initialize State 1 # TODO\n      self.update_time()\n      current_state = preprocess_image(self.env.reset()[0])\n      done = self.make_step(current_state)\n      if(step_index % self.update_frequency == 0):\n        self.update_parameters()\n      step_index +=1\n\n  def make_step(self,current_state: State | None = None):\n    # Get action a_t\n    a_t = self.get_action(current_state=current_state)\n    # Make action and retrieve r_t and x_t\n    next_state,reward,done,info = self.make_action(a_t)\n    # Store transition\n    transition = GameTransition(current_state,a_t,reward,next_state,done)\n    self.experiences.enqueue(transition)\n    # Sample minibatch of transitions\n    mini_batch = self.experiences.tensor_batch(self.batch_size)\n    self.train_model(mini_batch)\n    return done\n\n\n\n  def train_model(self,batch:list[GameTransition]) -> None:\n    self.model.train()\n    y_target = (1 - batch.done ) * self.target_model(batch.next_state) * self.gamma + batch.reward\n    y_target = y_target.max(1)[0].unsqueeze(1)\n    y_pred = self.model(batch.initial_state).gather(1,batch.action)\n    self.gradient_descent(y_target,y_pred)\n\n  def gradient_descent(self,y_target,y_pred):\n    loss = self.loss_func(y_target,y_pred)\n    self.optimizer.zero_grad()\n    loss.backward()\n    self.loss_evolution.append(loss.item())\n    self.optimizer.step()\n    if(self.verbose and self.epoch_count %100 ==0):\n      print(f'Epoch {self.epoch_count}: loss = {loss.item()}')\n    self.epoch_count +=1\n\n  def make_action(self, action: Action) -> tuple[State,float, bool,dict]:\n    state,reward,truncated, terminated,info = self.env.step(action)\n    next_state = preprocess_image(state)\n    done = truncated or terminated\n    return next_state,reward,done,info\n\n\n  def get_action(self,current_state: State) -> Action:\n    if(random.random() < self.epsilon):\n      return random.choice(self.legal_actions)\n\n    current_state = current_state.unsqueeze(0)\n    model_output = self.model(current_state)\n    action = model_output.argmax().item()\n    return action\n\n  def update_parameters(self) -> None:\n    self.target_model = copy.deepcopy(self.model)\n    self.target_model.eval()\n\n  def reboot_timer(self):\n    self.begin_time = time()\n    self.stop = False\n    self.loss_evolution = []\n\n  def update_time(self):\n    current_time = int(time())\n    delta = current_time - self.begin_time\n    self.stop = delta > self.time_limit\n    if(self.stop):\n      # Save model\n      torch.save(self.model,self.model_file+'.pt')\n      print(f'TIME OUT: Model stops training after {delta:.2f} seconds, Save model to {self.model_file}.pt')\n      print(f'Epochs {self.epoch_count}')\n","metadata":{"id":"uHMulMZustrI","execution":{"iopub.status.busy":"2023-11-20T10:23:30.346338Z","iopub.execute_input":"2023-11-20T10:23:30.346794Z","iopub.status.idle":"2023-11-20T10:23:30.375911Z","shell.execute_reply.started":"2023-11-20T10:23:30.346752Z","shell.execute_reply":"2023-11-20T10:23:30.374747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_agent_and_plot(env_name, model_type, num_episodes=25000):\n    env = gym.make(env_name, render_mode=\"rgb_array\")\n    input_data = get_input_shapes(env)\n\n    model = None\n    if model_type == 'simple_dqn':\n        model = DQN(input_data.n_action, input_data.height, input_data.width)\n    elif model_type == 'duelling_dqn':\n        model = DuellingDQN(input_data.n_action, input_data.height, input_data.width)\n\n    agent = DQNAgent(env, verbose=True, model_file=model_type, model=model)\n    agent.train_agent(num_episodes)\n\n    return agent\n\n\nagents = []\nenv_names = ['ALE/Breakout-v5'] * 2\nmodel_types = ['simple_dqn', 'duelling_dqn']\n\nfor env_name, model_type in zip(env_names, model_types):\n    agent = train_agent_and_plot(env_name, model_type)\n    agents.append(agent)\n\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\nfor agent,ax in zip(agents,axes):\n    ax.plot(agent.loss_evolution, label=agent.model_file)\n\n    ax.set_title('Loss evolution')\n    ax.legend()\n\nplt.show()\nfig.savefig('combined_loss.png')","metadata":{"id":"nRWJr4ztF52_","outputId":"87fb82be-af04-4d75-9f82-3d96fa1910cc","execution":{"iopub.status.busy":"2023-11-20T10:23:30.377471Z","iopub.execute_input":"2023-11-20T10:23:30.377930Z","iopub.status.idle":"2023-11-20T10:25:02.775034Z","shell.execute_reply.started":"2023-11-20T10:23:30.377890Z","shell.execute_reply":"2023-11-20T10:25:02.773337Z"},"trusted":true},"execution_count":null,"outputs":[]}]}