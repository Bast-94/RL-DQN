{
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30587,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bast-94/RL-DQN/blob/dqn-draft/RL-DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projet de Reinforcement Learning : Deep Q-Learning sur le casse-brique d'Atari"
      ],
      "metadata": {
        "id": "IxUx75ISnaC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gymnasium[\"accept-rom-license\"]\n",
        "! pip install gymnasium[\"atari\"]\n",
        "! pip install torchviz"
      ],
      "metadata": {
        "id": "IT6J8Qb1nTG4",
        "outputId": "57d87f80-a3de-430d-8fbb-db77de8e0592",
        "execution": {
          "iopub.status.busy": "2023-11-20T10:23:01.004589Z",
          "iopub.execute_input": "2023-11-20T10:23:01.004993Z",
          "iopub.status.idle": "2023-11-20T10:23:29.308609Z",
          "shell.execute_reply.started": "2023-11-20T10:23:01.004961Z",
          "shell.execute_reply": "2023-11-20T10:23:29.306590Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.66.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2023.7.22)\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.2.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (6.1.1)\n",
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.1.0+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4133 sha256=927d9e01fde3705e9456eac8b7fc942a239f69b2a5574a86475ff4c97720901f\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Présentation globale du projet"
      ],
      "metadata": {
        "id": "R6RsUvhwom-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objectifs du projet\n",
        "\n",
        "Le but de ce projet est de mettre en place un algorithme d'apprentissage par renforcement basé sur les réseaux de neurones capable de jouer au Casse Brique d'Atari en maximisant ses gains."
      ],
      "metadata": {
        "id": "NJtQe-8OO2_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algorithme principal"
      ],
      "metadata": {
        "id": "mdognuOSOxDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'algorithme de l'agent va être quelque peu différent de ceux vus dans les tavaux précédents (Sarsa, Qlearning). Plusieurs éléments vont compléxifier la tâche:\n",
        "- Les états sont sous formes d'images de taille $(210,160,3)$ et non plus sous forme numérique.\n",
        "- La fonction $Q(s,a)$ va faire intervenir un réseaux de neurones $\\theta$ qui devra être entraîné.\n",
        "- Les anciennes expériences devront être stockées dans le but de donner une vérité terrain pour l'entrainement du réseau."
      ],
      "metadata": {
        "id": "Zs_C95Ewndez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pseudo code de l'algorithme"
      ],
      "metadata": {
        "id": "QTVHA36NPEcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le pseudo-code de l'algorithme ci-dessous provient de la publication"
      ],
      "metadata": {
        "id": "mgyJ9MDNo04q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$\\text{Algorithme de Q-Leearning profond avec répétition d'expérience}$\n",
        "1. **Initialisation:**\n",
        "   - Initialiser le réseau de neurones $Q$ avec des poids aléatoires.\n",
        "   - Initialiser la mémoire de relecture $D$ avec capacité maximale $N$.\n",
        "   - Initialiser aléatoirement les paramètres d'apprentissage.\n",
        "   - Initialiser la fonction $Q$ avec des $\\theta$ aléatoire.\n",
        "   - Initialiser $\\hat{Q}$ avec $\\theta^⁻ = \\theta$.\n",
        "\n",
        "2. **Pour chaque épisode:**\n",
        "   - Initialiser l'environnement et l'état initial $s_1=\\{x_1\\}$\n",
        "   - Appliquer le prétraitement $\\phi_1 = \\phi(s_1)$\n",
        "   \n",
        "   3. **Pour chaque étape $t$ de l'épisode:**\n",
        "      - Choisir l'action $a_t$ avec la politique $\\varepsilon$-greedy\n",
        "        - $\\mathbb{P}(a_t = argmax_a(Q(s_t,a;\\theta)) = 1 - \\varepsilon$\n",
        "        - $\\mathbb{P}(a_t = \\text{random\\_sample(}A)) = \\varepsilon$\n",
        "      - Exécuter l'action $a_t$, observer la récompense $r_t$ et l'état suivant $s_{t+1}$\n",
        "      - Stocker la transition $(s_t, a_t, r_{t}, s_{t+1})$ dans la mémoire de relecture $D$\n",
        "      - Affecter $s_{t+1}=s_t,a_t,x_{t+1}$\n",
        "      - Prétraitement de $s_{t+1}$ : $\\phi_{t+1}=\\phi(s_{t+1})$\n",
        "      - Échantillonner un lot aléatoire de transitions $(s_i, a_i, r_i, s_{i+1})$ de $D$\n",
        "      - Calculer la vérité terrain $y_i$ pour chaque transition $(s_i, a_i, r_i, s_{i+1})$ en utilisant le réseau $\\hat{Q}$ aux paramètre $\\theta^-$\n",
        "      - Cloner $Q$ dans $\\hat{Q}$ toutes les $C$ étapes\n",
        "      \n"
      ],
      "metadata": {
        "id": "QX8se4unQDDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Détails des variables\n",
        "- $Q$ : Fonction de qualité qui pour un couple état-action évalue à quel point une action dans un état donné est favorable.\n",
        "- $C$ : Nombre d'étapes à laquelle $\\hat{Q}$ se met à jour sur $Q$.\n",
        "- $\\hat{Q}$ : Target Network , il correspond à une version ancienne de $Q$ avec des paramètres $\\theta^-$ sur les $C$ dernières étapes.\n",
        "- $\\theta$ : Correspond aux paramètres du réseau de neurones.\n",
        "- ${A}$ : L'ensemble des actions possibles.\n",
        "- $a_t$ : L'action faite par l'agent à l'étape $t$.\n",
        "- $x_t$ : Correspond à l'image brut du jeu à l'étape $t$.\n",
        "- $s_t$ : Correspond à une séquence de couples action-image $\\{a_i \\times x_i\\}_{i\\lt t}$ .\n",
        "- $\\phi_t$ : Correspond au pré-traitement de l'état $s_t$ (Plus de détails dans la suite du notebook).\n",
        "- $\\varepsilon \\in [0,1]$ : Probabilité de choisir une action aléatoire.\n",
        "- $r_t$ : Récompense obtenue par la réalisation de l'action $a_t$ à l'instant $s_t$\n",
        "- $D$ : Mémoire de relecture.\n",
        "- $N$ : Nombre de simulations."
      ],
      "metadata": {
        "id": "9Se6OPbaO_cU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Démarche de recherche et implémentation"
      ],
      "metadata": {
        "id": "mWhORuoko1v4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importation pour la création des réseaux de neurones\n",
        "from torch import nn\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Librairie dédiée à la simulation du Casse-Brique\n",
        "import gymnasium as gym\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "from time import time\n",
        "from collections import namedtuple\n",
        "# Librairie dédiée à la création des gif finaux\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "6MpkCLnpoSIw",
        "execution": {
          "iopub.status.busy": "2023-11-20T10:23:29.311654Z",
          "iopub.execute_input": "2023-11-20T10:23:29.312110Z",
          "iopub.status.idle": "2023-11-20T10:23:29.321689Z",
          "shell.execute_reply.started": "2023-11-20T10:23:29.312071Z",
          "shell.execute_reply": "2023-11-20T10:23:29.320349Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b42c6a-c2c0-4bb9-a85a-c2a5e728ee82"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prétraitement des données"
      ],
      "metadata": {
        "id": "Q78gPWyCXSAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le prétraitement des données se justifie par le fait que l'entrainement des des réseaux de neurones nécessite d'enregistrer les états précédents. Les états correspondent ici à une image du jeux sous frome de tableau de dimension $\\text{(hauteur,largeur,nombre de canaux)}$. Afin de réduire le coût en mémoire il faut réduire la taille de chaque état en les prétraitant et en conservant l'information. L'interface du casse-brique d'Atari est très pixelisée et redondante en terme de couleurs, il est donc possible de pouvoir faire une réduction de dimension au niveau de la couleur et de la taille de l'image"
      ],
      "metadata": {
        "id": "sXcCJXtob6NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(img_array:np.array) -> np.array :\n",
        "  transform = transforms.Compose([\n",
        "      transforms.ToPILImage(),\n",
        "      transforms.Resize((84, 84)),\n",
        "      transforms.Grayscale(num_output_channels=1),\n",
        "      transforms.ToTensor()])\n",
        "  return transform(img_array)\n"
      ],
      "metadata": {
        "id": "vvsrhD0sQxqC",
        "execution": {
          "iopub.status.busy": "2023-11-20T10:23:29.325233Z",
          "iopub.execute_input": "2023-11-20T10:23:29.325635Z",
          "iopub.status.idle": "2023-11-20T10:23:29.333531Z",
          "shell.execute_reply.started": "2023-11-20T10:23:29.325593Z",
          "shell.execute_reply": "2023-11-20T10:23:29.332323Z"
        },
        "trusted": true
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ainsi l'image passera d'une shape $(210,160,3)$ à une shape $(84,84,1)$ réduisant considérablement la taille."
      ],
      "metadata": {
        "id": "I6UE9iZUfVl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('ALE/Breakout-v5', render_mode=\"rgb_array\")\n",
        "state_img = env.reset()[0]\n",
        "print(env.step(1)[1:])\n",
        "fig,axes = plt.subplots(1,2)\n",
        "fig.suptitle('Images comparison')\n",
        "axes[0].set_title('Original image')\n",
        "axes[0].imshow(state_img)\n",
        "axes[0].axis('off')\n",
        "new_img = preprocess_image(state_img)\n",
        "print(f'{new_img.size() = }')\n",
        "axes[1].axis('off')\n",
        "axes[1].set_title('Preprocessed image')\n",
        "axes[1].imshow(new_img.permute(1,2,0))"
      ],
      "metadata": {
        "id": "60aiphhbXCe_",
        "outputId": "0efc7b8b-a7e5-4997-cc25-1d1b6f35601d",
        "execution": {
          "iopub.status.busy": "2023-11-20T10:23:29.335245Z",
          "iopub.execute_input": "2023-11-20T10:23:29.335574Z",
          "iopub.status.idle": "2023-11-20T10:23:29.918409Z",
          "shell.execute_reply.started": "2023-11-20T10:23:29.335546Z",
          "shell.execute_reply": "2023-11-20T10:23:29.917256Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.0, False, False, {'lives': 5, 'episode_frame_number': 4, 'frame_number': 4})\n",
            "new_img.size() = torch.Size([1, 84, 84])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7b9787b812a0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Input = namedtuple('input', ('height', 'width', 'n_action'))\n",
        "\n",
        "def get_input_shapes(env:gym.Env):\n",
        "  env_tensor = np.zeros(env.observation_space.shape,dtype=np.uint8)\n",
        "  x = preprocess_image(env_tensor)\n",
        "  _,height, width = x.shape\n",
        "  n_action=env.action_space.n\n",
        "  return Input(height,width,n_action)\n",
        "\n"
      ],
      "metadata": {
        "id": "Uj5w4MoD3dxR",
        "execution": {
          "iopub.status.busy": "2023-11-20T10:23:29.955365Z",
          "iopub.execute_input": "2023-11-20T10:23:29.955754Z",
          "iopub.status.idle": "2023-11-20T10:23:29.970257Z",
          "shell.execute_reply.started": "2023-11-20T10:23:29.955713Z",
          "shell.execute_reply": "2023-11-20T10:23:29.969213Z"
        },
        "trusted": true
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Elaboration de modèles de de DQN\n",
        "\n",
        "Ici deux modèles vont être implémentés pour étudier quelle est la stratégie la plus adaptée."
      ],
      "metadata": {
        "id": "0A1YB793pFc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DQN simple"
      ],
      "metadata": {
        "id": "FxrkcKgrgGX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_action, height, width, linear_size=1024, model_name=None):\n",
        "        super(DQN, self).__init__()\n",
        "        self.input_dimension = 1, height, width\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=self.input_dimension[0], out_channels=64, kernel_size=8, stride=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        x0 = torch.zeros(1, 1, height, width)\n",
        "        x0 = self.convolute(x0)\n",
        "        x0 = self.flatten(x0)\n",
        "        flatten_dim = x0.shape[1]\n",
        "\n",
        "        self.linear1 = nn.Linear(flatten_dim, linear_size)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(linear_size, n_action)\n",
        "\n",
        "    def flatten(self, x):\n",
        "        return x.view(x.shape[0], -1)\n",
        "\n",
        "    def convolute(self,x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.relu1(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.relu2(x)\n",
        "      x = self.conv3(x)\n",
        "      x = self.relu3(x)\n",
        "      return x\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.convolute(x)\n",
        "\n",
        "      x = self.flatten(x)\n",
        "\n",
        "      x = self.linear1(x)\n",
        "      x = self.relu4(x)\n",
        "      return self.linear2(x)"
      ],
      "metadata": {
        "id": "G823PH_jIctd",
        "execution": {
          "iopub.status.busy": "2023-11-20T10:23:29.922244Z",
          "iopub.execute_input": "2023-11-20T10:23:29.922613Z",
          "iopub.status.idle": "2023-11-20T10:23:29.937143Z",
          "shell.execute_reply.started": "2023-11-20T10:23:29.922575Z",
          "shell.execute_reply": "2023-11-20T10:23:29.936117Z"
        },
        "trusted": true
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n"
      ],
      "metadata": {
        "id": "j_YxBCCMmYAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class DuellingDQN(nn.Module):\n",
        "    def __init__(self, n_action, height, width, linear_size=1024, model_name=None):\n",
        "        super(DuellingDQN, self).__init__()\n",
        "        self.input_dimension = 1, height, width\n",
        "        self.model_name = model_name\n",
        "        self.conv1 = nn.Conv2d(in_channels=self.input_dimension[0], out_channels=64, kernel_size=8, stride=4)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        x0 = torch.zeros(1, 1, height, width)\n",
        "        x0 = self.convolute(x0)\n",
        "        x0 = self.flatten(x0)\n",
        "        flatten_dim = x0.shape[1]\n",
        "        self.value = nn.Linear(flatten_dim,1)\n",
        "        self.advantage = nn.Linear(flatten_dim,n_action)\n",
        "\n",
        "\n",
        "\n",
        "    def flatten(self, x):\n",
        "        return x.view(x.shape[0], -1)\n",
        "\n",
        "    def convolute(self,x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.relu1(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.relu2(x)\n",
        "      x = self.conv3(x)\n",
        "      x = self.relu3(x)\n",
        "      return x\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.convolute(x)\n",
        "      x = self.flatten(x)\n",
        "      value = self.value(x)\n",
        "      advantage = self.advantage(x)\n",
        "      q_value = value + (advantage - advantage.mean(dim=1,keepdim=True))\n",
        "      return q_value"
      ],
      "metadata": {
        "id": "ST4yINSA3j76",
        "execution": {
          "iopub.status.busy": "2023-11-20T10:23:29.938443Z",
          "iopub.execute_input": "2023-11-20T10:23:29.939314Z",
          "iopub.status.idle": "2023-11-20T10:23:29.953985Z",
          "shell.execute_reply.started": "2023-11-20T10:23:29.939278Z",
          "shell.execute_reply": "2023-11-20T10:23:29.952408Z"
        },
        "trusted": true
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testons"
      ],
      "metadata": {
        "id": "bfOjRVyt_TRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('ALE/Breakout-v5', render_mode=\"rgb_array\")\n",
        "input_data = get_input_shapes(env)\n",
        "dqn = DQN(input_data.n_action,input_data.height,input_data.width)\n",
        "duelling_dqn = DuellingDQN(input_data.n_action,input_data.height,input_data.width)\n",
        "x = preprocess_image(env.reset()[0])\n",
        "x = x.unsqueeze(0)\n",
        "for model in [dqn, duelling_dqn]:\n",
        "  output = model(x)\n",
        "  assert output.size(1) == env.action_space.n, print(f'{output.size(1)} != {env.action_space.n}')\n",
        "  print(\"Les shapes de sortie du modèle sont cohérentes.\")\n",
        "  print(f'{output.size()}')"
      ],
      "metadata": {
        "id": "bHL5liwk-X3B",
        "outputId": "e7349387-b21d-460e-9531-666a036e07ac",
        "execution": {
          "iopub.status.busy": "2023-11-20T10:23:29.971488Z",
          "iopub.execute_input": "2023-11-20T10:23:29.971981Z",
          "iopub.status.idle": "2023-11-20T10:23:30.303758Z",
          "shell.execute_reply.started": "2023-11-20T10:23:29.971936Z",
          "shell.execute_reply": "2023-11-20T10:23:30.302410Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les shapes de sortie du modèle sont cohérentes.\n",
            "torch.Size([1, 4])\n",
            "Les shapes de sortie du modèle sont cohérentes.\n",
            "torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gestion de la mémoire des expériences"
      ],
      "metadata": {
        "id": "GkoNo8z40ulV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GameTransition = namedtuple('game_transition', ('initial_state', 'action', 'reward','next_state', 'done'))"
      ],
      "metadata": {
        "id": "-EEcI22jgs3u",
        "execution": {
          "iopub.status.busy": "2023-11-20T10:23:30.305395Z",
          "iopub.execute_input": "2023-11-20T10:23:30.305790Z",
          "iopub.status.idle": "2023-11-20T10:23:30.311332Z",
          "shell.execute_reply.started": "2023-11-20T10:23:30.305756Z",
          "shell.execute_reply": "2023-11-20T10:23:30.310446Z"
        },
        "trusted": true
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExpStack(): # D\n",
        "  def __init__(self, max_size:int):\n",
        "    self.transitions = []\n",
        "    self.max_size = max_size # N\n",
        "    self.index = 0\n",
        "\n",
        "  def enqueue(self,transition:GameTransition):\n",
        "    if (len(self.transitions) < self.max_size):\n",
        "      self.transitions.append(transition)\n",
        "    else:\n",
        "      self.transitions[self.index] = transition\n",
        "    self.index +=1\n",
        "    self.index = self.index % self.max_size\n",
        "\n",
        "  def get_experiences(self,nb_exp=1):\n",
        "    return random.sample(self.transitions, nb_exp)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.transitions)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.transitions[index]\n",
        "\n",
        "  def sample_minibatch(self,batch_size:int=32):\n",
        "    if (batch_size>=len(self)):\n",
        "      return self.transitions\n",
        "\n",
        "    return random.sample(self.transitions, batch_size)\n",
        "\n",
        "  def tensor_batch(self,batch_size):\n",
        "    batch = self.sample_minibatch(batch_size)\n",
        "    batch = GameTransition(*(zip(*batch)))\n",
        "\n",
        "    initial_state = torch.cat(batch.initial_state, dim=0)\n",
        "    initial_state = initial_state.unsqueeze(1)\n",
        "\n",
        "    next_state = torch.cat(batch.next_state, dim=0)\n",
        "    next_state = next_state.unsqueeze(1)\n",
        "\n",
        "    reward = torch.tensor(batch.reward)\n",
        "    reward = reward.unsqueeze(1)\n",
        "\n",
        "    done = torch.tensor(batch.done).float()\n",
        "    done = done.unsqueeze(1)\n",
        "\n",
        "    action = torch.tensor(batch.action)\n",
        "    action = action.unsqueeze(1)\n",
        "\n",
        "    return GameTransition(initial_state,action,reward,next_state,done)"
      ],
      "metadata": {
        "id": "tsLFma1J00XQ",
        "execution": {
          "iopub.status.busy": "2023-11-20T10:23:30.312968Z",
          "iopub.execute_input": "2023-11-20T10:23:30.313540Z",
          "iopub.status.idle": "2023-11-20T10:23:30.329774Z",
          "shell.execute_reply.started": "2023-11-20T10:23:30.313507Z",
          "shell.execute_reply": "2023-11-20T10:23:30.328966Z"
        },
        "trusted": true
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lancement du modèle sur l'algorithme **\"deep Q-learning with experience replay.\"** issu de l'article"
      ],
      "metadata": {
        "id": "sYoZhQMapgsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Action = int\n",
        "State = torch.Tensor"
      ],
      "metadata": {
        "id": "1kzOr7vmRNwk",
        "execution": {
          "iopub.status.busy": "2023-11-20T10:23:30.331136Z",
          "iopub.execute_input": "2023-11-20T10:23:30.331880Z",
          "iopub.status.idle": "2023-11-20T10:23:30.344386Z",
          "shell.execute_reply.started": "2023-11-20T10:23:30.331845Z",
          "shell.execute_reply": "2023-11-20T10:23:30.343288Z"
        },
        "trusted": true
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent():\n",
        "  max_step = int(1e5)\n",
        "  epsilon = 0.05\n",
        "  batch_size = 32\n",
        "  gamma = 0.9\n",
        "  time_limit = 3600\n",
        "  def __init__(self, env: gym.Env, max_experiences:int = int(1e6),update_frequency:int=5, verbose:bool=True, model_file:str=None, model:nn.Module=None):\n",
        "    self.env = env\n",
        "\n",
        "    self.model = model\n",
        "    self.target_model = copy.deepcopy(self.model)\n",
        "    self.target_model.eval()\n",
        "\n",
        "    self.experiences = ExpStack(max_size=max_experiences)\n",
        "\n",
        "    self.verbose = verbose\n",
        "    self.update_frequency = update_frequency\n",
        "    self.legal_actions = list(range(env.action_space.n))\n",
        "\n",
        "    self.loss_evolution = []\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters())\n",
        "    self.loss_func = nn.MSELoss()\n",
        "    self.begin_time = time()\n",
        "    self.stop = False\n",
        "    self.epoch_count = 0\n",
        "    self.frames = []\n",
        "    if (model_file is None):\n",
        "      self.model_file = 'model.pt'\n",
        "    else:\n",
        "      self.model_file = model_file\n",
        "\n",
        "  def train_agent(self,nb_episodes:int):\n",
        "    episode_index = 0\n",
        "    self.reboot_timer()\n",
        "    self.clear_saving()\n",
        "    while(episode_index < nb_episodes and not self.stop):\n",
        "      total_reward = self.play()\n",
        "      episode_index +=1\n",
        "\n",
        "      self.log(f'{episode_index} is done')\n",
        "      self.log(f'Total reward {total_reward}')\n",
        "\n",
        "\n",
        "  def play(self,train:bool = True,record_frames:bool =False):\n",
        "    done = False\n",
        "    step_index = 0\n",
        "    current_state = self.env.reset()[0]\n",
        "    self.frames.append(current_state)\n",
        "    current_state = preprocess_image(current_state)\n",
        "    total_reward = 0\n",
        "    while(step_index <self.max_step and not done and not self.stop):\n",
        "      # Initialize State 1 # TODO\n",
        "      self.update_time()\n",
        "      #done = self.make_step(current_state)\n",
        "      a_t = self.get_action(current_state=current_state)\n",
        "      next_state,reward,done,info,next_state_frame = self.make_action(a_t)\n",
        "      self.frames.append(next_state_frame)\n",
        "      total_reward += reward\n",
        "\n",
        "      if (train):\n",
        "        transition = GameTransition(current_state,a_t,reward,next_state,done)\n",
        "        self.experiences.enqueue(transition)\n",
        "\n",
        "        mini_batch = self.experiences.tensor_batch(self.batch_size)\n",
        "        self.train_model(mini_batch)\n",
        "        if(step_index % self.update_frequency == 0):\n",
        "          self.update_parameters()\n",
        "      if(done):\n",
        "        self.log(f'Game over after {step_index} steps')\n",
        "      current_state = next_state\n",
        "      step_index +=1\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "  def train_model(self,batch:list[GameTransition]) -> None:\n",
        "    self.model.train()\n",
        "    y_target = (1 - batch.done ) * self.target_model(batch.next_state) * self.gamma + batch.reward\n",
        "    y_target = y_target.max(1)[0].unsqueeze(1)\n",
        "    y_pred = self.model(batch.initial_state).gather(1,batch.action)\n",
        "    self.gradient_descent(y_target,y_pred)\n",
        "\n",
        "  def gradient_descent(self,y_target,y_pred):\n",
        "    loss = self.loss_func(y_target,y_pred)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.loss_evolution.append(loss.item())\n",
        "    self.optimizer.step()\n",
        "    if(self.epoch_count %100 ==0):\n",
        "      self.log(f'Epoch {self.epoch_count}: loss = {loss.item()}')\n",
        "    self.epoch_count +=1\n",
        "\n",
        "  def make_action(self, action: Action) -> tuple[State,float, bool,dict,np.array]:\n",
        "    state,reward,truncated, terminated,info = self.env.step(action)\n",
        "    next_state = preprocess_image(state)\n",
        "    done = truncated or terminated\n",
        "\n",
        "    return next_state,reward,done,info,state\n",
        "\n",
        "\n",
        "  def get_action(self,current_state: State) -> Action:\n",
        "    if(random.random() < self.epsilon):\n",
        "      return random.choice(self.legal_actions)\n",
        "\n",
        "    current_state = current_state.unsqueeze(0)\n",
        "    model_output = self.model(current_state)\n",
        "    action = model_output.argmax().item()\n",
        "    return action\n",
        "\n",
        "  def update_parameters(self) -> None:\n",
        "    self.target_model = copy.deepcopy(self.model)\n",
        "    self.target_model.eval()\n",
        "\n",
        "  def reboot_timer(self):\n",
        "    self.begin_time = time()\n",
        "    self.stop = False\n",
        "\n",
        "  def clear_saving(self):\n",
        "    self.frames = []\n",
        "    self.experiences = ExpStack(self.experiences.max_size)\n",
        "    self.loss_evolution = []\n",
        "\n",
        "  def generate_gif(self, output_file:str) -> None:\n",
        "    self.play(train=False)\n",
        "    frames = [Image.fromarray(f, mode='RGB') for f in self.frames]\n",
        "    frames[0].save(output_file, format='GIF', append_images=frames[1:], save_all=True, duration=10, loop=0)\n",
        "    self.log(f'Saving {output_file}')\n",
        "\n",
        "  def log(self, *args,**kwargs):\n",
        "    if(self.verbose):\n",
        "      print(*args,**kwargs)\n",
        "\n",
        "  def update_time(self):\n",
        "    current_time = int(time())\n",
        "    delta = current_time - self.begin_time\n",
        "    self.stop = delta > self.time_limit\n",
        "    if(self.stop):\n",
        "      # Save model\n",
        "      torch.save(self.model,self.model_file+'.pt')\n",
        "      self.log(f'TIME OUT: Model stops training after {delta:.2f} seconds, Save model to {self.model_file}.pt')\n",
        "      self.log(f'Epochs {self.epoch_count}')"
      ],
      "metadata": {
        "id": "uHMulMZustrI",
        "execution": {
          "iopub.status.busy": "2023-11-20T10:23:30.346338Z",
          "iopub.execute_input": "2023-11-20T10:23:30.346794Z",
          "iopub.status.idle": "2023-11-20T10:23:30.375911Z",
          "shell.execute_reply.started": "2023-11-20T10:23:30.346752Z",
          "shell.execute_reply": "2023-11-20T10:23:30.374747Z"
        },
        "trusted": true
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent_and_plot(env_name, model_type, num_episodes=25000,time_limit:int = 3600):\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    input_data = get_input_shapes(env)\n",
        "\n",
        "    model = None\n",
        "    if model_type == 'simple_dqn':\n",
        "        model = DQN(input_data.n_action, input_data.height, input_data.width)\n",
        "    elif model_type == 'duelling_dqn':\n",
        "        model = DuellingDQN(input_data.n_action, input_data.height, input_data.width)\n",
        "\n",
        "    agent = DQNAgent(env, verbose=True, model_file=model_type, model=model)\n",
        "    agent.time_limit = time_limit\n",
        "    agent.train_agent(num_episodes)\n",
        "    agent.generate_gif(model_type+'.gif')\n",
        "    return agent\n",
        "\n",
        "\n",
        "agents = []\n",
        "env_name = 'ALE/Breakout-v5'\n",
        "model_types = ['simple_dqn', 'duelling_dqn']\n",
        "TIME_LIMIT = 3600\n",
        "\n",
        "\n",
        "for  model_type in (model_types):\n",
        "    agent = train_agent_and_plot(env_name, model_type,time_limit=TIME_LIMIT)\n",
        "    agents.append(agent)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "for agent,ax,model_type in zip(agents,axes,model_types):\n",
        "\n",
        "    ax.plot(agent.loss_evolution, label=agent.model_file,color='blue')\n",
        "    ax.set_xlabel('epochs')\n",
        "    ax.set_ylabel('loss value')\n",
        "    ax.set_title(f'Loss evolution for {model_type}')\n",
        "    ax.legend()\n",
        "\n",
        "plt.show()\n",
        "fig.savefig('combined_loss.png')"
      ],
      "metadata": {
        "id": "nRWJr4ztF52_",
        "outputId": "50e55e6a-b7d8-4583-9465-8213f9c42e68",
        "execution": {
          "iopub.status.busy": "2023-11-20T10:23:30.377471Z",
          "iopub.execute_input": "2023-11-20T10:23:30.377930Z",
          "iopub.status.idle": "2023-11-20T10:25:02.775034Z",
          "shell.execute_reply.started": "2023-11-20T10:23:30.377890Z",
          "shell.execute_reply": "2023-11-20T10:25:02.773337Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss = 8.112925570458174e-06\n",
            "Epoch 100: loss = 5.522559263226867e-07\n",
            "Game over after 160 steps\n",
            "1/25000 episodes are done\n",
            "Epoch 200: loss = 8.475968371612908e-09\n",
            "Epoch 300: loss = 1.8201435025844148e-08\n",
            "Game over after 166 steps\n",
            "2/25000 episodes are done\n",
            "Epoch 400: loss = 5.161574517842382e-05\n",
            "Epoch 500: loss = 0.00012095828424207866\n",
            "Game over after 251 steps\n",
            "3/25000 episodes are done\n",
            "Epoch 600: loss = 1.886272457340965e-06\n",
            "Epoch 700: loss = 2.7888716431334615e-05\n",
            "Epoch 800: loss = 4.02447449232568e-06\n",
            "Epoch 900: loss = 2.6376555979368277e-05\n",
            "Epoch 1000: loss = 8.478105542053527e-07\n",
            "Epoch 1100: loss = 7.657100468350109e-06\n",
            "Epoch 1200: loss = 7.908413681434467e-05\n",
            "Game over after 624 steps\n",
            "4/25000 episodes are done\n",
            "Epoch 1300: loss = 1.5518149893978261e-06\n",
            "Epoch 1400: loss = 2.0683390175690874e-05\n",
            "Epoch 1500: loss = 9.188781405100599e-06\n",
            "Epoch 1600: loss = 3.8430782296927646e-05\n",
            "Game over after 413 steps\n",
            "5/25000 episodes are done\n",
            "Epoch 1700: loss = 3.4386594052193686e-05\n",
            "Epoch 1800: loss = 2.3625949324923567e-05\n",
            "Epoch 1900: loss = 4.165052087046206e-05\n",
            "Epoch 2000: loss = 2.0836778276134282e-05\n",
            "Epoch 2100: loss = 9.798698556551244e-06\n",
            "Epoch 2200: loss = 1.6628815501462668e-05\n",
            "Epoch 2300: loss = 0.03127739205956459\n",
            "Game over after 759 steps\n",
            "6/25000 episodes are done\n",
            "Epoch 2400: loss = 9.08757374418201e-06\n",
            "Epoch 2500: loss = 0.03099384531378746\n",
            "Epoch 2600: loss = 5.460185548145091e-06\n",
            "Epoch 2700: loss = 6.945545464986935e-06\n",
            "Game over after 404 steps\n",
            "7/25000 episodes are done\n",
            "Epoch 2800: loss = 3.317690925541683e-06\n",
            "Epoch 2900: loss = 4.542766419035615e-06\n",
            "Epoch 3000: loss = 9.778516869118903e-06\n",
            "Epoch 3100: loss = 6.618534825975075e-06\n",
            "Game over after 372 steps\n",
            "8/25000 episodes are done\n",
            "Epoch 3200: loss = 6.727928121108562e-05\n",
            "Epoch 3300: loss = 4.033275217807386e-06\n",
            "Epoch 3400: loss = 1.2846361642004922e-05\n",
            "Epoch 3500: loss = 6.136107913334854e-06\n",
            "Epoch 3600: loss = 2.4201846827054396e-05\n",
            "Epoch 3700: loss = 1.6035059161367826e-05\n",
            "Game over after 549 steps\n",
            "9/25000 episodes are done\n",
            "Epoch 3800: loss = 0.030915267765522003\n",
            "Epoch 3900: loss = 1.544990300317295e-05\n",
            "Epoch 4000: loss = 1.7313650459982455e-05\n",
            "Epoch 4100: loss = 1.2283678188396152e-05\n",
            "Epoch 4200: loss = 6.4905884755717125e-06\n",
            "Epoch 4300: loss = 6.714498795190593e-06\n",
            "Game over after 651 steps\n",
            "10/25000 episodes are done\n",
            "Epoch 4400: loss = 8.68817005539313e-06\n",
            "Epoch 4500: loss = 8.619888831162825e-06\n",
            "Epoch 4600: loss = 3.950628524762578e-05\n",
            "Epoch 4700: loss = 6.770907930331305e-06\n",
            "Game over after 364 steps\n",
            "11/25000 episodes are done\n",
            "Epoch 4800: loss = 1.5589115719194524e-05\n",
            "Epoch 4900: loss = 9.93285357253626e-06\n",
            "Epoch 5000: loss = 7.1445365392719395e-06\n",
            "Epoch 5100: loss = 5.497344318428077e-06\n",
            "Epoch 5200: loss = 4.101303420611657e-05\n",
            "Game over after 566 steps\n",
            "12/25000 episodes are done\n",
            "Epoch 5300: loss = 1.0813589142344426e-05\n",
            "Epoch 5400: loss = 6.719325028825551e-06\n",
            "Epoch 5500: loss = 4.7497369450866245e-06\n",
            "Epoch 5600: loss = 0.03117501363158226\n",
            "Game over after 399 steps\n",
            "13/25000 episodes are done\n",
            "Epoch 5700: loss = 5.078043614048511e-06\n",
            "Epoch 5800: loss = 1.549765693198424e-05\n",
            "Epoch 5900: loss = 4.7530586016364396e-05\n",
            "Epoch 6000: loss = 7.745142283965833e-06\n",
            "Game over after 310 steps\n",
            "14/25000 episodes are done\n",
            "Epoch 6100: loss = 1.3039571058470756e-05\n",
            "Epoch 6200: loss = 7.025312243058579e-06\n",
            "Epoch 6300: loss = 6.2541662373405416e-06\n",
            "Epoch 6400: loss = 1.4072962585487403e-05\n",
            "Epoch 6500: loss = 1.6627562217763625e-05\n",
            "Epoch 6600: loss = 1.1669202649500221e-05\n",
            "Game over after 696 steps\n",
            "15/25000 episodes are done\n",
            "Epoch 6700: loss = 5.376105036702938e-06\n",
            "Epoch 6800: loss = 1.28736410260899e-05\n",
            "Epoch 6900: loss = 5.1195397645642515e-06\n",
            "Epoch 7000: loss = 7.27348697182606e-06\n",
            "Epoch 7100: loss = 1.2751509530062322e-05\n",
            "Epoch 7200: loss = 6.555573691002792e-06\n",
            "Game over after 506 steps\n",
            "16/25000 episodes are done\n",
            "Epoch 7300: loss = 1.851672277553007e-05\n",
            "Epoch 7400: loss = 1.1193912541784812e-05\n",
            "Epoch 7500: loss = 4.676909156842157e-06\n",
            "Game over after 338 steps\n",
            "17/25000 episodes are done\n",
            "Epoch 7600: loss = 9.00180748431012e-06\n",
            "Epoch 7700: loss = 4.796937355422415e-05\n",
            "Epoch 7800: loss = 1.0184585335082375e-05\n",
            "Epoch 7900: loss = 1.9707185856532305e-05\n",
            "Game over after 418 steps\n",
            "18/25000 episodes are done\n",
            "Epoch 8000: loss = 9.709829100756906e-06\n",
            "Epoch 8100: loss = 3.1996501093090046e-06\n",
            "Epoch 8200: loss = 1.9163535398547538e-06\n",
            "Epoch 8300: loss = 4.889547199127264e-06\n",
            "Epoch 8400: loss = 3.2465172807860654e-06\n",
            "Game over after 483 steps\n",
            "19/25000 episodes are done\n",
            "Epoch 8500: loss = 0.031063949689269066\n",
            "Epoch 8600: loss = 4.775566139869625e-06\n",
            "Epoch 8700: loss = 3.2887558063521283e-06\n",
            "Game over after 333 steps\n",
            "20/25000 episodes are done\n",
            "Epoch 8800: loss = 9.238169695890974e-06\n",
            "Epoch 8900: loss = 6.596960247406969e-06\n",
            "Epoch 9000: loss = 5.2467285058810376e-06\n",
            "Epoch 9100: loss = 5.922689524595626e-06\n",
            "Epoch 9200: loss = 1.0138986908714287e-05\n",
            "Epoch 9300: loss = 0.031031513586640358\n",
            "Epoch 9400: loss = 7.856292540964205e-06\n",
            "Game over after 638 steps\n",
            "21/25000 episodes are done\n",
            "Epoch 9500: loss = 1.196459743368905e-05\n",
            "Epoch 9600: loss = 7.85368956712773e-06\n",
            "Epoch 9700: loss = 1.198728205054067e-05\n",
            "Game over after 296 steps\n",
            "22/25000 episodes are done\n",
            "Epoch 9800: loss = 8.46966759127099e-06\n",
            "TIME OUT: Model stops training after 3600.04 seconds, Save model to simple_dqn.pt\n",
            "Epochs 9880\n",
            "23/25000 episodes are done\n",
            "Saving simple_dqn.gif\n",
            "Epoch 0: loss = 2.3312745724979322e-06\n",
            "Epoch 100: loss = 4.666259201258072e-08\n",
            "Epoch 200: loss = 3.6633596156576687e-10\n",
            "Game over after 290 steps\n",
            "1/25000 episodes are done\n",
            "Epoch 300: loss = 1.9935835204876184e-09\n",
            "Epoch 400: loss = 3.9756281600134e-10\n",
            "Game over after 193 steps\n",
            "2/25000 episodes are done\n",
            "Epoch 500: loss = 5.790627422008754e-10\n",
            "Epoch 600: loss = 0.00030618434539064765\n",
            "Epoch 700: loss = 0.031188230961561203\n",
            "Game over after 251 steps\n",
            "3/25000 episodes are done\n",
            "Epoch 800: loss = 0.03157315030694008\n",
            "Epoch 900: loss = 0.0002602649910841137\n",
            "Game over after 260 steps\n",
            "4/25000 episodes are done\n",
            "Epoch 1000: loss = 0.031110268086194992\n",
            "Epoch 1100: loss = 0.030502060428261757\n",
            "Epoch 1200: loss = 0.00011917428491869941\n",
            "Game over after 220 steps\n",
            "5/25000 episodes are done\n",
            "Epoch 1300: loss = 5.475351645145565e-05\n",
            "Epoch 1400: loss = 0.00012353801866993308\n",
            "Game over after 246 steps\n",
            "6/25000 episodes are done\n",
            "Epoch 1500: loss = 2.2070433260523714e-05\n",
            "Epoch 1600: loss = 0.0001171844414784573\n",
            "Epoch 1700: loss = 6.030482109053992e-05\n",
            "Epoch 1800: loss = 4.884550435235724e-05\n",
            "Game over after 402 steps\n",
            "7/25000 episodes are done\n",
            "Epoch 1900: loss = 1.9151801097905263e-05\n",
            "Epoch 2000: loss = 2.7390422474127263e-05\n",
            "Epoch 2100: loss = 0.00012083571346011013\n",
            "Epoch 2200: loss = 0.030584193766117096\n",
            "Game over after 353 steps\n",
            "8/25000 episodes are done\n",
            "Epoch 2300: loss = 0.000165526318596676\n",
            "Game over after 173 steps\n",
            "9/25000 episodes are done\n",
            "Epoch 2400: loss = 0.00014278553135227412\n",
            "Epoch 2500: loss = 0.00037569471169263124\n",
            "Game over after 141 steps\n",
            "10/25000 episodes are done\n",
            "Epoch 2600: loss = 5.3433825087267905e-05\n",
            "Epoch 2700: loss = 5.226545545156114e-05\n",
            "Game over after 213 steps\n",
            "11/25000 episodes are done\n",
            "Epoch 2800: loss = 0.00020500223035924137\n",
            "Epoch 2900: loss = 0.004840278998017311\n",
            "Game over after 187 steps\n",
            "12/25000 episodes are done\n",
            "Epoch 3000: loss = 0.002595896366983652\n",
            "Epoch 3100: loss = 0.009179678745567799\n",
            "Game over after 187 steps\n",
            "13/25000 episodes are done\n",
            "Epoch 3200: loss = 0.0004544714465737343\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-5d648fb682be>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m  \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_agent_and_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTIME_LIMIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-100-5d648fb682be>\u001b[0m in \u001b[0;36mtrain_agent_and_plot\u001b[0;34m(env_name, model_type, num_episodes, time_limit)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.gif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-98-15a71fad4abf>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(self, nb_episodes)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_saving\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnb_episodes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0mepisode_index\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{episode_index}/{nb_episodes} episodes are done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-98-15a71fad4abf>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, train, record_frames)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menqueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_index\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-96-c85de710f647>\u001b[0m in \u001b[0;36mtensor_batch\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGameTransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[](./simple_dqn.gif)"
      ],
      "metadata": {
        "id": "RnO4stmsNzA_"
      }
    }
  ]
}