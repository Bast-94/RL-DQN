{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682356a1",
   "metadata": {
    "id": "view-in-github",
    "papermill": {
     "duration": 0.012477,
     "end_time": "2023-11-21T08:08:13.644492",
     "exception": false,
     "start_time": "2023-11-21T08:08:13.632015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Bast-94/RL-DQN/blob/dqn-draft/RL-DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff1ec3",
   "metadata": {
    "id": "IxUx75ISnaC2",
    "papermill": {
     "duration": 0.011666,
     "end_time": "2023-11-21T08:08:13.668364",
     "exception": false,
     "start_time": "2023-11-21T08:08:13.656698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Projet de Reinforcement Learning : Deep Q-Learning sur le casse-brique d'Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568385ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:08:13.694965Z",
     "iopub.status.busy": "2023-11-21T08:08:13.694471Z",
     "iopub.status.idle": "2023-11-21T08:09:01.571433Z",
     "shell.execute_reply": "2023-11-21T08:09:01.570009Z"
    },
    "id": "IT6J8Qb1nTG4",
    "outputId": "e64dc404-94ee-4e38-9684-98c18ef21579",
    "papermill": {
     "duration": 47.894003,
     "end_time": "2023-11-21T08:09:01.574444",
     "exception": false,
     "start_time": "2023-11-21T08:08:13.680441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[accept-rom-license] in /opt/conda/lib/python3.10/site-packages (0.26.3)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[accept-rom-license]) (1.24.3)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[accept-rom-license]) (2.2.1)\r\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[accept-rom-license]) (0.0.1)\r\n",
      "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license])\r\n",
      "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.7)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.66.1)\r\n",
      "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license])\r\n",
      "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2023.7.22)\r\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license\r\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=3b667c8ade0da8578f79cf7da3826d3a03f7046914d7b8f4df8590ba0b457af9\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\r\n",
      "Successfully built AutoROM.accept-rom-license\r\n",
      "Installing collected packages: AutoROM.accept-rom-license, autorom\r\n",
      "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\r\n",
      "Requirement already satisfied: gymnasium[atari] in /opt/conda/lib/python3.10/site-packages (0.26.3)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[atari]) (1.24.3)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[atari]) (2.2.1)\r\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[atari]) (0.0.1)\r\n",
      "Collecting ale-py~=0.8.0 (from gymnasium[atari])\r\n",
      "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gymnasium[atari]) (5.13.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gymnasium[atari]) (4.5.0)\r\n",
      "Installing collected packages: ale-py\r\n",
      "Successfully installed ale-py-0.8.1\r\n"
     ]
    }
   ],
   "source": [
    "! pip install gymnasium[\"accept-rom-license\"]\n",
    "! pip install gymnasium[\"atari\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87860a",
   "metadata": {
    "id": "R6RsUvhwom-z",
    "papermill": {
     "duration": 0.015789,
     "end_time": "2023-11-21T08:09:01.606517",
     "exception": false,
     "start_time": "2023-11-21T08:09:01.590728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Présentation globale du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfce410",
   "metadata": {
    "id": "NJtQe-8OO2_1",
    "papermill": {
     "duration": 0.015441,
     "end_time": "2023-11-21T08:09:01.638079",
     "exception": false,
     "start_time": "2023-11-21T08:09:01.622638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Objectifs du projet\n",
    "\n",
    "Le but de ce projet est de mettre en place un algorithme d'apprentissage par renforcement basé sur les réseaux de neurones capable de jouer au Casse Brique d'Atari en maximisant ses gains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f5cbc9",
   "metadata": {
    "id": "mdognuOSOxDy",
    "papermill": {
     "duration": 0.015682,
     "end_time": "2023-11-21T08:09:01.671349",
     "exception": false,
     "start_time": "2023-11-21T08:09:01.655667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Algorithme principal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f84b466",
   "metadata": {
    "id": "Zs_C95Ewndez",
    "papermill": {
     "duration": 0.015457,
     "end_time": "2023-11-21T08:09:01.704354",
     "exception": false,
     "start_time": "2023-11-21T08:09:01.688897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "L'algorithme de l'agent va être quelque peu différent de ceux vus dans les tavaux précédents (Sarsa, Qlearning). Plusieurs éléments vont compléxifier la tâche:\n",
    "- Les états sont sous formes d'images de taille $(210,160,3)$ et non plus sous forme numérique.\n",
    "- La fonction $Q(s,a)$ va faire intervenir un réseaux de neurones $\\theta$ qui devra être entraîné.\n",
    "- Les anciennes expériences devront être stockées dans le but de donner une vérité terrain pour l'entrainement du réseau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ec7f58",
   "metadata": {
    "id": "QTVHA36NPEcA",
    "papermill": {
     "duration": 0.01545,
     "end_time": "2023-11-21T08:09:01.735764",
     "exception": false,
     "start_time": "2023-11-21T08:09:01.720314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Pseudo code de l'algorithme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503d7345",
   "metadata": {
    "id": "mgyJ9MDNo04q",
    "papermill": {
     "duration": 0.015738,
     "end_time": "2023-11-21T08:09:01.767208",
     "exception": false,
     "start_time": "2023-11-21T08:09:01.751470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Le pseudo-code de l'algorithme ci-dessous provient de la publication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dde0db",
   "metadata": {
    "id": "QX8se4unQDDr",
    "papermill": {
     "duration": 0.015542,
     "end_time": "2023-11-21T08:09:01.798587",
     "exception": false,
     "start_time": "2023-11-21T08:09:01.783045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "$\\text{Algorithme de Q-Leearning profond avec répétition d'expérience}$\n",
    "1. **Initialisation:**\n",
    "   - Initialiser le réseau de neurones $Q$ avec des poids aléatoires.\n",
    "   - Initialiser la mémoire de relecture $D$ avec capacité maximale $N$.\n",
    "   - Initialiser aléatoirement les paramètres d'apprentissage.\n",
    "   - Initialiser la fonction $Q$ avec des $\\theta$ aléatoire.\n",
    "   - Initialiser $\\hat{Q}$ avec $\\theta^⁻ = \\theta$.\n",
    "\n",
    "2. **Pour chaque épisode:**\n",
    "   - Initialiser l'environnement et l'état initial $s_1=\\{x_1\\}$\n",
    "   - Appliquer le prétraitement $\\phi_1 = \\phi(s_1)$\n",
    "   \n",
    "   3. **Pour chaque étape $t$ de l'épisode:**\n",
    "      - Choisir l'action $a_t$ avec la politique $\\varepsilon$-greedy\n",
    "        - $\\mathbb{P}(a_t = argmax_a(Q(s_t,a;\\theta)) = 1 - \\varepsilon$\n",
    "        - $\\mathbb{P}(a_t = \\text{random\\_sample(}A)) = \\varepsilon$\n",
    "      - Exécuter l'action $a_t$, observer la récompense $r_t$ et l'état suivant $s_{t+1}$\n",
    "      - Stocker la transition $(s_t, a_t, r_{t}, s_{t+1})$ dans la mémoire de relecture $D$\n",
    "      - Affecter $s_{t+1}=s_t,a_t,x_{t+1}$\n",
    "      - Prétraitement de $s_{t+1}$ : $\\phi_{t+1}=\\phi(s_{t+1})$\n",
    "      - Échantillonner un lot aléatoire de transitions $(s_i, a_i, r_i, s_{i+1})$ de $D$\n",
    "      - Calculer la vérité terrain $y_i$ pour chaque transition $(s_i, a_i, r_i, s_{i+1})$ en utilisant le réseau $\\hat{Q}$ aux paramètre $\\theta^-$\n",
    "      - Cloner $Q$ dans $\\hat{Q}$ toutes les $C$ étapes\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b51ec7",
   "metadata": {
    "id": "9Se6OPbaO_cU",
    "papermill": {
     "duration": 0.015511,
     "end_time": "2023-11-21T08:09:01.830084",
     "exception": false,
     "start_time": "2023-11-21T08:09:01.814573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Détails des variables\n",
    "- $Q$ : Fonction de qualité qui pour un couple état-action évalue à quel point une action dans un état donné est favorable.\n",
    "- $C$ : Nombre d'étapes à laquelle $\\hat{Q}$ se met à jour sur $Q$.\n",
    "- $\\hat{Q}$ : Target Network , il correspond à une version ancienne de $Q$ avec des paramètres $\\theta^-$ sur les $C$ dernières étapes.\n",
    "- $\\theta$ : Correspond aux paramètres du réseau de neurones.\n",
    "- ${A}$ : L'ensemble des actions possibles.\n",
    "- $a_t$ : L'action faite par l'agent à l'étape $t$.\n",
    "- $x_t$ : Correspond à l'image brut du jeu à l'étape $t$.\n",
    "- $s_t$ : Correspond à une séquence de couples action-image $\\{a_i \\times x_i\\}_{i\\lt t}$ .\n",
    "- $\\phi_t$ : Correspond au pré-traitement de l'état $s_t$ (Plus de détails dans la suite du notebook).\n",
    "- $\\varepsilon \\in [0,1]$ : Probabilité de choisir une action aléatoire.\n",
    "- $r_t$ : Récompense obtenue par la réalisation de l'action $a_t$ à l'instant $s_t$\n",
    "- $D$ : Mémoire de relecture.\n",
    "- $N$ : Nombre de simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da96a6",
   "metadata": {
    "id": "mWhORuoko1v4",
    "papermill": {
     "duration": 0.015965,
     "end_time": "2023-11-21T08:09:01.861971",
     "exception": false,
     "start_time": "2023-11-21T08:09:01.846006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Démarche de recherche et implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4595fcb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:01.896366Z",
     "iopub.status.busy": "2023-11-21T08:09:01.895366Z",
     "iopub.status.idle": "2023-11-21T08:09:02.731710Z",
     "shell.execute_reply": "2023-11-21T08:09:02.730411Z"
    },
    "papermill": {
     "duration": 0.857107,
     "end_time": "2023-11-21T08:09:02.734983",
     "exception": false,
     "start_time": "2023-11-21T08:09:01.877876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "if gymnasium.envs.registration.registry.get('ALE/Breakout-v5') is None:\n",
    "    import gym\n",
    "else:\n",
    "    import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5811b476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:02.769321Z",
     "iopub.status.busy": "2023-11-21T08:09:02.768882Z",
     "iopub.status.idle": "2023-11-21T08:09:06.741899Z",
     "shell.execute_reply": "2023-11-21T08:09:06.740795Z"
    },
    "id": "6MpkCLnpoSIw",
    "outputId": "ec7dcbf7-c456-4383-85d5-6949e51f5109",
    "papermill": {
     "duration": 3.993678,
     "end_time": "2023-11-21T08:09:06.744959",
     "exception": false,
     "start_time": "2023-11-21T08:09:02.751281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importation pour la création des réseaux de neurones\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from time import time\n",
    "from collections import namedtuple\n",
    "# Librairie dédiée à la création des gif finaux\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ae02c",
   "metadata": {
    "id": "Q78gPWyCXSAQ",
    "papermill": {
     "duration": 0.016508,
     "end_time": "2023-11-21T08:09:06.777995",
     "exception": false,
     "start_time": "2023-11-21T08:09:06.761487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Prétraitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2479b6f",
   "metadata": {
    "id": "sXcCJXtob6NO",
    "papermill": {
     "duration": 0.015498,
     "end_time": "2023-11-21T08:09:06.809559",
     "exception": false,
     "start_time": "2023-11-21T08:09:06.794061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Le prétraitement des données se justifie par le fait que l'entrainement des des réseaux de neurones nécessite d'enregistrer les états précédents. Les états correspondent ici à une séquence d'image du jeux sous frome de tableau de dimension $\\text{(hauteur,largeur,nombre de canaux)}$. Afin de réduire le coût en mémoire il faut réduire la taille de chaque état en les prétraitant et en conservant l'information. L'interface du casse-brique d'Atari est très pixelisée et redondante en terme de couleurs, il est donc possible de pouvoir faire une réduction de dimension au niveau de la couleur et de la taille de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76a1717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:06.843853Z",
     "iopub.status.busy": "2023-11-21T08:09:06.842792Z",
     "iopub.status.idle": "2023-11-21T08:09:06.849615Z",
     "shell.execute_reply": "2023-11-21T08:09:06.848479Z"
    },
    "id": "vvsrhD0sQxqC",
    "papermill": {
     "duration": 0.02658,
     "end_time": "2023-11-21T08:09:06.851934",
     "exception": false,
     "start_time": "2023-11-21T08:09:06.825354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img_array:np.array) -> np.array :\n",
    "  transform = transforms.Compose([\n",
    "      transforms.ToPILImage(),\n",
    "      transforms.Resize((84, 84)),\n",
    "      transforms.Grayscale(num_output_channels=1),\n",
    "      transforms.ToTensor()])\n",
    "  return transform(img_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ae831",
   "metadata": {
    "id": "I6UE9iZUfVl6",
    "papermill": {
     "duration": 0.015756,
     "end_time": "2023-11-21T08:09:06.883715",
     "exception": false,
     "start_time": "2023-11-21T08:09:06.867959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ainsi l'image passera d'une shape $(210,160,3)$ à une shape $(84,84,1)$ réduisant considérablement la taille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "197ea842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:06.917378Z",
     "iopub.status.busy": "2023-11-21T08:09:06.916921Z",
     "iopub.status.idle": "2023-11-21T08:09:07.646375Z",
     "shell.execute_reply": "2023-11-21T08:09:07.645443Z"
    },
    "id": "60aiphhbXCe_",
    "outputId": "09053a52-58b0-4858-f8fc-db3452414bdc",
    "papermill": {
     "duration": 0.749448,
     "end_time": "2023-11-21T08:09:07.648833",
     "exception": false,
     "start_time": "2023-11-21T08:09:06.899385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, False, False, {'lives': 5, 'episode_frame_number': 4, 'frame_number': 4})\n",
      "new_img.size() = torch.Size([1, 84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7d9177d1fe50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('ALE/Breakout-v5', render_mode=\"rgb_array\")\n",
    "state_img = env.reset()[0]\n",
    "print(env.step(1)[1:])\n",
    "fig,axes = plt.subplots(1,2)\n",
    "fig.suptitle('Images comparison')\n",
    "axes[0].set_title('Original image')\n",
    "axes[0].imshow(state_img)\n",
    "axes[0].axis('off')\n",
    "new_img = preprocess_image(state_img)\n",
    "print(f'{new_img.size() = }')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Preprocessed image')\n",
    "axes[1].imshow(new_img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d732df9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:07.777578Z",
     "iopub.status.busy": "2023-11-21T08:09:07.776881Z",
     "iopub.status.idle": "2023-11-21T08:09:07.783257Z",
     "shell.execute_reply": "2023-11-21T08:09:07.782210Z"
    },
    "id": "Uj5w4MoD3dxR",
    "papermill": {
     "duration": 0.02823,
     "end_time": "2023-11-21T08:09:07.786003",
     "exception": false,
     "start_time": "2023-11-21T08:09:07.757773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Input = namedtuple('input', ('height', 'width', 'n_action'))\n",
    "\n",
    "def get_input_shapes(env:gym.Env):\n",
    "  env_tensor = np.zeros(env.observation_space.shape,dtype=np.uint8)\n",
    "  x = preprocess_image(env_tensor)\n",
    "  _,height, width = x.shape\n",
    "  n_action=env.action_space.n\n",
    "  return Input(height,width,n_action)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5ce25",
   "metadata": {
    "id": "sotZRDhx4Mc8",
    "papermill": {
     "duration": 0.01673,
     "end_time": "2023-11-21T08:09:07.819950",
     "exception": false,
     "start_time": "2023-11-21T08:09:07.803220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Il faut ensuite pouvoir faire en sorte que chaque état corresponde à un séquence d'images réduites du jeu. Nous allons donc créer la classe `StateGenerator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97e1374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:07.856473Z",
     "iopub.status.busy": "2023-11-21T08:09:07.855960Z",
     "iopub.status.idle": "2023-11-21T08:09:07.869079Z",
     "shell.execute_reply": "2023-11-21T08:09:07.867866Z"
    },
    "id": "ecO981RXCLVj",
    "outputId": "5cba5934-921d-4981-f7e2-0ac853487db8",
    "papermill": {
     "duration": 0.034801,
     "end_time": "2023-11-21T08:09:07.871871",
     "exception": false,
     "start_time": "2023-11-21T08:09:07.837070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Action = int\n",
    "from collections import deque\n",
    "class StateGenerator():\n",
    "  def __init__(self,env:gym.Env,seq_len:int):\n",
    "    self.env = env\n",
    "    env_tensor = np.zeros(env.observation_space.shape,dtype=np.uint8)\n",
    "    x = preprocess_image(env_tensor)\n",
    "    _,height, width = x.shape\n",
    "    n_action=env.action_space.n\n",
    "    self.seq_len = seq_len\n",
    "    self.stack = torch.zeros(seq_len,height,width)\n",
    "    self.current_frame = np.ndarray(shape=(height,width,1))\n",
    "\n",
    "  def init_env(self):\n",
    "    self.current_frame  = self.env.reset()[0]\n",
    "    state = preprocess_image(self.current_frame)\n",
    "    self.stack[:] =state\n",
    "    return self.stack, self.current_frame\n",
    "\n",
    "  def make_action(self,action:Action):\n",
    "    next_state,reward,truncated, terminated,info =self.env.step(action)\n",
    "    self.current_frame = next_state\n",
    "    next_state = preprocess_image(next_state)\n",
    "    done = truncated or terminated\n",
    "    self._update_stack(next_state)\n",
    "\n",
    "    return self.stack,reward, done, info, self.current_frame\n",
    "\n",
    "  def _update_stack(self, new_state: torch.Tensor):\n",
    "        self.stack[:-1] = self.stack[1:].clone()\n",
    "        self.stack[-1] = new_state\n",
    "\n",
    "  def reset_stack(self):\n",
    "    self.stack = torch.zeros(self.seq_len, 1, *self.stack.shape[2:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9630ce1e",
   "metadata": {
    "id": "0A1YB793pFc2",
    "papermill": {
     "duration": 0.016756,
     "end_time": "2023-11-21T08:09:07.905970",
     "exception": false,
     "start_time": "2023-11-21T08:09:07.889214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Elaboration de modèles de de DQN\n",
    "\n",
    "Ici deux modèles vont être implémentés pour étudier quelle est la stratégie la plus adaptée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f3878",
   "metadata": {
    "id": "FxrkcKgrgGX5",
    "papermill": {
     "duration": 0.016591,
     "end_time": "2023-11-21T08:09:07.939592",
     "exception": false,
     "start_time": "2023-11-21T08:09:07.923001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### DQN simple\n",
    "\n",
    "Le premier dispose de 3 couches de convolutions et d'une couche linaire. Le tenseur de sorti correspond aux Q values de chaque action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4b90aca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:07.976072Z",
     "iopub.status.busy": "2023-11-21T08:09:07.975515Z",
     "iopub.status.idle": "2023-11-21T08:09:07.989130Z",
     "shell.execute_reply": "2023-11-21T08:09:07.988106Z"
    },
    "id": "G823PH_jIctd",
    "outputId": "f6fc310e-80d7-471a-e819-064b6e0a5ee2",
    "papermill": {
     "duration": 0.034784,
     "end_time": "2023-11-21T08:09:07.991246",
     "exception": false,
     "start_time": "2023-11-21T08:09:07.956462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_action, height, width,seq_len=1 ,linear_size=1024, model_name=None,):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_dimension = seq_len, height, width\n",
    "        self.conv1 = nn.Conv2d(in_channels=seq_len, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        x0 = torch.zeros(1, seq_len, height, width)\n",
    "        x0 = self.convolute(x0)\n",
    "        x0 = self.flatten(x0)\n",
    "        flatten_dim = x0.shape[1]\n",
    "\n",
    "        self.linear1 = nn.Linear(flatten_dim, linear_size)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(linear_size, n_action)\n",
    "\n",
    "    def flatten(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "    def convolute(self,x):\n",
    "      x = self.conv1(x)\n",
    "      x = self.relu1(x)\n",
    "      x = self.conv2(x)\n",
    "      x = self.relu2(x)\n",
    "      x = self.conv3(x)\n",
    "      x = self.relu3(x)\n",
    "      return x\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.convolute(x)\n",
    "\n",
    "      x = self.flatten(x)\n",
    "\n",
    "      x = self.linear1(x)\n",
    "      x = self.relu4(x)\n",
    "      return self.linear2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878775c8",
   "metadata": {
    "id": "J8rKs2lx6yG1",
    "papermill": {
     "duration": 0.016983,
     "end_time": "2023-11-21T08:09:08.025715",
     "exception": false,
     "start_time": "2023-11-21T08:09:08.008732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Le second est un  dueling network, il décompose la valeur d'action en deux parties: la valeur de l'état (à quel point l'état actuel est cool) et l'avantage de l'action (à quel point choisir cette action par rapport aux autres est cool). Cela permet au réseau de mieux comprendre ce qui se passe dans le jeu et d'apprendre de manière plus efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a39786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:08.063678Z",
     "iopub.status.busy": "2023-11-21T08:09:08.062494Z",
     "iopub.status.idle": "2023-11-21T08:09:08.077110Z",
     "shell.execute_reply": "2023-11-21T08:09:08.075671Z"
    },
    "id": "ST4yINSA3j76",
    "papermill": {
     "duration": 0.036727,
     "end_time": "2023-11-21T08:09:08.080332",
     "exception": false,
     "start_time": "2023-11-21T08:09:08.043605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DuellingDQN(nn.Module):\n",
    "    def __init__(self, n_action, height, width, seq_len =1,linear_size=1024, model_name=None):\n",
    "        super(DuellingDQN, self).__init__()\n",
    "        self.input_dimension = seq_len, height, width\n",
    "        self.model_name = model_name\n",
    "        self.conv1 = nn.Conv2d(in_channels=self.input_dimension[0], out_channels=64, kernel_size=8, stride=4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        x0 = torch.zeros(1, seq_len, height, width)\n",
    "        x0 = self.convolute(x0)\n",
    "        x0 = self.flatten(x0)\n",
    "        flatten_dim = x0.shape[1]\n",
    "        self.value = nn.Linear(flatten_dim,1)\n",
    "        self.advantage = nn.Linear(flatten_dim,n_action)\n",
    "\n",
    "\n",
    "\n",
    "    def flatten(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "    def convolute(self,x):\n",
    "      x = self.conv1(x)\n",
    "      x = self.relu1(x)\n",
    "      x = self.conv2(x)\n",
    "      x = self.relu2(x)\n",
    "      x = self.conv3(x)\n",
    "      x = self.relu3(x)\n",
    "      return x\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.convolute(x)\n",
    "      x = self.flatten(x)\n",
    "      value = self.value(x)\n",
    "      advantage = self.advantage(x)\n",
    "      q_value = value + (advantage - advantage.mean(dim=1,keepdim=True))\n",
    "      return q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08dc2f2",
   "metadata": {
    "id": "bfOjRVyt_TRz",
    "papermill": {
     "duration": 0.016481,
     "end_time": "2023-11-21T08:09:08.114436",
     "exception": false,
     "start_time": "2023-11-21T08:09:08.097955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Vérifions l'homogénéité des shapes d'entrée et de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06f01cec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:08.150933Z",
     "iopub.status.busy": "2023-11-21T08:09:08.150429Z",
     "iopub.status.idle": "2023-11-21T08:09:08.547461Z",
     "shell.execute_reply": "2023-11-21T08:09:08.546035Z"
    },
    "id": "bHL5liwk-X3B",
    "outputId": "2c29616f-bf30-46ff-d98c-9987a59c6617",
    "papermill": {
     "duration": 0.425098,
     "end_time": "2023-11-21T08:09:08.557301",
     "exception": false,
     "start_time": "2023-11-21T08:09:08.132203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les shapes de sortie du modèle sont cohérentes.\n",
      "torch.Size([1, 4])\n",
      "Les shapes de sortie du modèle sont cohérentes.\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('ALE/Breakout-v5', render_mode=\"rgb_array\")\n",
    "input_data = get_input_shapes(env)\n",
    "\n",
    "dqn = DQN(input_data.n_action,input_data.height,input_data.width)\n",
    "duelling_dqn = DuellingDQN(input_data.n_action,input_data.height,input_data.width)\n",
    "\n",
    "x = preprocess_image(env.reset()[0])\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "for model in [dqn, duelling_dqn]:\n",
    "  output = model(x)\n",
    "  assert output.size(1) == env.action_space.n, print(f'{output.size(1)} != {env.action_space.n}')\n",
    "  print(\"Les shapes de sortie du modèle sont cohérentes.\")\n",
    "  print(f'{output.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279cf6a4",
   "metadata": {
    "id": "GkoNo8z40ulV",
    "papermill": {
     "duration": 0.022451,
     "end_time": "2023-11-21T08:09:08.599587",
     "exception": false,
     "start_time": "2023-11-21T08:09:08.577136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Gestion de la mémoire des expériences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732ecb1",
   "metadata": {
    "id": "y_hG3WCN7nBT",
    "papermill": {
     "duration": 0.01798,
     "end_time": "2023-11-21T08:09:08.642286",
     "exception": false,
     "start_time": "2023-11-21T08:09:08.624306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Le but est de stocker les états précédents afin de permettre un meilleur apprentissage des Q values. Nous allons donc créer la classe `ExpStack`, qui génèrera des batchs aléatoires. La taille d'une telle structure est plafonnée afin d'éviter que la mémoire ne sature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "768c1274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:08.678030Z",
     "iopub.status.busy": "2023-11-21T08:09:08.677543Z",
     "iopub.status.idle": "2023-11-21T08:09:08.682889Z",
     "shell.execute_reply": "2023-11-21T08:09:08.682062Z"
    },
    "id": "-EEcI22jgs3u",
    "papermill": {
     "duration": 0.025683,
     "end_time": "2023-11-21T08:09:08.685106",
     "exception": false,
     "start_time": "2023-11-21T08:09:08.659423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GameTransition = namedtuple('game_transition', ('initial_state', 'action', 'reward','next_state', 'done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f95b9f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:08.721571Z",
     "iopub.status.busy": "2023-11-21T08:09:08.721078Z",
     "iopub.status.idle": "2023-11-21T08:09:08.735104Z",
     "shell.execute_reply": "2023-11-21T08:09:08.733877Z"
    },
    "id": "tsLFma1J00XQ",
    "outputId": "6fa7d2da-d097-4280-8af8-f1afc78979e4",
    "papermill": {
     "duration": 0.035251,
     "end_time": "2023-11-21T08:09:08.737770",
     "exception": false,
     "start_time": "2023-11-21T08:09:08.702519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ExpStack(): # D\n",
    "  def __init__(self, max_size:int, seq_len:int = 4):\n",
    "    self.transitions = []\n",
    "    self.max_size = max_size # N\n",
    "    self.index = 0\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "  def enqueue(self,transition:GameTransition):\n",
    "    if (len(self.transitions) < self.max_size):\n",
    "      self.transitions.append(transition)\n",
    "    else:\n",
    "      self.transitions[self.index] = transition\n",
    "    self.index +=1\n",
    "    self.index = self.index % self.max_size\n",
    "\n",
    "  def get_experiences(self,nb_exp=1):\n",
    "    return random.sample(self.transitions, nb_exp)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.transitions)\n",
    "\n",
    "  def __getitem__(self,index):\n",
    "    return self.transitions[index]\n",
    "\n",
    "  def sample_minibatch(self,batch_size:int=32):\n",
    "    if (batch_size>len(self)):\n",
    "      return self.transitions\n",
    "\n",
    "    return random.sample(self.transitions, batch_size)\n",
    "\n",
    "  def tensor_batch(self,batch_size):\n",
    "    batch = random.sample(self.transitions, batch_size)\n",
    "    batch = GameTransition(*(zip(*batch)))\n",
    "    initial_state = torch.stack(batch.initial_state)\n",
    "\n",
    "    next_state = torch.stack(batch.next_state)\n",
    "\n",
    "    reward = torch.tensor(batch.reward)\n",
    "    reward = reward.unsqueeze(1)\n",
    "\n",
    "    done = torch.tensor(batch.done).float()\n",
    "    done = done.unsqueeze(1)\n",
    "\n",
    "    action = torch.tensor(batch.action)\n",
    "    action = action.unsqueeze(1)\n",
    "\n",
    "    return GameTransition(initial_state,action,reward,next_state,done)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13937c24",
   "metadata": {
    "id": "sYoZhQMapgsH",
    "papermill": {
     "duration": 0.016516,
     "end_time": "2023-11-21T08:09:08.771733",
     "exception": false,
     "start_time": "2023-11-21T08:09:08.755217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Lancement du modèle sur l'algorithme **\"deep Q-learning with experience replay.\"** issu de l'article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55201c69",
   "metadata": {
    "id": "8LU2or0X8Isq",
    "papermill": {
     "duration": 0.016238,
     "end_time": "2023-11-21T08:09:08.805027",
     "exception": false,
     "start_time": "2023-11-21T08:09:08.788789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Implémentation de l'Agent\n",
    "\n",
    "Nous allons créer la classe `DQNAgent` qui va pouvoir entraîner un modèle et mettre à jour $\\hat{Q}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b54147a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:08.840798Z",
     "iopub.status.busy": "2023-11-21T08:09:08.839964Z",
     "iopub.status.idle": "2023-11-21T08:09:08.844834Z",
     "shell.execute_reply": "2023-11-21T08:09:08.844086Z"
    },
    "id": "1kzOr7vmRNwk",
    "papermill": {
     "duration": 0.025222,
     "end_time": "2023-11-21T08:09:08.847073",
     "exception": false,
     "start_time": "2023-11-21T08:09:08.821851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "State = torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86efe21f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:08.883001Z",
     "iopub.status.busy": "2023-11-21T08:09:08.882134Z",
     "iopub.status.idle": "2023-11-21T08:09:08.903887Z",
     "shell.execute_reply": "2023-11-21T08:09:08.902978Z"
    },
    "id": "uHMulMZustrI",
    "papermill": {
     "duration": 0.042707,
     "end_time": "2023-11-21T08:09:08.906590",
     "exception": false,
     "start_time": "2023-11-21T08:09:08.863883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "  epsilon = 0.05\n",
    "  batch_size = 32\n",
    "  gamma = 0.9\n",
    "  max_experiences:int = 2500\n",
    "  def __init__(self, env: gym.Env,\n",
    "\n",
    "               seq_len:int=5,\n",
    "               verbose:bool=True,\n",
    "               model_file:str=None,\n",
    "               model:nn.Module=None):\n",
    "    self.state_gen = StateGenerator(env,seq_len=seq_len)\n",
    "    self.seq_len = seq_len\n",
    "    self.model = model\n",
    "    self.target_model = copy.deepcopy(self.model)\n",
    "    self.target_model.eval()\n",
    "\n",
    "    self.experiences = ExpStack(max_size=self.max_experiences,seq_len=1)\n",
    "    self.legal_actions = list(range(env.action_space.n))\n",
    "\n",
    "    self.loss_evolution = []\n",
    "    self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "    self.loss_func = nn.MSELoss()\n",
    "\n",
    "    self.epoch_count = 0\n",
    "    self.frames = []\n",
    "    if (model_file is None):\n",
    "      self.model_file = 'model.pt'\n",
    "    else:\n",
    "      self.model_file = model_file\n",
    "  def clear_frames(self):\n",
    "    del self.frames\n",
    "    self.frames = []\n",
    "  def clear_experiences(self):\n",
    "    del self.experiences\n",
    "    self.experiences = ExpStack(self.max_experiences)\n",
    "\n",
    "  def train_model(self) -> None:\n",
    "    self.model.train()\n",
    "    self.epoch_count +=1\n",
    "    if (len(self.experiences) >= self.batch_size):\n",
    "      batch = self.experiences.tensor_batch(self.batch_size)\n",
    "      y_target = (1 - batch.done ) * self.target_model(batch.next_state) * self.gamma + batch.reward\n",
    "      y_target = y_target.max(1)[0].unsqueeze(1)\n",
    "      y_pred = self.model(batch.initial_state).gather(1,batch.action)\n",
    "      loss = self.loss_func(y_target,y_pred)\n",
    "      del y_target, y_pred, batch\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.loss_evolution.append(loss.item())\n",
    "      self.optimizer.step()\n",
    "\n",
    "      return loss.item()\n",
    "\n",
    "  def make_action(self, action: Action) -> tuple[State,float, bool,dict,np.array]:\n",
    "    res = self.state_gen.make_action(action)\n",
    "    assert res[0].shape[0] == self.seq_len, f'{res[0].shape} is wrong'\n",
    "    return res\n",
    "\n",
    "\n",
    "  def get_action(self,current_state: State) -> Action:\n",
    "    if(random.random() < self.epsilon):\n",
    "      return random.choice(self.legal_actions)\n",
    "\n",
    "    current_state = current_state.unsqueeze(0)\n",
    "    model_output = self.model(current_state)\n",
    "    action = model_output.argmax().item()\n",
    "    return action\n",
    "\n",
    "  def update_parameters(self) -> None:\n",
    "    self.target_model.load_state_dict(self.model.state_dict())\n",
    "    self.target_model.eval()\n",
    "\n",
    "  def save_model(self):\n",
    "    torch.save(self.model,self.model_file+'.pt')\n",
    "\n",
    "\n",
    "  def generate_gif(self, output_file:str) -> None:\n",
    "\n",
    "    frames = [Image.fromarray(f, mode='RGB') for f in self.frames]\n",
    "    frames[0].save(output_file, format='GIF', append_images=frames[1:], save_all=True, duration=10, loop=0)\n",
    "    print(f'Saving {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d23cf",
   "metadata": {
    "id": "HNsw26Xv8lCy",
    "papermill": {
     "duration": 0.016274,
     "end_time": "2023-11-21T08:09:08.940055",
     "exception": false,
     "start_time": "2023-11-21T08:09:08.923781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "La classe `Trainer` est ensuite définie dans le but de pouvoir entraîner un `DQNAgent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f7357cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:08.977053Z",
     "iopub.status.busy": "2023-11-21T08:09:08.976133Z",
     "iopub.status.idle": "2023-11-21T08:09:08.995170Z",
     "shell.execute_reply": "2023-11-21T08:09:08.994095Z"
    },
    "id": "BVCi0zEfNJW5",
    "papermill": {
     "duration": 0.040499,
     "end_time": "2023-11-21T08:09:08.997858",
     "exception": false,
     "start_time": "2023-11-21T08:09:08.957359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  max_step = int(1e5)\n",
    "  epsilon = 0.05\n",
    "  batch_size = 32\n",
    "  gamma = 0.9\n",
    "  update_frequency = 5\n",
    "  def __init__(self, agent: DQNAgent, verbose:bool=True,time_limit=3600,nb_episodes:int=25000):\n",
    "      self.agent = agent\n",
    "      self.verbose = verbose\n",
    "      self.epochs_count = 0\n",
    "      self.stop = False\n",
    "      self.time_limit = time_limit\n",
    "      self.nb_episodes = nb_episodes\n",
    "\n",
    "  def train_agent(self,save_model :bool =True, gif_file:str=None):\n",
    "      episode_index = 0\n",
    "      total_reward = 0\n",
    "      self.reboot_timer()\n",
    "      while episode_index < self.nb_episodes and not self.stop:\n",
    "          reward = self.play()\n",
    "          episode_index += 1\n",
    "          total_reward+=reward\n",
    "          self.log(f'Episode {episode_index} is done')\n",
    "          self.log(f'Total reward {reward}')\n",
    "      if(save_model):\n",
    "        self.agent.save_model()\n",
    "      if(gif_file is not None):\n",
    "        self.agent.generate_gif(gif_file)\n",
    "      return total_reward/episode_index\n",
    "\n",
    "\n",
    "\n",
    "  def log(self, *args,**kwargs):\n",
    "    if(self.verbose):\n",
    "      print(*args,**kwargs)\n",
    "\n",
    "  def reboot_timer(self):\n",
    "    self.begin_time = time()\n",
    "    self.stop = False\n",
    "\n",
    "  def update_time(self):\n",
    "    current_time = int(time())\n",
    "    delta = current_time - self.begin_time\n",
    "    self.stop = delta > self.time_limit\n",
    "    if(self.stop):\n",
    "      self.log(f'TIME OUT: Model stops training after {delta:.2f} seconds, Save model to {self.agent.model_file}.pt')\n",
    "\n",
    "\n",
    "  def play(self, train:bool=True,keep_frame:bool=False):\n",
    "      done = False\n",
    "      step_index = 0\n",
    "      agent = self.agent\n",
    "      current_state,frame = agent.state_gen.init_env()\n",
    "      agent.frames.append(frame)\n",
    "\n",
    "      total_reward = 0\n",
    "\n",
    "      while step_index < self.max_step and not done and not self.stop:\n",
    "          self.update_time()\n",
    "          a_t = agent.get_action(current_state=current_state)\n",
    "          next_state, reward, done, info, next_state_frame = agent.make_action(a_t)\n",
    "          assert next_state.shape[0] == (agent.seq_len), \"Bad Shapes\"\n",
    "          assert current_state.shape[0] == (agent.seq_len), \"Bad Shapes\"\n",
    "          if(keep_frame):\n",
    "            agent.frames.append(next_state_frame)\n",
    "          total_reward += reward\n",
    "\n",
    "          if train:\n",
    "              transition = GameTransition(current_state, a_t, reward, next_state, done)\n",
    "              agent.experiences.enqueue(transition)\n",
    "              loss = agent.train_model()\n",
    "              if step_index % self.update_frequency == 0:\n",
    "                  agent.update_parameters()\n",
    "              if(agent.epoch_count % 100 == 0):\n",
    "                self.log(f'Epoch: {agent.epoch_count}, loss:{loss}')\n",
    "\n",
    "          if done:\n",
    "              self.log(f'Game over after {step_index} steps')\n",
    "          current_state = next_state\n",
    "          step_index += 1\n",
    "\n",
    "      return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f83e3dd",
   "metadata": {
    "id": "G40Gtt_v878L",
    "papermill": {
     "duration": 0.017027,
     "end_time": "2023-11-21T08:09:09.032109",
     "exception": false,
     "start_time": "2023-11-21T08:09:09.015082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Nous allons donc entraîner les deux agents avec les réseaux `DQN` et `DuellingDQN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69dff93a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T08:09:09.069345Z",
     "iopub.status.busy": "2023-11-21T08:09:09.068312Z",
     "iopub.status.idle": "2023-11-21T10:09:15.146980Z",
     "shell.execute_reply": "2023-11-21T10:09:15.145566Z"
    },
    "id": "As5XInr-2l7B",
    "papermill": {
     "duration": 7206.100409,
     "end_time": "2023-11-21T10:09:15.149861",
     "exception": false,
     "start_time": "2023-11-21T08:09:09.049452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, loss:0.0032211525831371546\n",
      "Epoch: 200, loss:0.000614985532592982\n",
      "Game over after 284 steps\n",
      "Episode 1 is done\n",
      "Total reward 3.0\n",
      "Epoch: 300, loss:0.06206010282039642\n",
      "Epoch: 400, loss:0.03156163915991783\n",
      "Game over after 191 steps\n",
      "Episode 2 is done\n",
      "Total reward 1.0\n",
      "Epoch: 500, loss:0.06063912436366081\n",
      "Epoch: 600, loss:0.00023178425908554345\n",
      "Game over after 196 steps\n",
      "Episode 3 is done\n",
      "Total reward 0.0\n",
      "Epoch: 700, loss:2.625080924190115e-06\n",
      "Epoch: 800, loss:1.5955874914652668e-05\n",
      "Epoch: 900, loss:0.0002179545845137909\n",
      "Epoch: 1000, loss:3.897121860063635e-05\n",
      "Epoch: 1100, loss:0.0003413204976823181\n",
      "Game over after 495 steps\n",
      "Episode 4 is done\n",
      "Total reward 2.0\n",
      "Epoch: 1200, loss:5.9861944464500993e-05\n",
      "Epoch: 1300, loss:4.1138009692076594e-05\n",
      "Epoch: 1400, loss:1.872222128440626e-05\n",
      "Game over after 232 steps\n",
      "Episode 5 is done\n",
      "Total reward 1.0\n",
      "Epoch: 1500, loss:5.4029944294597954e-05\n",
      "Epoch: 1600, loss:3.521234248182736e-05\n",
      "Game over after 282 steps\n",
      "Episode 6 is done\n",
      "Total reward 0.0\n",
      "Epoch: 1700, loss:6.022912202752195e-06\n",
      "Epoch: 1800, loss:2.8631297027459368e-05\n",
      "Game over after 210 steps\n",
      "Episode 7 is done\n",
      "Total reward 2.0\n",
      "Epoch: 1900, loss:5.770399366156198e-05\n",
      "Epoch: 2000, loss:8.597125997766852e-05\n",
      "Epoch: 2100, loss:4.8623023758409545e-05\n",
      "Game over after 284 steps\n",
      "Episode 8 is done\n",
      "Total reward 2.0\n",
      "Epoch: 2200, loss:0.03033430315554142\n",
      "Epoch: 2300, loss:6.886609480716288e-05\n",
      "Epoch: 2400, loss:3.857100455206819e-05\n",
      "Epoch: 2500, loss:7.434980943799019e-05\n",
      "Game over after 319 steps\n",
      "Episode 9 is done\n",
      "Total reward 3.0\n",
      "Epoch: 2600, loss:4.5432763727148995e-05\n",
      "Epoch: 2700, loss:6.170606502564624e-05\n",
      "Epoch: 2800, loss:0.00012076907296432182\n",
      "Game over after 344 steps\n",
      "Episode 10 is done\n",
      "Total reward 4.0\n",
      "Epoch: 2900, loss:0.030761443078517914\n",
      "Epoch: 3000, loss:9.461190347792581e-05\n",
      "Epoch: 3100, loss:0.0002054188953479752\n",
      "Epoch: 3200, loss:0.0002416801726212725\n",
      "Game over after 378 steps\n",
      "Episode 11 is done\n",
      "Total reward 2.0\n",
      "Epoch: 3300, loss:0.0009623278165236115\n",
      "Epoch: 3400, loss:8.98854123079218e-05\n",
      "Epoch: 3500, loss:0.03084370493888855\n",
      "Game over after 296 steps\n",
      "Episode 12 is done\n",
      "Total reward 2.0\n",
      "Epoch: 3600, loss:4.6689481678185984e-05\n",
      "Game over after 168 steps\n",
      "Episode 13 is done\n",
      "Total reward 1.0\n",
      "Epoch: 3700, loss:0.031227178871631622\n",
      "Epoch: 3800, loss:0.030656997114419937\n",
      "Epoch: 3900, loss:0.031219979748129845\n",
      "Epoch: 4000, loss:2.4056376787484623e-05\n",
      "Epoch: 4100, loss:0.0881619080901146\n",
      "Game over after 410 steps\n",
      "Episode 14 is done\n",
      "Total reward 6.0\n",
      "Epoch: 4200, loss:0.030880779027938843\n",
      "Epoch: 4300, loss:0.0002560622524470091\n",
      "Game over after 234 steps\n",
      "Episode 15 is done\n",
      "Total reward 2.0\n",
      "Epoch: 4400, loss:0.030848288908600807\n",
      "Epoch: 4500, loss:3.958972229156643e-05\n",
      "Game over after 238 steps\n",
      "Episode 16 is done\n",
      "Total reward 1.0\n",
      "Epoch: 4600, loss:5.2513976697809994e-05\n",
      "Epoch: 4700, loss:7.96830136096105e-05\n",
      "Game over after 180 steps\n",
      "Episode 17 is done\n",
      "Total reward 2.0\n",
      "Epoch: 4800, loss:8.817161142360419e-05\n",
      "Epoch: 4900, loss:6.613384903175756e-05\n",
      "Epoch: 5000, loss:9.59686076384969e-05\n",
      "Game over after 294 steps\n",
      "Episode 18 is done\n",
      "Total reward 3.0\n",
      "Epoch: 5100, loss:8.636615530122072e-05\n",
      "Epoch: 5200, loss:7.202633423730731e-05\n",
      "Game over after 224 steps\n",
      "Episode 19 is done\n",
      "Total reward 1.0\n",
      "Epoch: 5300, loss:7.009445107541978e-05\n",
      "Epoch: 5400, loss:8.922949928091839e-05\n",
      "Epoch: 5500, loss:0.030869070440530777\n",
      "Epoch: 5600, loss:2.9738570447079837e-05\n",
      "Game over after 360 steps\n",
      "Episode 20 is done\n",
      "Total reward 2.0\n",
      "Epoch: 5700, loss:0.00011890262248925865\n",
      "Epoch: 5800, loss:0.00016262863937299699\n",
      "Epoch: 5900, loss:9.940438758349046e-05\n",
      "Game over after 275 steps\n",
      "Episode 21 is done\n",
      "Total reward 2.0\n",
      "Epoch: 6000, loss:0.00012413902732077986\n",
      "Epoch: 6100, loss:0.00011721993359969929\n",
      "Game over after 189 steps\n",
      "Episode 22 is done\n",
      "Total reward 1.0\n",
      "Epoch: 6200, loss:0.03099396452307701\n",
      "Game over after 178 steps\n",
      "Episode 23 is done\n",
      "Total reward 0.0\n",
      "Epoch: 6300, loss:7.403046765830368e-05\n",
      "Epoch: 6400, loss:0.031045876443386078\n",
      "Epoch: 6500, loss:4.5402568503050134e-05\n",
      "Epoch: 6600, loss:0.030705776065587997\n",
      "Epoch: 6700, loss:7.388933590846136e-05\n",
      "Epoch: 6800, loss:0.0003239653306081891\n",
      "Game over after 531 steps\n",
      "Episode 24 is done\n",
      "Total reward 7.0\n",
      "Epoch: 6900, loss:6.206310354173183e-05\n",
      "Epoch: 7000, loss:9.247730486094952e-05\n",
      "Epoch: 7100, loss:0.030480995774269104\n",
      "Game over after 380 steps\n",
      "Episode 25 is done\n",
      "Total reward 3.0\n",
      "Epoch: 7200, loss:0.00013293542724568397\n",
      "Epoch: 7300, loss:0.0001468756381655112\n",
      "Epoch: 7400, loss:0.00010877339082071558\n",
      "Epoch: 7500, loss:2.628972288221121e-05\n",
      "Epoch: 7600, loss:8.068044553510845e-05\n",
      "Epoch: 7700, loss:4.8496120143681765e-05\n",
      "Epoch: 7800, loss:6.086240682634525e-05\n",
      "Game over after 611 steps\n",
      "Episode 26 is done\n",
      "Total reward 7.0\n",
      "Epoch: 7900, loss:8.163813618011773e-05\n",
      "Epoch: 8000, loss:0.00011444526899140328\n",
      "Epoch: 8100, loss:7.420110341627151e-05\n",
      "Epoch: 8200, loss:3.9125559851527214e-05\n",
      "Epoch: 8300, loss:5.210941162658855e-05\n",
      "Game over after 496 steps\n",
      "Episode 27 is done\n",
      "Total reward 0.0\n",
      "Epoch: 8400, loss:0.031368907541036606\n",
      "Epoch: 8500, loss:0.03066127933561802\n",
      "Epoch: 8600, loss:0.03095061518251896\n",
      "Epoch: 8700, loss:5.926161611569114e-05\n",
      "Epoch: 8800, loss:5.635829802486114e-05\n",
      "Game over after 561 steps\n",
      "Episode 28 is done\n",
      "Total reward 3.0\n",
      "Epoch: 8900, loss:4.292103767511435e-05\n",
      "Epoch: 9000, loss:7.331812230404466e-05\n",
      "Epoch: 9100, loss:5.2744842832908034e-05\n",
      "Epoch: 9200, loss:0.030814221128821373\n",
      "Epoch: 9300, loss:6.70783847454004e-05\n",
      "Game over after 485 steps\n",
      "Episode 29 is done\n",
      "Total reward 0.0\n",
      "Epoch: 9400, loss:3.082392140640877e-05\n",
      "Epoch: 9500, loss:3.2835927413543686e-05\n",
      "Epoch: 9600, loss:3.345409277244471e-05\n",
      "Epoch: 9700, loss:1.7130800188169815e-05\n",
      "Game over after 432 steps\n",
      "Episode 30 is done\n",
      "Total reward 2.0\n",
      "Epoch: 9800, loss:2.9391854695859365e-05\n",
      "Epoch: 9900, loss:4.089081267011352e-05\n",
      "Epoch: 10000, loss:6.155051232781261e-05\n",
      "Epoch: 10100, loss:3.594940426410176e-05\n",
      "Epoch: 10200, loss:4.851417543250136e-05\n",
      "Epoch: 10300, loss:4.3187395931454375e-05\n",
      "Game over after 525 steps\n",
      "Episode 31 is done\n",
      "Total reward 4.0\n",
      "Epoch: 10400, loss:9.983462223317474e-05\n",
      "Epoch: 10500, loss:3.849608765449375e-05\n",
      "Epoch: 10600, loss:6.177844625199214e-05\n",
      "Epoch: 10700, loss:3.757636295631528e-05\n",
      "Epoch: 10800, loss:0.030863143503665924\n",
      "Game over after 556 steps\n",
      "Episode 32 is done\n",
      "Total reward 0.0\n",
      "Epoch: 10900, loss:0.0001719364372547716\n",
      "Epoch: 11000, loss:0.00031320934067480266\n",
      "Epoch: 11100, loss:3.0188703021849506e-05\n",
      "Epoch: 11200, loss:3.5637796827359125e-05\n",
      "Game over after 426 steps\n",
      "Episode 33 is done\n",
      "Total reward 1.0\n",
      "Epoch: 11300, loss:2.178226350224577e-05\n",
      "Epoch: 11400, loss:1.9837663785438053e-05\n",
      "Epoch: 11500, loss:1.5988602172001265e-05\n",
      "Epoch: 11600, loss:2.28045664698584e-05\n",
      "Epoch: 11700, loss:1.610913204785902e-05\n",
      "Epoch: 11800, loss:3.887678758474067e-05\n",
      "Game over after 546 steps\n",
      "Episode 34 is done\n",
      "Total reward 5.0\n",
      "Epoch: 11900, loss:1.4005629054736346e-05\n",
      "Epoch: 12000, loss:4.1520685044815764e-05\n",
      "Epoch: 12100, loss:0.030834930017590523\n",
      "Epoch: 12200, loss:1.3784695511276368e-05\n",
      "Epoch: 12300, loss:3.787918831221759e-05\n",
      "Epoch: 12400, loss:1.57984031829983e-05\n",
      "Game over after 644 steps\n",
      "Episode 35 is done\n",
      "Total reward 2.0\n",
      "Epoch: 12500, loss:2.1837864551343955e-05\n",
      "Epoch: 12600, loss:0.00010558603389654309\n",
      "Epoch: 12700, loss:1.7737063899403438e-05\n",
      "Epoch: 12800, loss:1.4193576134857722e-05\n",
      "Epoch: 12900, loss:1.7027175999828614e-05\n",
      "Game over after 490 steps\n",
      "Episode 36 is done\n",
      "Total reward 3.0\n",
      "Epoch: 13000, loss:1.733907629386522e-05\n",
      "Epoch: 13100, loss:4.1324590711155906e-05\n",
      "Epoch: 13200, loss:3.1465115171158686e-05\n",
      "Epoch: 13300, loss:2.5588211428839713e-05\n",
      "Epoch: 13400, loss:2.1528423530980945e-05\n",
      "Epoch: 13500, loss:5.772613803856075e-05\n",
      "Epoch: 13600, loss:2.5304630980826914e-05\n",
      "Epoch: 13700, loss:3.032839231309481e-05\n",
      "Game over after 742 steps\n",
      "Episode 37 is done\n",
      "Total reward 3.0\n",
      "Epoch: 13800, loss:4.4184074795339257e-05\n",
      "Epoch: 13900, loss:0.030996140092611313\n",
      "Epoch: 14000, loss:1.642616916797124e-05\n",
      "Epoch: 14100, loss:3.3310971048194915e-05\n",
      "Game over after 446 steps\n",
      "Episode 38 is done\n",
      "Total reward 3.0\n",
      "Epoch: 14200, loss:3.080371243413538e-05\n",
      "Epoch: 14300, loss:3.0490129574900493e-05\n",
      "Epoch: 14400, loss:4.4748569052899256e-05\n",
      "Epoch: 14500, loss:3.6427954910323024e-05\n",
      "Epoch: 14600, loss:3.340133844176307e-05\n",
      "Epoch: 14700, loss:1.2351204532023985e-05\n",
      "Epoch: 14800, loss:3.255166666349396e-05\n",
      "Game over after 719 steps\n",
      "Episode 39 is done\n",
      "Total reward 3.0\n",
      "Epoch: 14900, loss:3.958529850933701e-05\n",
      "Epoch: 15000, loss:2.907757880166173e-05\n",
      "Epoch: 15100, loss:1.8991799151990563e-05\n",
      "Epoch: 15200, loss:9.69446682574926e-06\n",
      "Epoch: 15300, loss:0.00012416001118253917\n",
      "Epoch: 15400, loss:2.786309050861746e-05\n",
      "Game over after 535 steps\n",
      "Episode 40 is done\n",
      "Total reward 2.0\n",
      "Epoch: 15500, loss:3.4057895391015336e-05\n",
      "Epoch: 15600, loss:2.8394397304509766e-05\n",
      "Epoch: 15700, loss:2.250174293294549e-05\n",
      "Epoch: 15800, loss:2.665965075721033e-05\n",
      "Game over after 450 steps\n",
      "Episode 41 is done\n",
      "Total reward 7.0\n",
      "Epoch: 15900, loss:4.538649955065921e-05\n",
      "Epoch: 16000, loss:0.00010439442121423781\n",
      "Epoch: 16100, loss:0.030697152018547058\n",
      "Epoch: 16200, loss:9.363856952404603e-05\n",
      "Epoch: 16300, loss:7.061469659674913e-05\n",
      "Epoch: 16400, loss:8.088512549875304e-05\n",
      "Game over after 580 steps\n",
      "Episode 42 is done\n",
      "Total reward 3.0\n",
      "Epoch: 16500, loss:8.213533146772534e-05\n",
      "Epoch: 16600, loss:7.574461778858677e-05\n",
      "Epoch: 16700, loss:5.941334893577732e-05\n",
      "Epoch: 16800, loss:4.746780177811161e-05\n",
      "Epoch: 16900, loss:0.030847543850541115\n",
      "Epoch: 17000, loss:2.7883921575266868e-05\n",
      "Epoch: 17100, loss:5.146836338099092e-05\n",
      "Game over after 712 steps\n",
      "Episode 43 is done\n",
      "Total reward 3.0\n",
      "Epoch: 17200, loss:0.00016904716903809458\n",
      "Epoch: 17300, loss:0.03371230885386467\n",
      "Game over after 220 steps\n",
      "Episode 44 is done\n",
      "Total reward 2.0\n",
      "Epoch: 17400, loss:3.630884748417884e-05\n",
      "Epoch: 17500, loss:4.619606625055894e-05\n",
      "Epoch: 17600, loss:0.0002544573799241334\n",
      "Game over after 250 steps\n",
      "Episode 45 is done\n",
      "Total reward 3.0\n",
      "Epoch: 17700, loss:0.00014103198191151023\n",
      "Epoch: 17800, loss:7.355291018029675e-05\n",
      "Epoch: 17900, loss:0.00013799754378851503\n",
      "Epoch: 18000, loss:5.2394527301657945e-05\n",
      "Game over after 374 steps\n",
      "Episode 46 is done\n",
      "Total reward 2.0\n",
      "Epoch: 18100, loss:0.00011502260895213112\n",
      "Epoch: 18200, loss:0.00018988695228472352\n",
      "Epoch: 18300, loss:5.629699080600403e-05\n",
      "Game over after 377 steps\n",
      "Episode 47 is done\n",
      "Total reward 4.0\n",
      "Epoch: 18400, loss:0.0306792464107275\n",
      "Epoch: 18500, loss:8.013635670067742e-05\n",
      "Epoch: 18600, loss:8.109464397421107e-05\n",
      "Epoch: 18700, loss:0.0001296598929911852\n",
      "Game over after 349 steps\n",
      "Episode 48 is done\n",
      "Total reward 3.0\n",
      "Epoch: 18800, loss:1.1111044841527473e-05\n",
      "Epoch: 18900, loss:0.0001494872703915462\n",
      "Epoch: 19000, loss:0.00013380308519117534\n",
      "Epoch: 19100, loss:0.00015635155432391912\n",
      "Epoch: 19200, loss:8.028581214603037e-05\n",
      "Game over after 455 steps\n",
      "Episode 49 is done\n",
      "Total reward 3.0\n",
      "Epoch: 19300, loss:0.0001090028599719517\n",
      "Epoch: 19400, loss:0.0004483369120862335\n",
      "Game over after 229 steps\n",
      "Episode 50 is done\n",
      "Total reward 1.0\n",
      "Epoch: 19500, loss:7.551103772129864e-05\n",
      "Epoch: 19600, loss:7.954326429171488e-05\n",
      "Epoch: 19700, loss:7.667855970794335e-05\n",
      "Game over after 332 steps\n",
      "Episode 51 is done\n",
      "Total reward 2.0\n",
      "Epoch: 19800, loss:8.545955643057823e-05\n",
      "Epoch: 19900, loss:0.030884983018040657\n",
      "Epoch: 20000, loss:7.565226405858994e-05\n",
      "Game over after 329 steps\n",
      "Episode 52 is done\n",
      "Total reward 3.0\n",
      "Epoch: 20100, loss:6.281783134909347e-05\n",
      "Epoch: 20200, loss:0.03068297542631626\n",
      "Epoch: 20300, loss:2.6579025870887563e-05\n",
      "Epoch: 20400, loss:7.190527685452253e-05\n",
      "Epoch: 20500, loss:6.93712427164428e-05\n",
      "Game over after 458 steps\n",
      "Episode 53 is done\n",
      "Total reward 3.0\n",
      "Epoch: 20600, loss:0.030800728127360344\n",
      "Epoch: 20700, loss:4.687639375333674e-05\n",
      "Epoch: 20800, loss:8.122080907924101e-05\n",
      "Epoch: 20900, loss:6.534531712532043e-05\n",
      "Epoch: 21000, loss:0.0003601089701987803\n",
      "Epoch: 21100, loss:5.6669505283934996e-05\n",
      "Epoch: 21200, loss:0.03146543726325035\n",
      "Epoch: 21300, loss:4.9880429287441075e-05\n",
      "Game over after 753 steps\n",
      "Episode 54 is done\n",
      "Total reward 5.0\n",
      "Epoch: 21400, loss:5.261218029772863e-05\n",
      "Epoch: 21500, loss:0.030654698610305786\n",
      "Epoch: 21600, loss:5.60395565116778e-05\n",
      "Game over after 351 steps\n",
      "Episode 55 is done\n",
      "Total reward 5.0\n",
      "Epoch: 21700, loss:0.0004326055059209466\n",
      "Epoch: 21800, loss:7.292881491594017e-05\n",
      "Game over after 177 steps\n",
      "Episode 56 is done\n",
      "Total reward 0.0\n",
      "Epoch: 21900, loss:0.030636055395007133\n",
      "Epoch: 22000, loss:9.984395001083612e-05\n",
      "Game over after 247 steps\n",
      "Episode 57 is done\n",
      "Total reward 3.0\n",
      "Epoch: 22100, loss:7.408158853650093e-05\n",
      "Epoch: 22200, loss:0.00010473850124981254\n",
      "Epoch: 22300, loss:6.726612627971917e-05\n",
      "Game over after 284 steps\n",
      "Episode 58 is done\n",
      "Total reward 2.0\n",
      "Epoch: 22400, loss:0.0005258417804725468\n",
      "Epoch: 22500, loss:0.030521364882588387\n",
      "Game over after 197 steps\n",
      "Episode 59 is done\n",
      "Total reward 2.0\n",
      "Epoch: 22600, loss:6.17781188338995e-05\n",
      "Game over after 122 steps\n",
      "Episode 60 is done\n",
      "Total reward 0.0\n",
      "Epoch: 22700, loss:8.31870929687284e-05\n",
      "Epoch: 22800, loss:0.00013380016025621444\n",
      "Game over after 121 steps\n",
      "Episode 61 is done\n",
      "Total reward 0.0\n",
      "Epoch: 22900, loss:0.030555550009012222\n",
      "Epoch: 23000, loss:0.03062763251364231\n",
      "Game over after 226 steps\n",
      "Episode 62 is done\n",
      "Total reward 3.0\n",
      "Epoch: 23100, loss:0.00010894994920818135\n",
      "Game over after 149 steps\n",
      "Episode 63 is done\n",
      "Total reward 1.0\n",
      "Epoch: 23200, loss:0.030603960156440735\n",
      "Epoch: 23300, loss:9.103611228056252e-05\n",
      "Game over after 196 steps\n",
      "Episode 64 is done\n",
      "Total reward 2.0\n",
      "Epoch: 23400, loss:8.79933504620567e-05\n",
      "Epoch: 23500, loss:5.131852958584204e-05\n",
      "Epoch: 23600, loss:0.0003775138466153294\n",
      "Game over after 250 steps\n",
      "Episode 65 is done\n",
      "Total reward 3.0\n",
      "Epoch: 23700, loss:0.061254970729351044\n",
      "Epoch: 23800, loss:9.963640331989154e-05\n",
      "Epoch: 23900, loss:0.00042999308789148927\n",
      "Epoch: 24000, loss:9.441438305657357e-05\n",
      "Game over after 380 steps\n",
      "Episode 66 is done\n",
      "Total reward 3.0\n",
      "Epoch: 24100, loss:0.00010921878856606781\n",
      "Epoch: 24200, loss:0.00013333617243915796\n",
      "Epoch: 24300, loss:0.030616844072937965\n",
      "Game over after 293 steps\n",
      "Episode 67 is done\n",
      "Total reward 3.0\n",
      "Epoch: 24400, loss:0.00010183780977968127\n",
      "Game over after 121 steps\n",
      "Episode 68 is done\n",
      "Total reward 0.0\n",
      "Epoch: 24500, loss:9.135231812251732e-05\n",
      "Epoch: 24600, loss:3.9560407458338886e-05\n",
      "Epoch: 24700, loss:0.00011607298074522987\n",
      "Game over after 303 steps\n",
      "Episode 69 is done\n",
      "Total reward 2.0\n",
      "Epoch: 24800, loss:0.00043876521522179246\n",
      "Epoch: 24900, loss:3.28033602272626e-05\n",
      "Epoch: 25000, loss:0.061365921050310135\n",
      "Game over after 353 steps\n",
      "Episode 70 is done\n",
      "Total reward 2.0\n",
      "Epoch: 25100, loss:2.4358359951293096e-05\n",
      "Epoch: 25200, loss:8.67211856530048e-05\n",
      "Game over after 174 steps\n",
      "Episode 71 is done\n",
      "Total reward 0.0\n",
      "Epoch: 25300, loss:0.03101370669901371\n",
      "Epoch: 25400, loss:4.643943248083815e-05\n",
      "Epoch: 25500, loss:5.5318145314231515e-05\n",
      "Game over after 264 steps\n",
      "Episode 72 is done\n",
      "Total reward 0.0\n",
      "Epoch: 25600, loss:0.000537792919203639\n",
      "Epoch: 25700, loss:0.030590146780014038\n",
      "Game over after 196 steps\n",
      "Episode 73 is done\n",
      "Total reward 2.0\n",
      "Epoch: 25800, loss:9.144384239334613e-05\n",
      "Epoch: 25900, loss:7.399962487397715e-05\n",
      "Epoch: 26000, loss:6.145991937955841e-05\n",
      "Epoch: 26100, loss:3.597024624468759e-05\n",
      "Game over after 465 steps\n",
      "Episode 74 is done\n",
      "Total reward 4.0\n",
      "Epoch: 26200, loss:0.0007401169859804213\n",
      "Epoch: 26300, loss:0.00015069462824612856\n",
      "Epoch: 26400, loss:0.030506618320941925\n",
      "Epoch: 26500, loss:3.775636650971137e-05\n",
      "Game over after 375 steps\n",
      "Episode 75 is done\n",
      "Total reward 11.0\n",
      "Epoch: 26600, loss:9.65030922088772e-05\n",
      "Epoch: 26700, loss:0.00016309329657815397\n",
      "Epoch: 26800, loss:0.0010486977407708764\n",
      "Epoch: 26900, loss:0.00020184913591947407\n",
      "Epoch: 27000, loss:0.06239940971136093\n",
      "Epoch: 27100, loss:0.031185461208224297\n",
      "Game over after 578 steps\n",
      "Episode 76 is done\n",
      "Total reward 3.0\n",
      "Epoch: 27200, loss:0.00023037006030790508\n",
      "Epoch: 27300, loss:0.030578361824154854\n",
      "Epoch: 27400, loss:0.00010879873298108578\n",
      "Epoch: 27500, loss:0.00014564691809937358\n",
      "Game over after 421 steps\n",
      "Episode 77 is done\n",
      "Total reward 3.0\n",
      "Epoch: 27600, loss:0.0001842622586991638\n",
      "Epoch: 27700, loss:0.00014692009426653385\n",
      "Epoch: 27800, loss:9.333471825812012e-05\n",
      "Game over after 288 steps\n",
      "Episode 78 is done\n",
      "Total reward 2.0\n",
      "Epoch: 27900, loss:0.00015207400429062545\n",
      "Epoch: 28000, loss:0.0006659645005129278\n",
      "Epoch: 28100, loss:9.652337757870555e-05\n",
      "Game over after 247 steps\n",
      "Episode 79 is done\n",
      "Total reward 1.0\n",
      "Epoch: 28200, loss:0.00014913713675923645\n",
      "Epoch: 28300, loss:0.0001412823621649295\n",
      "Epoch: 28400, loss:0.0006744418642483652\n",
      "Game over after 371 steps\n",
      "Episode 80 is done\n",
      "Total reward 7.0\n",
      "Epoch: 28500, loss:0.030966857448220253\n",
      "Epoch: 28600, loss:0.00011298413301119581\n",
      "Game over after 131 steps\n",
      "Episode 81 is done\n",
      "Total reward 0.0\n",
      "Epoch: 28700, loss:0.00016037840396165848\n",
      "Epoch: 28800, loss:0.03134823963046074\n",
      "Game over after 218 steps\n",
      "Episode 82 is done\n",
      "Total reward 2.0\n",
      "Epoch: 28900, loss:0.03238571807742119\n",
      "Epoch: 29000, loss:0.49771183729171753\n",
      "Game over after 196 steps\n",
      "Episode 83 is done\n",
      "Total reward 2.0\n",
      "Epoch: 29100, loss:0.00017462816322222352\n",
      "Game over after 121 steps\n",
      "Episode 84 is done\n",
      "Total reward 0.0\n",
      "Epoch: 29200, loss:0.00014911641483195126\n",
      "Game over after 121 steps\n",
      "Episode 85 is done\n",
      "Total reward 0.0\n",
      "Epoch: 29300, loss:0.030453115701675415\n",
      "Epoch: 29400, loss:0.0009021066944114864\n",
      "Game over after 205 steps\n",
      "Episode 86 is done\n",
      "Total reward 2.0\n",
      "Epoch: 29500, loss:7.505269604735076e-05\n",
      "Epoch: 29600, loss:0.00016511989815626293\n",
      "Epoch: 29700, loss:0.00015349435852840543\n",
      "Game over after 231 steps\n",
      "Episode 87 is done\n",
      "Total reward 2.0\n",
      "Epoch: 29800, loss:0.00014711287803947926\n",
      "Epoch: 29900, loss:0.0005240084137767553\n",
      "Epoch: 30000, loss:0.030552199110388756\n",
      "Game over after 294 steps\n",
      "Episode 88 is done\n",
      "Total reward 3.0\n",
      "Epoch: 30100, loss:0.49697282910346985\n",
      "Game over after 182 steps\n",
      "Episode 89 is done\n",
      "Total reward 2.0\n",
      "Epoch: 30200, loss:0.0006063915207050741\n",
      "Epoch: 30300, loss:9.328465966973454e-05\n",
      "Epoch: 30400, loss:5.570625944528729e-05\n",
      "Epoch: 30500, loss:8.870685996953398e-05\n",
      "Game over after 396 steps\n",
      "Episode 90 is done\n",
      "Total reward 1.0\n",
      "Epoch: 30600, loss:0.00011356762115610763\n",
      "Epoch: 30700, loss:8.098791295196861e-05\n",
      "Game over after 169 steps\n",
      "Episode 91 is done\n",
      "Total reward 0.0\n",
      "Epoch: 30800, loss:0.00010586150892777368\n",
      "Epoch: 30900, loss:0.0001374637649860233\n",
      "Game over after 175 steps\n",
      "Episode 92 is done\n",
      "Total reward 1.0\n",
      "Epoch: 31000, loss:9.333714115200564e-05\n",
      "Epoch: 31100, loss:0.03071475401520729\n",
      "Game over after 233 steps\n",
      "Episode 93 is done\n",
      "Total reward 2.0\n",
      "Epoch: 31200, loss:6.03054286330007e-05\n",
      "Epoch: 31300, loss:0.031237192451953888\n",
      "Epoch: 31400, loss:5.347526166588068e-05\n",
      "Game over after 246 steps\n",
      "Episode 94 is done\n",
      "Total reward 2.0\n",
      "Epoch: 31500, loss:0.00027798034716397524\n",
      "Game over after 123 steps\n",
      "Episode 95 is done\n",
      "Total reward 0.0\n",
      "Epoch: 31600, loss:4.418146636453457e-05\n",
      "Epoch: 31700, loss:4.077221092302352e-05\n",
      "Epoch: 31800, loss:0.030750062316656113\n",
      "Game over after 306 steps\n",
      "Episode 96 is done\n",
      "Total reward 4.0\n",
      "Epoch: 31900, loss:7.508583803428337e-05\n",
      "Epoch: 32000, loss:4.271759098628536e-05\n",
      "Game over after 207 steps\n",
      "Episode 97 is done\n",
      "Total reward 2.0\n",
      "Epoch: 32100, loss:0.030757155269384384\n",
      "Epoch: 32200, loss:6.470723747042939e-05\n",
      "Game over after 153 steps\n",
      "Episode 98 is done\n",
      "Total reward 0.0\n",
      "Epoch: 32300, loss:7.19492236385122e-05\n",
      "Game over after 121 steps\n",
      "Episode 99 is done\n",
      "Total reward 0.0\n",
      "Epoch: 32400, loss:9.34084237087518e-05\n",
      "Epoch: 32500, loss:0.0003822184226009995\n",
      "Game over after 178 steps\n",
      "Episode 100 is done\n",
      "Total reward 0.0\n",
      "Epoch: 32600, loss:6.349964678520337e-05\n",
      "Epoch: 32700, loss:5.350972060114145e-05\n",
      "Game over after 284 steps\n",
      "Episode 101 is done\n",
      "Total reward 2.0\n",
      "Epoch: 32800, loss:5.677337321685627e-05\n",
      "Epoch: 32900, loss:0.030798761174082756\n",
      "Game over after 167 steps\n",
      "Episode 102 is done\n",
      "Total reward 0.0\n",
      "Epoch: 33000, loss:2.6548372261459008e-05\n",
      "Epoch: 33100, loss:4.546405398286879e-05\n",
      "Game over after 169 steps\n",
      "Episode 103 is done\n",
      "Total reward 1.0\n",
      "Epoch: 33200, loss:1.1714269930962473e-05\n",
      "Epoch: 33300, loss:4.560075467452407e-05\n",
      "Game over after 221 steps\n",
      "Episode 104 is done\n",
      "Total reward 1.0\n",
      "Epoch: 33400, loss:5.54142716282513e-05\n",
      "Game over after 142 steps\n",
      "Episode 105 is done\n",
      "Total reward 0.0\n",
      "Epoch: 33500, loss:3.8598253013333306e-05\n",
      "Epoch: 33600, loss:0.00021438529074657708\n",
      "Game over after 183 steps\n",
      "Episode 106 is done\n",
      "Total reward 1.0\n",
      "Epoch: 33700, loss:0.0002116527612088248\n",
      "Epoch: 33800, loss:1.2634601262107026e-05\n",
      "Game over after 194 steps\n",
      "Episode 107 is done\n",
      "Total reward 2.0\n",
      "Epoch: 33900, loss:5.494275683304295e-06\n",
      "Epoch: 34000, loss:0.0002784590469673276\n",
      "Epoch: 34100, loss:1.7669712178758346e-05\n",
      "Epoch: 34200, loss:0.00035865779500454664\n",
      "Game over after 325 steps\n",
      "Episode 108 is done\n",
      "Total reward 2.0\n",
      "Epoch: 34300, loss:3.9736387407174334e-05\n",
      "Epoch: 34400, loss:0.0311286598443985\n",
      "Epoch: 34500, loss:0.00027357140788808465\n",
      "Epoch: 34600, loss:0.00024586141807958484\n",
      "Epoch: 34700, loss:2.7135358322993852e-05\n",
      "Game over after 581 steps\n",
      "Episode 109 is done\n",
      "Total reward 11.0\n",
      "Epoch: 34800, loss:5.813526877318509e-05\n",
      "Epoch: 34900, loss:5.918520764680579e-05\n",
      "Epoch: 35000, loss:8.981617429526523e-05\n",
      "Epoch: 35100, loss:0.03072778880596161\n",
      "Game over after 386 steps\n",
      "Episode 110 is done\n",
      "Total reward 3.0\n",
      "Epoch: 35200, loss:5.6088512792484835e-05\n",
      "Epoch: 35300, loss:0.0011754465522244573\n",
      "Epoch: 35400, loss:0.000560425512958318\n",
      "Epoch: 35500, loss:9.119372407440096e-05\n",
      "Game over after 394 steps\n",
      "Episode 111 is done\n",
      "Total reward 2.0\n",
      "Epoch: 35600, loss:0.030537445098161697\n",
      "Epoch: 35700, loss:0.0001060891809174791\n",
      "Epoch: 35800, loss:0.00012546786456368864\n",
      "Game over after 276 steps\n",
      "Episode 112 is done\n",
      "Total reward 2.0\n",
      "Epoch: 35900, loss:4.1562325350241736e-05\n",
      "Epoch: 36000, loss:0.0004432509304024279\n",
      "Game over after 179 steps\n",
      "Episode 113 is done\n",
      "Total reward 1.0\n",
      "Epoch: 36100, loss:9.258122008759528e-05\n",
      "Game over after 121 steps\n",
      "Episode 114 is done\n",
      "Total reward 0.0\n",
      "Epoch: 36200, loss:2.3985676307347603e-05\n",
      "Epoch: 36300, loss:7.678247493458912e-05\n",
      "Game over after 157 steps\n",
      "Episode 115 is done\n",
      "Total reward 1.0\n",
      "Epoch: 36400, loss:9.360898548038676e-05\n",
      "Epoch: 36500, loss:0.00042883260175585747\n",
      "Game over after 205 steps\n",
      "Episode 116 is done\n",
      "Total reward 1.0\n",
      "Epoch: 36600, loss:0.030756963416934013\n",
      "Epoch: 36700, loss:7.915481546660885e-05\n",
      "Game over after 192 steps\n",
      "Episode 117 is done\n",
      "Total reward 2.0\n",
      "Epoch: 36800, loss:9.062243771040812e-05\n",
      "Epoch: 36900, loss:0.03089059889316559\n",
      "Game over after 243 steps\n",
      "Episode 118 is done\n",
      "Total reward 3.0\n",
      "Epoch: 37000, loss:0.00010282999573973939\n",
      "Game over after 151 steps\n",
      "Episode 119 is done\n",
      "Total reward 1.0\n",
      "Epoch: 37100, loss:4.561337846098468e-05\n",
      "Epoch: 37200, loss:0.031114570796489716\n",
      "Epoch: 37300, loss:0.000625036540441215\n",
      "TIME OUT: Model stops training after 3600.66 seconds, Save model to simple_dqn.pt\n",
      "Episode 120 is done\n",
      "Total reward 1.0\n",
      "Game over after 355 steps\n",
      "Saving simple_dqn.gif\n",
      "Epoch: 100, loss:5.457889074023115e-07\n",
      "Epoch: 200, loss:7.306632323889062e-05\n",
      "Game over after 248 steps\n",
      "Episode 1 is done\n",
      "Total reward 1.0\n",
      "Epoch: 300, loss:0.00043797504622489214\n",
      "Epoch: 400, loss:0.0011399677023291588\n",
      "Game over after 241 steps\n",
      "Episode 2 is done\n",
      "Total reward 3.0\n",
      "Epoch: 500, loss:0.03167201578617096\n",
      "Epoch: 600, loss:6.18651247350499e-05\n",
      "Epoch: 700, loss:0.030312977731227875\n",
      "Game over after 256 steps\n",
      "Episode 3 is done\n",
      "Total reward 3.0\n",
      "Epoch: 800, loss:0.03037884086370468\n",
      "Epoch: 900, loss:7.597269723191857e-05\n",
      "Epoch: 1000, loss:0.0001745452027535066\n",
      "Game over after 275 steps\n",
      "Episode 4 is done\n",
      "Total reward 3.0\n",
      "Epoch: 1100, loss:9.688896534498781e-05\n",
      "Epoch: 1200, loss:0.00011944967991439626\n",
      "Game over after 198 steps\n",
      "Episode 5 is done\n",
      "Total reward 2.0\n",
      "Epoch: 1300, loss:0.030563967302441597\n",
      "Epoch: 1400, loss:0.00011310935951769352\n",
      "Epoch: 1500, loss:0.00010156733333133161\n",
      "Game over after 316 steps\n",
      "Episode 6 is done\n",
      "Total reward 2.0\n",
      "Epoch: 1600, loss:0.031626347452402115\n",
      "Epoch: 1700, loss:0.030550731346011162\n",
      "Epoch: 1800, loss:6.431324436562136e-05\n",
      "Game over after 274 steps\n",
      "Episode 7 is done\n",
      "Total reward 2.0\n",
      "Epoch: 1900, loss:3.578528048819862e-05\n",
      "Epoch: 2000, loss:3.7645644624717534e-05\n",
      "Game over after 201 steps\n",
      "Episode 8 is done\n",
      "Total reward 0.0\n",
      "Epoch: 2100, loss:8.498518582200631e-05\n",
      "Epoch: 2200, loss:7.689282938372344e-05\n",
      "Epoch: 2300, loss:7.678953988943249e-05\n",
      "Game over after 344 steps\n",
      "Episode 9 is done\n",
      "Total reward 2.0\n",
      "Epoch: 2400, loss:8.646906644571573e-05\n",
      "Epoch: 2500, loss:0.030703742057085037\n",
      "Game over after 204 steps\n",
      "Episode 10 is done\n",
      "Total reward 1.0\n",
      "Epoch: 2600, loss:0.0008405234548263252\n",
      "Epoch: 2700, loss:6.781058618798852e-05\n",
      "Game over after 183 steps\n",
      "Episode 11 is done\n",
      "Total reward 0.0\n",
      "Epoch: 2800, loss:0.031239334493875504\n",
      "Epoch: 2900, loss:4.645860826713033e-05\n",
      "Epoch: 3000, loss:5.7007750001503155e-05\n",
      "Game over after 298 steps\n",
      "Episode 12 is done\n",
      "Total reward 5.0\n",
      "Epoch: 3100, loss:0.00026386859826743603\n",
      "Epoch: 3200, loss:0.00011418719805078581\n",
      "Epoch: 3300, loss:0.0002231885737273842\n",
      "Game over after 302 steps\n",
      "Episode 13 is done\n",
      "Total reward 7.0\n",
      "Epoch: 3400, loss:4.01488141505979e-05\n",
      "Epoch: 3500, loss:6.081768515286967e-05\n",
      "Game over after 155 steps\n",
      "Episode 14 is done\n",
      "Total reward 1.0\n",
      "Epoch: 3600, loss:8.925946895033121e-05\n",
      "Game over after 163 steps\n",
      "Episode 15 is done\n",
      "Total reward 1.0\n",
      "Epoch: 3700, loss:0.00028344205929897726\n",
      "Epoch: 3800, loss:0.061322446912527084\n",
      "Game over after 160 steps\n",
      "Episode 16 is done\n",
      "Total reward 1.0\n",
      "Epoch: 3900, loss:0.0003451967495493591\n",
      "Game over after 121 steps\n",
      "Episode 17 is done\n",
      "Total reward 0.0\n",
      "Epoch: 4000, loss:0.0007334225811064243\n",
      "Epoch: 4100, loss:0.00010623366688378155\n",
      "Game over after 187 steps\n",
      "Episode 18 is done\n",
      "Total reward 2.0\n",
      "Epoch: 4200, loss:7.706659380346537e-05\n",
      "Epoch: 4300, loss:5.024114580010064e-05\n",
      "Epoch: 4400, loss:6.569990364369005e-05\n",
      "Epoch: 4500, loss:0.030478985980153084\n",
      "Game over after 355 steps\n",
      "Episode 19 is done\n",
      "Total reward 3.0\n",
      "Epoch: 4600, loss:0.00011524898582138121\n",
      "Epoch: 4700, loss:0.030597776174545288\n",
      "Epoch: 4800, loss:0.00010525571269681677\n",
      "Game over after 325 steps\n",
      "Episode 20 is done\n",
      "Total reward 5.0\n",
      "Epoch: 4900, loss:0.03121035173535347\n",
      "Epoch: 5000, loss:0.00013620723620988429\n",
      "Game over after 202 steps\n",
      "Episode 21 is done\n",
      "Total reward 2.0\n",
      "Epoch: 5100, loss:0.030557341873645782\n",
      "Epoch: 5200, loss:0.0002764957898762077\n",
      "Game over after 230 steps\n",
      "Episode 22 is done\n",
      "Total reward 2.0\n",
      "Epoch: 5300, loss:0.00016154901823028922\n",
      "Epoch: 5400, loss:7.529099093517289e-05\n",
      "Epoch: 5500, loss:8.17840191302821e-05\n",
      "Game over after 267 steps\n",
      "Episode 23 is done\n",
      "Total reward 4.0\n",
      "Epoch: 5600, loss:0.0001770186354406178\n",
      "Game over after 154 steps\n",
      "Episode 24 is done\n",
      "Total reward 1.0\n",
      "Epoch: 5700, loss:0.030790654942393303\n",
      "Epoch: 5800, loss:0.00011949767940677702\n",
      "Epoch: 5900, loss:0.03039827197790146\n",
      "Game over after 218 steps\n",
      "Episode 25 is done\n",
      "Total reward 2.0\n",
      "Epoch: 6000, loss:4.844709110329859e-05\n",
      "Game over after 186 steps\n",
      "Episode 26 is done\n",
      "Total reward 2.0\n",
      "Epoch: 6100, loss:0.03126821666955948\n",
      "Epoch: 6200, loss:0.00011033596820198\n",
      "Epoch: 6300, loss:0.030877213925123215\n",
      "Game over after 264 steps\n",
      "Episode 27 is done\n",
      "Total reward 1.0\n",
      "Epoch: 6400, loss:0.00015330876340158284\n",
      "Epoch: 6500, loss:7.55000946810469e-05\n",
      "Epoch: 6600, loss:0.031049370765686035\n",
      "Epoch: 6700, loss:7.629537867615e-05\n",
      "Game over after 404 steps\n",
      "Episode 28 is done\n",
      "Total reward 1.0\n",
      "Epoch: 6800, loss:8.794300811132416e-05\n",
      "Epoch: 6900, loss:7.319844735320657e-05\n",
      "Epoch: 7000, loss:0.030686931684613228\n",
      "Epoch: 7100, loss:0.030964868143200874\n",
      "Game over after 347 steps\n",
      "Episode 29 is done\n",
      "Total reward 1.0\n",
      "Epoch: 7200, loss:4.7885343519737944e-05\n",
      "Epoch: 7300, loss:5.557840268011205e-05\n",
      "Game over after 273 steps\n",
      "Episode 30 is done\n",
      "Total reward 2.0\n",
      "Epoch: 7400, loss:6.36014374322258e-05\n",
      "Epoch: 7500, loss:7.662423013243824e-05\n",
      "Epoch: 7600, loss:5.1630428060889244e-05\n",
      "Game over after 242 steps\n",
      "Episode 31 is done\n",
      "Total reward 0.0\n",
      "Epoch: 7700, loss:3.27225134242326e-05\n",
      "Epoch: 7800, loss:0.00010611771722324193\n",
      "Game over after 260 steps\n",
      "Episode 32 is done\n",
      "Total reward 0.0\n",
      "Epoch: 7900, loss:5.2038383728358895e-05\n",
      "Epoch: 8000, loss:3.145341543131508e-05\n",
      "Epoch: 8100, loss:4.709259155788459e-05\n",
      "Game over after 291 steps\n",
      "Episode 33 is done\n",
      "Total reward 2.0\n",
      "Epoch: 8200, loss:0.0002280244807479903\n",
      "Epoch: 8300, loss:2.0677563952631317e-05\n",
      "Epoch: 8400, loss:2.3867036361480132e-05\n",
      "Epoch: 8500, loss:0.00014442607061937451\n",
      "Epoch: 8600, loss:2.2370251826941967e-05\n",
      "Game over after 469 steps\n",
      "Episode 34 is done\n",
      "Total reward 6.0\n",
      "Epoch: 8700, loss:3.178694169037044e-05\n",
      "Epoch: 8800, loss:6.452619709307328e-05\n",
      "Epoch: 8900, loss:0.062308475375175476\n",
      "Epoch: 9000, loss:5.6241784477606416e-05\n",
      "Game over after 449 steps\n",
      "Episode 35 is done\n",
      "Total reward 9.0\n",
      "Epoch: 9100, loss:0.00013517154729925096\n",
      "Epoch: 9200, loss:0.03058343380689621\n",
      "Epoch: 9300, loss:3.4583776141516864e-05\n",
      "Epoch: 9400, loss:0.0002112227084580809\n",
      "Epoch: 9500, loss:0.00012574468564707786\n",
      "Epoch: 9600, loss:5.4134179663378745e-05\n",
      "Game over after 591 steps\n",
      "Episode 36 is done\n",
      "Total reward 2.0\n",
      "Epoch: 9700, loss:0.0002689397078938782\n",
      "Epoch: 9800, loss:0.00011974673543591052\n",
      "Epoch: 9900, loss:5.6051798310363665e-05\n",
      "Epoch: 10000, loss:0.497049480676651\n",
      "Game over after 332 steps\n",
      "Episode 37 is done\n",
      "Total reward 3.0\n",
      "Epoch: 10100, loss:0.03040151856839657\n",
      "Epoch: 10200, loss:5.095750748296268e-05\n",
      "Epoch: 10300, loss:0.00018430253840051591\n",
      "Epoch: 10400, loss:0.00017087061132770032\n",
      "Epoch: 10500, loss:0.03057711198925972\n",
      "Epoch: 10600, loss:0.00013614243653137237\n",
      "Game over after 606 steps\n",
      "Episode 38 is done\n",
      "Total reward 2.0\n",
      "Epoch: 10700, loss:0.030658608302474022\n",
      "Epoch: 10800, loss:0.0001103730610338971\n",
      "Epoch: 10900, loss:0.00010280760034220293\n",
      "Epoch: 11000, loss:0.4984639883041382\n",
      "Game over after 454 steps\n",
      "Episode 39 is done\n",
      "Total reward 7.0\n",
      "Epoch: 11100, loss:5.9637379308696836e-05\n",
      "Epoch: 11200, loss:0.0001081721275113523\n",
      "Game over after 200 steps\n",
      "Episode 40 is done\n",
      "Total reward 2.0\n",
      "Epoch: 11300, loss:0.0001779740850906819\n",
      "Epoch: 11400, loss:0.00023173016961663961\n",
      "Game over after 121 steps\n",
      "Episode 41 is done\n",
      "Total reward 0.0\n",
      "Epoch: 11500, loss:8.871539466781542e-05\n",
      "Epoch: 11600, loss:8.869388693710789e-05\n",
      "Game over after 260 steps\n",
      "Episode 42 is done\n",
      "Total reward 0.0\n",
      "Epoch: 11700, loss:3.4281907574040815e-05\n",
      "Epoch: 11800, loss:4.3082534830318764e-05\n",
      "Epoch: 11900, loss:0.030584270134568214\n",
      "Game over after 245 steps\n",
      "Episode 43 is done\n",
      "Total reward 1.0\n",
      "Epoch: 12000, loss:0.06164254620671272\n",
      "Epoch: 12100, loss:5.143943053553812e-05\n",
      "Game over after 242 steps\n",
      "Episode 44 is done\n",
      "Total reward 1.0\n",
      "Epoch: 12200, loss:0.0003109811805188656\n",
      "Epoch: 12300, loss:5.5261716624954715e-05\n",
      "Epoch: 12400, loss:5.013249028706923e-05\n",
      "Epoch: 12500, loss:1.8463390006218106e-05\n",
      "Game over after 353 steps\n",
      "Episode 45 is done\n",
      "Total reward 1.0\n",
      "Epoch: 12600, loss:0.06144079938530922\n",
      "Epoch: 12700, loss:0.00027460517594590783\n",
      "Epoch: 12800, loss:3.987480522482656e-05\n",
      "Game over after 373 steps\n",
      "Episode 46 is done\n",
      "Total reward 3.0\n",
      "Epoch: 12900, loss:3.7082212656969205e-05\n",
      "Epoch: 13000, loss:0.0001508815330453217\n",
      "Game over after 212 steps\n",
      "Episode 47 is done\n",
      "Total reward 2.0\n",
      "Epoch: 13100, loss:6.365257286233827e-05\n",
      "Epoch: 13200, loss:8.634554251329973e-05\n",
      "Epoch: 13300, loss:4.605608410201967e-05\n",
      "Epoch: 13400, loss:0.00041788662201724946\n",
      "Epoch: 13500, loss:3.99707569158636e-05\n",
      "Game over after 444 steps\n",
      "Episode 48 is done\n",
      "Total reward 4.0\n",
      "Epoch: 13600, loss:3.2675299735274166e-05\n",
      "Epoch: 13700, loss:6.216968904482201e-05\n",
      "Epoch: 13800, loss:2.7279929781798273e-05\n",
      "Epoch: 13900, loss:6.298804510151967e-05\n",
      "Epoch: 14000, loss:0.00027923929155804217\n",
      "Epoch: 14100, loss:3.736946382559836e-05\n",
      "Game over after 561 steps\n",
      "Episode 49 is done\n",
      "Total reward 4.0\n",
      "Epoch: 14200, loss:4.0078972233459353e-05\n",
      "Epoch: 14300, loss:4.273651939001866e-05\n",
      "Epoch: 14400, loss:0.03056851401925087\n",
      "Epoch: 14500, loss:6.550136458827183e-05\n",
      "Game over after 426 steps\n",
      "Episode 50 is done\n",
      "Total reward 3.0\n",
      "Epoch: 14600, loss:5.401122325565666e-05\n",
      "Epoch: 14700, loss:1.4665316484752111e-05\n",
      "Epoch: 14800, loss:1.2484114449762274e-05\n",
      "Epoch: 14900, loss:9.882028098218143e-05\n",
      "Epoch: 15000, loss:2.6653924578567967e-05\n",
      "Game over after 528 steps\n",
      "Episode 51 is done\n",
      "Total reward 3.0\n",
      "Epoch: 15100, loss:6.609117554035038e-05\n",
      "Game over after 121 steps\n",
      "Episode 52 is done\n",
      "Total reward 0.0\n",
      "Epoch: 15200, loss:7.790823292452842e-05\n",
      "Epoch: 15300, loss:0.0006037511047907174\n",
      "Epoch: 15400, loss:3.8341895560733974e-05\n",
      "Game over after 230 steps\n",
      "Episode 53 is done\n",
      "Total reward 2.0\n",
      "Epoch: 15500, loss:5.497819438460283e-05\n",
      "Epoch: 15600, loss:0.03074863739311695\n",
      "Epoch: 15700, loss:0.030917592346668243\n",
      "Game over after 342 steps\n",
      "Episode 54 is done\n",
      "Total reward 3.0\n",
      "Epoch: 15800, loss:6.41934821032919e-05\n",
      "Epoch: 15900, loss:8.25131282908842e-05\n",
      "Epoch: 16000, loss:0.06127200648188591\n",
      "Game over after 288 steps\n",
      "Episode 55 is done\n",
      "Total reward 2.0\n",
      "Epoch: 16100, loss:6.862229201942682e-05\n",
      "Epoch: 16200, loss:4.900649582850747e-05\n",
      "Epoch: 16300, loss:4.145826824242249e-05\n",
      "Epoch: 16400, loss:2.030365794780664e-05\n",
      "Game over after 446 steps\n",
      "Episode 56 is done\n",
      "Total reward 3.0\n",
      "Epoch: 16500, loss:3.527911030687392e-05\n",
      "Epoch: 16600, loss:0.030712932348251343\n",
      "Epoch: 16700, loss:4.210050974506885e-05\n",
      "Epoch: 16800, loss:0.030954288318753242\n",
      "Epoch: 16900, loss:8.198020805139095e-05\n",
      "Game over after 424 steps\n",
      "Episode 57 is done\n",
      "Total reward 2.0\n",
      "Epoch: 17000, loss:8.515993249602616e-05\n",
      "Epoch: 17100, loss:6.115090218372643e-05\n",
      "Game over after 275 steps\n",
      "Episode 58 is done\n",
      "Total reward 4.0\n",
      "Epoch: 17200, loss:8.79729341249913e-05\n",
      "Epoch: 17300, loss:6.182697688927874e-05\n",
      "Epoch: 17400, loss:3.667983764898963e-05\n",
      "Game over after 300 steps\n",
      "Episode 59 is done\n",
      "Total reward 3.0\n",
      "Epoch: 17500, loss:0.0001185025175800547\n",
      "Epoch: 17600, loss:0.03106168657541275\n",
      "Epoch: 17700, loss:4.982025347999297e-05\n",
      "Epoch: 17800, loss:7.512816955568269e-05\n",
      "Game over after 383 steps\n",
      "Episode 60 is done\n",
      "Total reward 1.0\n",
      "Epoch: 17900, loss:6.381454295478761e-05\n",
      "Epoch: 18000, loss:0.00039701693458482623\n",
      "Epoch: 18100, loss:5.3056202887091786e-05\n",
      "Game over after 227 steps\n",
      "Episode 61 is done\n",
      "Total reward 2.0\n",
      "Epoch: 18200, loss:0.030503761023283005\n",
      "Epoch: 18300, loss:7.028596883174032e-05\n",
      "Game over after 275 steps\n",
      "Episode 62 is done\n",
      "Total reward 3.0\n",
      "Epoch: 18400, loss:0.03064071759581566\n",
      "Epoch: 18500, loss:6.015439430484548e-05\n",
      "Epoch: 18600, loss:7.987854041857645e-05\n",
      "Epoch: 18700, loss:0.031313322484493256\n",
      "Game over after 343 steps\n",
      "Episode 63 is done\n",
      "Total reward 3.0\n",
      "Epoch: 18800, loss:0.00012569790123961866\n",
      "Epoch: 18900, loss:8.790694118943065e-05\n",
      "Game over after 225 steps\n",
      "Episode 64 is done\n",
      "Total reward 1.0\n",
      "Epoch: 19000, loss:0.00012307045108173043\n",
      "Epoch: 19100, loss:0.00011875797645188868\n",
      "Game over after 224 steps\n",
      "Episode 65 is done\n",
      "Total reward 2.0\n",
      "Epoch: 19200, loss:0.00013495006714947522\n",
      "Epoch: 19300, loss:0.00012393460201565176\n",
      "Epoch: 19400, loss:0.03086552955210209\n",
      "Game over after 224 steps\n",
      "Episode 66 is done\n",
      "Total reward 1.0\n",
      "Epoch: 19500, loss:0.030841514468193054\n",
      "Epoch: 19600, loss:0.030984826385974884\n",
      "Game over after 211 steps\n",
      "Episode 67 is done\n",
      "Total reward 1.0\n",
      "Epoch: 19700, loss:0.030916228890419006\n",
      "Epoch: 19800, loss:0.03063114546239376\n",
      "Game over after 184 steps\n",
      "Episode 68 is done\n",
      "Total reward 0.0\n",
      "Epoch: 19900, loss:0.00036203840863890946\n",
      "Epoch: 20000, loss:6.925733032403514e-05\n",
      "Game over after 269 steps\n",
      "Episode 69 is done\n",
      "Total reward 3.0\n",
      "Epoch: 20100, loss:4.1237868572352454e-05\n",
      "Epoch: 20200, loss:0.00010426279186503962\n",
      "Epoch: 20300, loss:4.163559788139537e-05\n",
      "Epoch: 20400, loss:0.030846336856484413\n",
      "Game over after 360 steps\n",
      "Episode 70 is done\n",
      "Total reward 3.0\n",
      "Epoch: 20500, loss:0.00010245853627566248\n",
      "Epoch: 20600, loss:0.00011740556510630995\n",
      "Game over after 259 steps\n",
      "Episode 71 is done\n",
      "Total reward 2.0\n",
      "Epoch: 20700, loss:0.031207147985696793\n",
      "Epoch: 20800, loss:8.316200546687469e-05\n",
      "Epoch: 20900, loss:0.00045862761908210814\n",
      "Game over after 222 steps\n",
      "Episode 72 is done\n",
      "Total reward 2.0\n",
      "Epoch: 21000, loss:6.395731179509312e-05\n",
      "Game over after 122 steps\n",
      "Episode 73 is done\n",
      "Total reward 0.0\n",
      "Epoch: 21100, loss:0.031020253896713257\n",
      "Epoch: 21200, loss:6.134970317361876e-05\n",
      "Game over after 257 steps\n",
      "Episode 74 is done\n",
      "Total reward 2.0\n",
      "Epoch: 21300, loss:0.03102373518049717\n",
      "Epoch: 21400, loss:0.0309663787484169\n",
      "Epoch: 21500, loss:4.359043305157684e-05\n",
      "Game over after 243 steps\n",
      "Episode 75 is done\n",
      "Total reward 0.0\n",
      "Epoch: 21600, loss:2.3852751837694086e-05\n",
      "Epoch: 21700, loss:3.652463783510029e-05\n",
      "Game over after 175 steps\n",
      "Episode 76 is done\n",
      "Total reward 0.0\n",
      "Epoch: 21800, loss:0.03066312149167061\n",
      "Epoch: 21900, loss:0.0005294838338159025\n",
      "Epoch: 22000, loss:3.387985270819627e-05\n",
      "Game over after 364 steps\n",
      "Episode 77 is done\n",
      "Total reward 4.0\n",
      "Epoch: 22100, loss:0.030941898003220558\n",
      "Epoch: 22200, loss:3.0985545890871435e-05\n",
      "Epoch: 22300, loss:4.018255276605487e-05\n",
      "Epoch: 22400, loss:2.4633876819279976e-05\n",
      "Epoch: 22500, loss:0.00011835045006591827\n",
      "Game over after 514 steps\n",
      "Episode 78 is done\n",
      "Total reward 6.0\n",
      "Epoch: 22600, loss:7.926784019218758e-05\n",
      "Epoch: 22700, loss:3.733170888153836e-05\n",
      "Epoch: 22800, loss:0.0008509199833497405\n",
      "Game over after 290 steps\n",
      "Episode 79 is done\n",
      "Total reward 0.0\n",
      "Epoch: 22900, loss:8.274074934888631e-05\n",
      "Epoch: 23000, loss:0.00010885197116294876\n",
      "Game over after 191 steps\n",
      "Episode 80 is done\n",
      "Total reward 0.0\n",
      "Epoch: 23100, loss:0.030567852780222893\n",
      "Epoch: 23200, loss:0.06144570931792259\n",
      "Game over after 203 steps\n",
      "Episode 81 is done\n",
      "Total reward 1.0\n",
      "Epoch: 23300, loss:3.815197851508856e-05\n",
      "Epoch: 23400, loss:7.719932909822091e-05\n",
      "Epoch: 23500, loss:3.695783016155474e-05\n",
      "Game over after 302 steps\n",
      "Episode 82 is done\n",
      "Total reward 3.0\n",
      "Epoch: 23600, loss:4.250936763128266e-05\n",
      "Epoch: 23700, loss:0.0005548960180021822\n",
      "Game over after 191 steps\n",
      "Episode 83 is done\n",
      "Total reward 1.0\n",
      "Epoch: 23800, loss:9.78668758762069e-05\n",
      "Epoch: 23900, loss:0.03073384426534176\n",
      "Epoch: 24000, loss:0.0309736467897892\n",
      "Game over after 307 steps\n",
      "Episode 84 is done\n",
      "Total reward 2.0\n",
      "Epoch: 24100, loss:5.471451731864363e-05\n",
      "Epoch: 24200, loss:5.222550316830166e-05\n",
      "Game over after 207 steps\n",
      "Episode 85 is done\n",
      "Total reward 1.0\n",
      "Epoch: 24300, loss:3.631225990829989e-05\n",
      "Epoch: 24400, loss:7.870585250202566e-05\n",
      "Epoch: 24500, loss:0.030805818736553192\n",
      "Game over after 284 steps\n",
      "Episode 86 is done\n",
      "Total reward 3.0\n",
      "Epoch: 24600, loss:5.525291635422036e-05\n",
      "Epoch: 24700, loss:4.668976180255413e-05\n",
      "Epoch: 24800, loss:0.0002766890684142709\n",
      "Epoch: 24900, loss:0.00033788688597269356\n",
      "Game over after 333 steps\n",
      "Episode 87 is done\n",
      "Total reward 3.0\n",
      "Epoch: 25000, loss:0.00030728214187547565\n",
      "Epoch: 25100, loss:5.432985199149698e-05\n",
      "Epoch: 25200, loss:6.271992606343701e-05\n",
      "Game over after 287 steps\n",
      "Episode 88 is done\n",
      "Total reward 2.0\n",
      "Epoch: 25300, loss:3.598204057198018e-05\n",
      "Epoch: 25400, loss:6.49339344818145e-05\n",
      "Epoch: 25500, loss:0.00011819748760899529\n",
      "Epoch: 25600, loss:6.928609946044162e-05\n",
      "Epoch: 25700, loss:8.530736522516236e-05\n",
      "Game over after 513 steps\n",
      "Episode 89 is done\n",
      "Total reward 2.0\n",
      "Epoch: 25800, loss:8.385344699490815e-05\n",
      "Epoch: 25900, loss:4.597912993631326e-05\n",
      "Game over after 249 steps\n",
      "Episode 90 is done\n",
      "Total reward 1.0\n",
      "Epoch: 26000, loss:3.22118321491871e-05\n",
      "Epoch: 26100, loss:3.1077175663085654e-05\n",
      "Epoch: 26200, loss:7.843250932637602e-05\n",
      "Game over after 304 steps\n",
      "Episode 91 is done\n",
      "Total reward 1.0\n",
      "Epoch: 26300, loss:3.0658811738248914e-05\n",
      "Epoch: 26400, loss:2.9937582439742982e-05\n",
      "Game over after 168 steps\n",
      "Episode 92 is done\n",
      "Total reward 0.0\n",
      "Epoch: 26500, loss:5.202308238949627e-05\n",
      "Epoch: 26600, loss:2.5962024665204808e-05\n",
      "Game over after 228 steps\n",
      "Episode 93 is done\n",
      "Total reward 1.0\n",
      "Epoch: 26700, loss:5.1280338084325194e-05\n",
      "Epoch: 26800, loss:0.00010872426355490461\n",
      "Epoch: 26900, loss:7.373087282758206e-05\n",
      "Epoch: 27000, loss:1.917195004352834e-05\n",
      "Game over after 391 steps\n",
      "Episode 94 is done\n",
      "Total reward 1.0\n",
      "Epoch: 27100, loss:4.873304715147242e-05\n",
      "Epoch: 27200, loss:2.3168324332800694e-05\n",
      "Epoch: 27300, loss:6.13881929893978e-05\n",
      "Game over after 269 steps\n",
      "Episode 95 is done\n",
      "Total reward 2.0\n",
      "Epoch: 27400, loss:0.00022704267757944763\n",
      "Epoch: 27500, loss:3.171735079376958e-05\n",
      "Game over after 240 steps\n",
      "Episode 96 is done\n",
      "Total reward 2.0\n",
      "Epoch: 27600, loss:3.37299206876196e-05\n",
      "Epoch: 27700, loss:0.03119603358209133\n",
      "Game over after 223 steps\n",
      "Episode 97 is done\n",
      "Total reward 3.0\n",
      "Epoch: 27800, loss:0.03089415468275547\n",
      "Epoch: 27900, loss:4.017751052742824e-05\n",
      "Epoch: 28000, loss:5.1665578212123364e-05\n",
      "Game over after 205 steps\n",
      "Episode 98 is done\n",
      "Total reward 2.0\n",
      "Epoch: 28100, loss:6.422937440220267e-05\n",
      "Game over after 180 steps\n",
      "Episode 99 is done\n",
      "Total reward 0.0\n",
      "Epoch: 28200, loss:0.030572114512324333\n",
      "Epoch: 28300, loss:4.918439663015306e-05\n",
      "Game over after 215 steps\n",
      "Episode 100 is done\n",
      "Total reward 2.0\n",
      "Epoch: 28400, loss:0.030756831169128418\n",
      "Epoch: 28500, loss:4.453988731256686e-05\n",
      "Epoch: 28600, loss:4.8312849685316905e-05\n",
      "Game over after 227 steps\n",
      "Episode 101 is done\n",
      "Total reward 2.0\n",
      "Epoch: 28700, loss:5.053877612226643e-05\n",
      "Epoch: 28800, loss:0.030701300129294395\n",
      "Game over after 201 steps\n",
      "Episode 102 is done\n",
      "Total reward 2.0\n",
      "Epoch: 28900, loss:0.00015434644592460245\n",
      "Epoch: 29000, loss:0.030648307874798775\n",
      "Epoch: 29100, loss:6.268505967454985e-05\n",
      "Game over after 272 steps\n",
      "Episode 103 is done\n",
      "Total reward 2.0\n",
      "Epoch: 29200, loss:0.030577586963772774\n",
      "Game over after 121 steps\n",
      "Episode 104 is done\n",
      "Total reward 0.0\n",
      "Epoch: 29300, loss:6.693573232041672e-05\n",
      "Epoch: 29400, loss:7.008672400843352e-05\n",
      "Game over after 203 steps\n",
      "Episode 105 is done\n",
      "Total reward 2.0\n",
      "Epoch: 29500, loss:4.706153413280845e-05\n",
      "Epoch: 29600, loss:2.8556409233715385e-05\n",
      "Game over after 220 steps\n",
      "Episode 106 is done\n",
      "Total reward 0.0\n",
      "Epoch: 29700, loss:0.0002807459095492959\n",
      "Epoch: 29800, loss:1.8239632481709123e-05\n",
      "Game over after 189 steps\n",
      "Episode 107 is done\n",
      "Total reward 0.0\n",
      "Epoch: 29900, loss:5.719766704714857e-05\n",
      "Epoch: 30000, loss:9.029036300489679e-05\n",
      "Game over after 186 steps\n",
      "Episode 108 is done\n",
      "Total reward 1.0\n",
      "Epoch: 30100, loss:3.44116342603229e-05\n",
      "Game over after 133 steps\n",
      "Episode 109 is done\n",
      "Total reward 0.0\n",
      "Epoch: 30200, loss:7.366787031060085e-05\n",
      "Epoch: 30300, loss:0.03131901100277901\n",
      "Game over after 221 steps\n",
      "Episode 110 is done\n",
      "Total reward 2.0\n",
      "Epoch: 30400, loss:4.01991383114364e-05\n",
      "Epoch: 30500, loss:5.524038442672463e-06\n",
      "Epoch: 30600, loss:2.276638406328857e-05\n",
      "Game over after 310 steps\n",
      "Episode 111 is done\n",
      "Total reward 3.0\n",
      "Epoch: 30700, loss:4.641918349079788e-05\n",
      "Epoch: 30800, loss:4.627526504918933e-05\n",
      "Epoch: 30900, loss:4.108688881387934e-05\n",
      "Game over after 255 steps\n",
      "Episode 112 is done\n",
      "Total reward 2.0\n",
      "Epoch: 31000, loss:3.705213021021336e-05\n",
      "Epoch: 31100, loss:4.21531185565982e-05\n",
      "Game over after 236 steps\n",
      "Episode 113 is done\n",
      "Total reward 3.0\n",
      "Epoch: 31200, loss:6.747737643308938e-05\n",
      "Epoch: 31300, loss:0.030819961801171303\n",
      "Epoch: 31400, loss:8.365399116883054e-05\n",
      "Epoch: 31500, loss:7.93422368587926e-05\n",
      "Game over after 404 steps\n",
      "Episode 114 is done\n",
      "Total reward 11.0\n",
      "Epoch: 31600, loss:3.9119691791711375e-05\n",
      "Epoch: 31700, loss:0.0001556773204356432\n",
      "Epoch: 31800, loss:8.917986997403204e-05\n",
      "Game over after 211 steps\n",
      "Episode 115 is done\n",
      "Total reward 3.0\n",
      "Epoch: 31900, loss:0.00048272553249262273\n",
      "Epoch: 32000, loss:0.0001784668565960601\n",
      "Game over after 265 steps\n",
      "Episode 116 is done\n",
      "Total reward 4.0\n",
      "Epoch: 32100, loss:0.0009840585989877582\n",
      "Epoch: 32200, loss:0.5262975096702576\n",
      "Epoch: 32300, loss:0.0605451799929142\n",
      "Game over after 275 steps\n",
      "Episode 117 is done\n",
      "Total reward 4.0\n",
      "Epoch: 32400, loss:0.0015676772454753518\n",
      "Game over after 121 steps\n",
      "Episode 118 is done\n",
      "Total reward 0.0\n",
      "Epoch: 32500, loss:0.0004403047787491232\n",
      "Game over after 121 steps\n",
      "Episode 119 is done\n",
      "Total reward 0.0\n",
      "Epoch: 32600, loss:0.00014555059897247702\n",
      "Epoch: 32700, loss:0.00016619933012407273\n",
      "Epoch: 32800, loss:0.00014835660113021731\n",
      "Game over after 287 steps\n",
      "Episode 120 is done\n",
      "Total reward 3.0\n",
      "Epoch: 32900, loss:0.00012957473518326879\n",
      "Game over after 121 steps\n",
      "Episode 121 is done\n",
      "Total reward 0.0\n",
      "Epoch: 33000, loss:0.00023269731900654733\n",
      "Epoch: 33100, loss:0.00021991939865984023\n",
      "Game over after 156 steps\n",
      "Episode 122 is done\n",
      "Total reward 1.0\n",
      "Epoch: 33200, loss:0.00043398624984547496\n",
      "Game over after 121 steps\n",
      "Episode 123 is done\n",
      "Total reward 0.0\n",
      "Epoch: 33300, loss:0.0002451417676638812\n",
      "Epoch: 33400, loss:0.00018102435569744557\n",
      "Epoch: 33500, loss:0.00023013172904029489\n",
      "Game over after 271 steps\n",
      "Episode 124 is done\n",
      "Total reward 2.0\n",
      "Epoch: 33600, loss:0.0007937775226309896\n",
      "Epoch: 33700, loss:0.030758583918213844\n",
      "Epoch: 33800, loss:0.00013016874436289072\n",
      "Epoch: 33900, loss:0.0001710549695417285\n",
      "Game over after 371 steps\n",
      "Episode 125 is done\n",
      "Total reward 5.0\n",
      "Epoch: 34000, loss:0.0001759425940690562\n",
      "Epoch: 34100, loss:4.387424633023329e-05\n",
      "Epoch: 34200, loss:6.764059799024835e-05\n",
      "Game over after 280 steps\n",
      "Episode 126 is done\n",
      "Total reward 2.0\n",
      "Epoch: 34300, loss:7.65211007092148e-05\n",
      "Epoch: 34400, loss:0.0010791259119287133\n",
      "Epoch: 34500, loss:0.030747344717383385\n",
      "Epoch: 34600, loss:6.353227945510298e-05\n",
      "Game over after 428 steps\n",
      "Episode 127 is done\n",
      "Total reward 2.0\n",
      "Epoch: 34700, loss:0.0006586642120964825\n",
      "Epoch: 34800, loss:6.531788676511496e-05\n",
      "Epoch: 34900, loss:0.0004390006943140179\n",
      "Epoch: 35000, loss:0.00011436521162977442\n",
      "Epoch: 35100, loss:0.00012892193626612425\n",
      "Epoch: 35200, loss:9.261134982807562e-05\n",
      "Game over after 583 steps\n",
      "Episode 128 is done\n",
      "Total reward 3.0\n",
      "Epoch: 35300, loss:0.061530951410532\n",
      "Epoch: 35400, loss:7.553354953415692e-05\n",
      "Epoch: 35500, loss:4.855079896515235e-05\n",
      "Epoch: 35600, loss:4.6992950956337154e-05\n",
      "Game over after 455 steps\n",
      "Episode 129 is done\n",
      "Total reward 2.0\n",
      "Epoch: 35700, loss:4.282364170649089e-05\n",
      "Epoch: 35800, loss:0.030757052823901176\n",
      "Epoch: 35900, loss:9.829325426835567e-05\n",
      "Epoch: 36000, loss:7.983876275829971e-05\n",
      "Game over after 337 steps\n",
      "Episode 130 is done\n",
      "Total reward 3.0\n",
      "Epoch: 36100, loss:0.00013943192607257515\n",
      "Epoch: 36200, loss:3.323503187857568e-05\n",
      "Epoch: 36300, loss:7.131727761588991e-05\n",
      "Game over after 308 steps\n",
      "Episode 131 is done\n",
      "Total reward 4.0\n",
      "Epoch: 36400, loss:0.00012559538299683481\n",
      "Epoch: 36500, loss:4.475378955248743e-05\n",
      "Epoch: 36600, loss:0.030501823872327805\n",
      "Game over after 296 steps\n",
      "Episode 132 is done\n",
      "Total reward 3.0\n",
      "Epoch: 36700, loss:4.300957880332135e-05\n",
      "Epoch: 36800, loss:0.030531184747815132\n",
      "Game over after 255 steps\n",
      "Episode 133 is done\n",
      "Total reward 4.0\n",
      "Epoch: 36900, loss:0.030406000092625618\n",
      "Epoch: 37000, loss:0.0002219263114966452\n",
      "Game over after 178 steps\n",
      "Episode 134 is done\n",
      "Total reward 1.0\n",
      "Epoch: 37100, loss:7.5267962529324e-05\n",
      "Epoch: 37200, loss:0.03126528114080429\n",
      "Game over after 221 steps\n",
      "Episode 135 is done\n",
      "Total reward 2.0\n",
      "Epoch: 37300, loss:9.563566709402949e-05\n",
      "Epoch: 37400, loss:0.00011258137237746269\n",
      "Game over after 207 steps\n",
      "Episode 136 is done\n",
      "Total reward 2.0\n",
      "Epoch: 37500, loss:0.030337687581777573\n",
      "Epoch: 37600, loss:7.98821565695107e-05\n",
      "Epoch: 37700, loss:5.734678779845126e-05\n",
      "Game over after 308 steps\n",
      "Episode 137 is done\n",
      "Total reward 3.0\n",
      "Epoch: 37800, loss:6.003951421007514e-05\n",
      "Epoch: 37900, loss:0.00012755575880873948\n",
      "Epoch: 38000, loss:8.676795550854877e-05\n",
      "Game over after 243 steps\n",
      "Episode 138 is done\n",
      "Total reward 1.0\n",
      "Epoch: 38100, loss:7.86882228567265e-05\n",
      "Epoch: 38200, loss:0.030909722670912743\n",
      "Game over after 174 steps\n",
      "Episode 139 is done\n",
      "Total reward 0.0\n",
      "Epoch: 38300, loss:8.410988812102005e-05\n",
      "Epoch: 38400, loss:0.030908839777112007\n",
      "Epoch: 38500, loss:0.030734732747077942\n",
      "Game over after 302 steps\n",
      "Episode 140 is done\n",
      "Total reward 3.0\n",
      "Epoch: 38600, loss:3.29573085764423e-05\n",
      "Epoch: 38700, loss:5.7845303672365844e-05\n",
      "Game over after 262 steps\n",
      "Episode 141 is done\n",
      "Total reward 3.0\n",
      "Epoch: 38800, loss:8.26717441668734e-05\n",
      "Epoch: 38900, loss:5.564796447288245e-05\n",
      "Epoch: 39000, loss:9.410967322764918e-05\n",
      "Game over after 293 steps\n",
      "Episode 142 is done\n",
      "Total reward 3.0\n",
      "Epoch: 39100, loss:7.351295062107965e-05\n",
      "Epoch: 39200, loss:6.841368303867057e-05\n",
      "Epoch: 39300, loss:0.03106478415429592\n",
      "Epoch: 39400, loss:0.00010858497262233868\n",
      "Epoch: 39500, loss:0.03095731884241104\n",
      "Epoch: 39600, loss:9.729339217301458e-05\n",
      "Game over after 605 steps\n",
      "Episode 143 is done\n",
      "Total reward 4.0\n",
      "Epoch: 39700, loss:0.00012379030522424728\n",
      "Epoch: 39800, loss:5.8936238929163665e-05\n",
      "Epoch: 39900, loss:0.00033567615901120007\n",
      "Game over after 292 steps\n",
      "Episode 144 is done\n",
      "Total reward 1.0\n",
      "Epoch: 40000, loss:0.030734235420823097\n",
      "Epoch: 40100, loss:0.030897125601768494\n",
      "Epoch: 40200, loss:8.257973240688443e-05\n",
      "Epoch: 40300, loss:8.53386809467338e-05\n",
      "Epoch: 40400, loss:0.03078361041843891\n",
      "Epoch: 40500, loss:4.906912727165036e-05\n",
      "Game over after 631 steps\n",
      "Episode 145 is done\n",
      "Total reward 7.0\n",
      "Epoch: 40600, loss:6.393223884515464e-05\n",
      "Epoch: 40700, loss:0.0003822463331744075\n",
      "Epoch: 40800, loss:2.9326862204470672e-05\n",
      "Epoch: 40900, loss:0.030558016151189804\n",
      "Epoch: 41000, loss:0.00011247504153288901\n",
      "Epoch: 41100, loss:0.0309901162981987\n",
      "Game over after 595 steps\n",
      "Episode 146 is done\n",
      "Total reward 3.0\n",
      "Epoch: 41200, loss:5.249517198535614e-05\n",
      "Epoch: 41300, loss:6.036227205186151e-05\n",
      "Epoch: 41400, loss:6.910460069775581e-05\n",
      "Epoch: 41500, loss:0.0001434977020835504\n",
      "Game over after 383 steps\n",
      "Episode 147 is done\n",
      "Total reward 3.0\n",
      "Epoch: 41600, loss:2.737276918196585e-05\n",
      "Epoch: 41700, loss:0.03056940995156765\n",
      "Game over after 121 steps\n",
      "Episode 148 is done\n",
      "Total reward 0.0\n",
      "Epoch: 41800, loss:0.030434351414442062\n",
      "Epoch: 41900, loss:7.406145596178249e-05\n",
      "Game over after 270 steps\n",
      "Episode 149 is done\n",
      "Total reward 0.0\n",
      "Epoch: 42000, loss:8.433410403085873e-05\n",
      "Epoch: 42100, loss:0.00010772938549052924\n",
      "Epoch: 42200, loss:0.00010844408097909763\n",
      "Epoch: 42300, loss:6.480055162683129e-05\n",
      "Game over after 414 steps\n",
      "Episode 150 is done\n",
      "Total reward 4.0\n",
      "Epoch: 42400, loss:0.03064342401921749\n",
      "Epoch: 42500, loss:0.00010632778139552101\n",
      "Epoch: 42600, loss:0.030984606593847275\n",
      "Epoch: 42700, loss:0.00017739910981617868\n",
      "Game over after 322 steps\n",
      "Episode 151 is done\n",
      "Total reward 0.0\n",
      "Epoch: 42800, loss:9.052510722540319e-05\n",
      "Epoch: 42900, loss:0.0001448323455406353\n",
      "Epoch: 43000, loss:6.070163726690225e-05\n",
      "Epoch: 43100, loss:8.50775686558336e-05\n",
      "Epoch: 43200, loss:3.909353836206719e-05\n",
      "Game over after 518 steps\n",
      "Episode 152 is done\n",
      "Total reward 3.0\n",
      "Epoch: 43300, loss:4.432392961462028e-05\n",
      "Epoch: 43400, loss:2.9732158509432338e-05\n",
      "Game over after 242 steps\n",
      "Episode 153 is done\n",
      "Total reward 0.0\n",
      "Epoch: 43500, loss:1.122315188695211e-05\n",
      "TIME OUT: Model stops training after 3600.12 seconds, Save model to duelling_dqn.pt\n",
      "Episode 154 is done\n",
      "Total reward 0.0\n",
      "Game over after 196 steps\n",
      "Saving duelling_dqn.gif\n"
     ]
    }
   ],
   "source": [
    "seq_len = 5\n",
    "TIME_LIMIT = 3600\n",
    "env_name = 'ALE/Breakout-v5'\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "\n",
    "model_types = ['simple_dqn', 'duelling_dqn']\n",
    "simple_dqn = DQN(input_data.n_action, input_data.height, input_data.width,seq_len=seq_len)\n",
    "duelling_dqn = DuellingDQN(input_data.n_action, input_data.height, input_data.width,seq_len=seq_len)\n",
    "models = [simple_dqn,duelling_dqn]\n",
    "agents = []\n",
    "for model,model_type in zip(models,model_types):\n",
    "  agent = DQNAgent(env,model_file=model_type,model=model)\n",
    "  trainer = Trainer(agent,time_limit=TIME_LIMIT)\n",
    "  mean_reward = trainer.train_agent()\n",
    "  agent.clear_experiences()\n",
    "  agents.append(agent)\n",
    "  trainer.reboot_timer()\n",
    "  trainer.play(train=False,keep_frame=True)\n",
    "  agent.generate_gif(model_type+'.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb446b85",
   "metadata": {
    "id": "RnO4stmsNzA_",
    "papermill": {
     "duration": 0.119059,
     "end_time": "2023-11-21T10:09:15.392103",
     "exception": false,
     "start_time": "2023-11-21T10:09:15.273044",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[](./simple_dqn.gif)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7266.709733,
   "end_time": "2023-11-21T10:09:16.761509",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-21T08:08:10.051776",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
